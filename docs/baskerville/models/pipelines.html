<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, minimum-scale=1" name="viewport"/>
  <meta content="pdoc 0.7.2" name="generator"/>
  <title>
   baskerville.models.pipelines API documentation
  </title>
  <meta content="" name="description"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet"/>
  <style>
   .flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}
  </style>
  <style media="screen and (min-width: 700px)">
   @media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}
  </style>
  <style media="print">
   @media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}
  </style>
 </head>
 <body>
  <main>
   <article id="content">
    <header>
     <h1 class="title">
      Module
      <code>
       baskerville.models.pipelines
      </code>
     </h1>
    </header>
    <section id="section-intro">
     <details class="source">
      <summary>
       <span>
        Expand source code
       </span>
      </summary>
      <pre><code class="python"># Copyright (c) 2020, eQualit.ie inc.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.


import json
import traceback
from datetime import timedelta, datetime
import math

from baskerville.models.base_spark import SparkPipelineBase
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils


class ElasticsearchPipeline(SparkPipelineBase):
    """
    A pipeline for processing data directly from an ElasticSearch instance.
    """

    def __init__(
            self, db_conf, els_conf, engine_conf, spark_conf, clean_up=True
    ):
        super(ElasticsearchPipeline, self).__init__(
            db_conf, engine_conf, spark_conf, clean_up
        )

        self.els_conf = els_conf
        self.manual_conf = engine_conf.es_log
        self.start = self.manual_conf.start
        self.stop = self.manual_conf.stop
        self.batch_length = self.manual_conf.batch_length
        self.batch_timedelta = timedelta(minutes=int(self.batch_length))
        self.hosts = None
        if self.manual_conf.hosts:
            self.hosts = ', '.join(self.manual_conf.hosts)
        self.save_logs_dir = self.manual_conf.save_logs_dir
        self.batch_start = self.start
        self.batch_stop = self.batch_start + self.batch_timedelta
        self.batch_i = 1
        self.batch_n = math.ceil(
            float((self.stop - self.start).total_seconds()) /
            (self.batch_length * 60.)
        )

    def initialize(self):
        """
        Start sessions, initialize cache/features/model/dfs.
        :return:
        """
        super().initialize()
        self.set_up_es()

    def run(self):

        while self.batch_start &lt; self.stop:
            self.batch_stop = self.batch_start + self.batch_timedelta

            self.create_runtime()
            self.get_data()
            self.process_data()
            self.reset()
            self.batch_start = self.batch_stop
            self.batch_i += 1

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            start=self.batch_start,
            stop=self.batch_stop,
            target_site=self.hosts,
            conf=self.engine_conf,
            comment=f'batch runtime {self.batch_i} of {self.batch_n}'
        )

    def get_data(self):
        from pyspark.sql import functions as F

        filter_condition = (F.col('@timestamp') &gt;= self.runtime.start) &amp; \
                           (F.col('@timestamp') &lt; self.runtime.stop)

        if self.hosts is not None:
            host_filter = (F.col('client_request_host')
                           == self.manual_conf.hosts[0])
            if len(self.manual_conf.hosts) &gt; 1:
                for h in self.manual_conf.hosts[1:]:
                    host_filter = host_filter | (
                        F.col('client_request_host') == h
                    )
            filter_condition = filter_condition &amp; host_filter

        self.logs_df = self.es_storage.get(
            self.runtime.start,
            self.runtime.stop,
            filter_condition=filter_condition,
            extra_config={
                'es.mapping.include': ','.join(
                    self.group_by_cols + self.feature_manager.active_columns
                )
            },
            columns_to_keep=list(
                self.group_by_cols + self.feature_manager.active_columns
            )
        ).select(
            *self.group_by_cols, *self.feature_manager.active_columns
        ).persist(self.spark_conf.storage_level)

        self.logger.info('Will be retrieving {} rows'.format(
            self.logs_df.count()
        )
        )

        if self.save_logs_dir:
            log_name = f'/{self.runtime.start.strftime("%Y-%m-%d-%H%M%S")}' \
                       f'_' + \
                       f'{self.runtime.stop.strftime("%Y-%m-%d-%H%M%S")}'
            if self.runtime.target:
                log_name += f'_{"_".join(self.runtime.target)}' \
                    if isinstance(self.runtime.target, list) \
                    else f'_{self.runtime.target}'
            self.save_logs(self.logs_df, self.save_logs_dir + log_name)

    def set_up_es(self):

        from es_retriever.es.storage import EsStorage
        from es_retriever.config import Config

        self.es_config = Config(
            es_url=self.els_conf.host,
            es_user=self.els_conf.user,
            es_pass=self.els_conf.password,
            es_base_index=self.els_conf.base_index,
            es_index_type=self.els_conf.index_type,
        )
        # todo: fix this in es-retriever: fix setup to include jars
        conf = self.es_config.spark_conf.copy()
        conf['spark.jars'] = self.spark_conf.jars
        self.es_config.spark_conf = conf
        self.es_storage = EsStorage(self.es_config, init_session=False)
        self.es_storage.spark_conf = conf
        self.es_storage.session_getter = self.es_session_getter
        self.es_storage.session_getter()

    def es_session_getter(self):
        from pyspark.sql import SparkSession
        from pyspark import SparkConf

        conf = SparkConf()
        conf.set('spark.logConf', 'true')
        conf.set('spark.jars', self.spark_conf.jars)
        conf.set('spark.driver.memory', '6G')
        conf.set(
            'spark.sql.session.timeZone', self.spark_conf.session_timezone
        )
        conf.set('spark.sql.shuffle.partitions',
                 self.spark_conf.shuffle_partitions)

        spark = SparkSession.builder \
            .config(conf=conf) \
            .appName('Baskerville Spark') \
            .getOrCreate()

        if self.spark_conf.log_level:
            spark.sparkContext.setLogLevel(self.spark_conf.log_level)

        spark.conf.set('spark.jars', self.spark_conf.jars)

        for k, v in self.es_config.es_read_conf.items():
            spark.conf.set(k, v)
        spark.conf.set("es.port", "9200")
        return spark

    def save_logs(self, spark_df, save_logs_path):
        spark_df.coalesce(1).write.mode('overwrite').format('json').save(
            save_logs_path)


class RawLogPipeline(SparkPipelineBase):
    """
    A pipeline that processes a list of raw files.
    """
    log_paths: list

    def __init__(self, db_conf, engine_conf, spark_conf, clean_up=True):
        self.log_paths = engine_conf.raw_log.paths
        super(RawLogPipeline, self).__init__(
            db_conf, engine_conf, spark_conf, clean_up
        )
        self.log_paths = engine_conf.raw_log.paths
        self.batch_i = 1
        self.batch_n = len(self.log_paths)
        self.current_log_path = None

    def run(self):
        for log in self.log_paths:
            self.logger.info(f'Processing {log}...')
            self.current_log_path = log

            self.create_runtime()
            self.get_data()
            self.process_data()
            self.reset()

            self.batch_i += 1

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            file_name=self.current_log_path,
            conf=self.engine_conf,
            comment=f'batch runtime {self.batch_i} of {self.batch_n}'
        )
        self.logger.info('Created runtime {}'.format(self.runtime.id))

    def get_data(self):
        """
        Gets the dataframe according to the configuration
        :return: None
        """

        self.logs_df = self.spark.read.json(
            self.current_log_path
        ).persist(self.spark_conf.storage_level)
        # .repartition(*self.group_by_cols)

        self.logger.info('Got dataframe of #{} records'.format(
            self.logs_df.count())
        )
        self.load_test()


class KafkaPipeline(SparkPipelineBase):
    """
    A pipeline that processes data from a Kafka instance every x seconds.
    """

    def __init__(
            self,
            db_conf,
            engine_conf,
            kafka_conf,
            spark_conf,
            clean_up=True
    ):
        super(KafkaPipeline, self).__init__(
            db_conf, engine_conf, spark_conf, clean_up=clean_up,
        )

        self.kafka_conf = kafka_conf
        self.start = None
        self.ssc = None

    def initialize(self):
        super().initialize()
        self.ssc = StreamingContext(
            self.spark.sparkContext, self.engine_conf.time_bucket
        )

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            start=self.start,
            conf=self.engine_conf
        )

    def get_data(self):

        self.logs_df = self.logs_df.map(lambda l: json.loads(l[1])).toDF(
            self.data_parser.schema
        ).repartition(*self.group_by_cols).persist(
            self.spark_conf.storage_level)

        self.load_test()

    def run(self):
        self.create_runtime()

        kafkaParams = {
            # 'bootstrap.servers': self.kafka_conf.bootstrap_servers,
            'metadata.broker.list': self.kafka_conf.bootstrap_servers,
            'auto.offset.reset': 'largest',
            # 'security.protocol': self.kafka_conf.security_protocol,
            # 'ssl.truststore.location': self.kafka_conf
            # .ssl_truststore_location,
            # 'ssl.truststore.password': self.kafka_conf
            # .ssl_truststore_password,
            # 'ssl.keystore.type': 'JKS',
            # 'ssl.truststore.type': 'JKS',
            # 'ssl.keystore.location': self.kafka_conf.ssl_keystore_location,
            # 'ssl.keystore.password': self.kafka_conf.ssl_keystore_password,
            # 'ssl.key.password': self.kafka_conf.ssl_key_password,
            # 'ssl.endpoint.identification.algorithm':
            # self.kafka_conf.ssl_endpoint_identification_algorithm,

            'group.id': self.kafka_conf.consume_group,
            # 'auto.create.topics.enable': 'true'
        }

        kafkaStream = KafkaUtils.createDirectStream(
            self.ssc,
            [self.kafka_conf.logs_topic],
            kafkaParams=kafkaParams,
            # fromOffsets={TopicAndPartition(
            # self.kafka_conf.consume_topic, 0): 0}
        )

        # from pympler import muppy, summary, tracker
        # tr = tracker.SummaryTracker()

        def process_subsets(time, rdd):
            global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

            self.logger.info('Data until {}'.format(time))
            if not rdd.isEmpty():
                # print('*-' * 25, 'BEFORE', '*-' * 25)
                # all_objects = muppy.get_objects()
                # self.logger.debug(f'**** Length of all objects BEFORE:
                # {len(all_objects)}')
                # tr.print_diff()
                try:
                    # set dataframe to process later on
                    # todo: handle edge cases
                    # todo: what happens if we have a missing column here?
                    # todo: does the time this takes to complete affects the
                    # kafka messages consumption?
                    self.logs_df = rdd
                    self.get_data()
                    self.remaining_steps = list(self.step_to_action.keys())

                    for step, action in self.step_to_action.items():
                        self.logger.info('Starting step {}'.format(step))
                        action()
                        self.logger.info('Completed step {}'.format(step))
                        self.remaining_steps.remove(step)
                    # print('*'*50, 'AFTER', '*'*50)
                    # all_objects = muppy.get_objects()
                    # self.logger.debug(
                    #     f'**** Length of all objects AFTER:
                    #     {len(all_objects)}')
                    # tr.print_diff()
                    self.logger.debug(
                        f'self.spark.sparkContext.'
                        f'_jsc.getPersistentRDDs().items() '
                        f'{ len(self.spark.sparkContext._jsc.getPersistentRDDs().items())}')  # noqa
                    rdd.unpersist()
                    del rdd
                except Exception as e:
                    traceback.print_exc()
                    self.logger.error(e)
                finally:
                    self.reset()
            else:
                self.logger.info('Empty RDD...')

        kafkaStream.foreachRDD(process_subsets)

        self.ssc.start()
        self.ssc.awaitTermination()


class SparkStructuredStreamingRealTimePipeline(SparkPipelineBase):
    def __init__(
            self,
            db_conf,
            engine_conf,
            kafka_conf,
            spark_conf,
            clean_up=True
    ):
        super(SparkStructuredStreamingRealTimePipeline, self).__init__(
            db_conf,
            engine_conf,
            spark_conf,
            clean_up=clean_up,
        )
        self.engine_conf = engine_conf
        self.features_conf = engine_conf.features
        self.kafka_conf = kafka_conf
        self.log_parser = self.engine_conf.data_config.parser
        self.start = None

        self.ssc = StreamingContext(
            self.spark.sparkContext, self.engine_conf.time_bucket
        )

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            start=self.start,
            dt_bucket=self.engine_conf.time_bucket
        )

    def get_data(self):
        self.logs_df = self.logs_df.map(lambda l: json.loads(l[1])).toDF(
            self.log_parser.schema
        )
        # since filtering is done in a next step, do not use actions here,
        # because it will cause calculations with the whole dataset
        # self.logger.info('Got dataframe of #{} records'.format(
        #     self.logs_df.count())
        # )

    def run(self):
        self.start = datetime.utcnow()
        self.create_runtime()

        # Subscribe to a pattern, read from the end of the stream:
        # kafkaStream = self.spark \
        #     .read \
        #     .format("kafka") \
        #     .option("startingOffsets", "earliest") \
        #     .option("kafka.bootstrap.servers", self.kafka_conf.url) \
        #     .option("subscribe", self.kafka_conf.consume_topic) \
        #     .option("auto.offset.reset", "earliest") \
        #     .load()

        # .option("kafka.partition.assignment.strategy", "range") \
        # .option("security.protocol", "SSL") \
        # .option("ssl.truststore.location",
        # "/deflect-analytics-ecosystem/
        # containers/kafka/local_cert/client.truststore.jks") \
        # .option("ssl.truststore.password", "kafkadocker") \
        # .option("ssl.keystore.location",
        # "/deflect-analytics-ecosystem/containers/kafka/
        # local_cert/kafka.server.keystore.jks") \
        # .option("ssl.keystore.password", "kafkadocker") \
        # .option("ssl.key.password", "kafkadocker") \
        # .load()

        # .option("subscribePattern", self.kafka_conf.consume_topic) \

        # kafkaStream = self.spark \
        #     .readStream \
        #     .format("kafka") \
        #     .option("kafka.bootstrap.servers", self.kafka_conf.zookeeper) \
        #     .option("startingOffsets", "earliest") \
        #     .option("subscribe", self.kafka_conf.consume_topic) \
        #     .option("auto.offset.reset", "earliest") \
        #     .option("security.protocol", "SSL") \
        #     .option("ssl.truststore.location",
        #             "/deflect-analytics-ecosystem/containers/
        #             kafka/local_cert/client.truststore.jks") \
        #     .option("ssl.truststore.password", "kafkadocker") \
        #     .option("ssl.keystore.location",
        #             "/deflect-analytics-ecosystem/
        #             containers/kafka/local_cert/kafka.server.keystore.jks") \
        #     .option("ssl.keystore.password", "kafkadocker") \
        #     .option("ssl.key.password", "kafkadocker") \
        #     .load()
        #
        # import pyspark.sql.functions as F
        # df1 = kafkaStream.selectExpr("CAST(value AS STRING)",
        #                     "CAST(timestamp AS TIMESTAMP)"
        #       ).select(F.from_json("value", self.data_parser.schema))
        #
        # q = df1.writeStream.format("console") \
        #         .option("truncate","false")\
        #         .start()\
        #         .awaitTermination()
        # # .option("kafka.partition.assignment.strategy", "range") \
        #
        # NOTE: make sure kafka 8 jar is in the spark.jars, won't work with 10
        # https://elephant.tech/spark-2-0-streaming-from-ssl-kafka-with-hdp-2-4/  # noqa

        # topicPartion = TopicAndPartition(self.kafka_conf.consume_topic, 0)

        kafkaStream = KafkaUtils.createDirectStream(
            self.ssc,
            [self.kafka_conf.logs_topic],
            {
                # 'bootstrap.servers': self.kafka_conf.zookeeper,
                'metadata.broker.list': self.kafka_conf.url,
                # "kafka.sasl.kerberos.service.name": "kafka",
                # "kafka.sasl.kerberos.service.name":
                # "/usr/lib/jvm/jdk1.8.0_162/jre/lib/security/cacerts",
                'group.id': self.kafka_conf.consume_group,
                # 'auto.offset.reset': 'largest',
                # 'security.protocol': self.kafka_conf.security_protocol,
                # "kafka.ssl.truststore.location":
                # self.kafka_conf.ssl_truststore_location,
                # "kafka.ssl.truststore.password":
                # self.kafka_conf.ssl_truststore_password
            }
            # fromOffset={topicPartion: int(0)}
        )

        # readDF = kafkaStream.selectExpr("CAST(key AS STRING)",
        #                            "CAST(value AS STRING)")

        # readDF.show()

        # query = readDF.writeStream.format("console").start()
        # import time
        # time.sleep(10)  # sleep 10 seconds
        # query.stop()
        # windowed_stream = kafkaStream.withWatermark("@timestamp",
        # f"{self.engine_conf.time_bucket} seconds").selectExpr(
        # "CAST(value AS STRING)",
        # "CAST(timestamp AS TIMESTAMP)").toDF('log', 'timestamp')
        #
        # q = kafkaStream.writeStream \
        #     .format("console") \
        #     .option("truncate", "false")

        # q = windowed_stream.count()
        # print(q)
        #
        # windowed_stream.start().awaitTermination()

        def process_subsets(time, rdd):
            self.logger.info('Data until {}'.format(time))
            if not rdd.isEmpty():
                # set dataframe to process later on
                self.logs_df = rdd
                self._get_df()
                self.remaining_steps = list(self.step_to_action.keys())

                for step, action in self.step_to_action.items():
                    self.logger.info('Starting step {}'.format(step))
                    action()
                    self.logger.info('Completed step {}'.format(step))
                    self.remaining_steps.remove(step)
            else:
                self.logger.info('Empty RDD...')
            self.reset()

        kafkaStream.foreachRDD(process_subsets)

        self.ssc.start()

        self.ssc.awaitTermination()</code></pre>
     </details>
    </section>
    <section>
    </section>
    <section>
    </section>
    <section>
    </section>
    <section>
     <h2 class="section-title" id="header-classes">
      Classes
     </h2>
     <dl>
      <dt id="baskerville.models.pipelines.ElasticsearchPipeline">
       <code class="flex name class">
        <span>
         class
         <span class="ident">
          ElasticsearchPipeline
         </span>
        </span>
        <span>
         (
        </span>
        <span>
         db_conf, els_conf, engine_conf, spark_conf, clean_up=True)
        </span>
       </code>
      </dt>
      <dd>
       <section class="desc">
        <p>
         A pipeline for processing data directly from an ElasticSearch instance.
        </p>
       </section>
       <details class="source">
        <summary>
         <span>
          Expand source code
         </span>
        </summary>
        <pre><code class="python">class ElasticsearchPipeline(SparkPipelineBase):
    """
    A pipeline for processing data directly from an ElasticSearch instance.
    """

    def __init__(
            self, db_conf, els_conf, engine_conf, spark_conf, clean_up=True
    ):
        super(ElasticsearchPipeline, self).__init__(
            db_conf, engine_conf, spark_conf, clean_up
        )

        self.els_conf = els_conf
        self.manual_conf = engine_conf.es_log
        self.start = self.manual_conf.start
        self.stop = self.manual_conf.stop
        self.batch_length = self.manual_conf.batch_length
        self.batch_timedelta = timedelta(minutes=int(self.batch_length))
        self.hosts = None
        if self.manual_conf.hosts:
            self.hosts = ', '.join(self.manual_conf.hosts)
        self.save_logs_dir = self.manual_conf.save_logs_dir
        self.batch_start = self.start
        self.batch_stop = self.batch_start + self.batch_timedelta
        self.batch_i = 1
        self.batch_n = math.ceil(
            float((self.stop - self.start).total_seconds()) /
            (self.batch_length * 60.)
        )

    def initialize(self):
        """
        Start sessions, initialize cache/features/model/dfs.
        :return:
        """
        super().initialize()
        self.set_up_es()

    def run(self):

        while self.batch_start &lt; self.stop:
            self.batch_stop = self.batch_start + self.batch_timedelta

            self.create_runtime()
            self.get_data()
            self.process_data()
            self.reset()
            self.batch_start = self.batch_stop
            self.batch_i += 1

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            start=self.batch_start,
            stop=self.batch_stop,
            target_site=self.hosts,
            conf=self.engine_conf,
            comment=f'batch runtime {self.batch_i} of {self.batch_n}'
        )

    def get_data(self):
        from pyspark.sql import functions as F

        filter_condition = (F.col('@timestamp') &gt;= self.runtime.start) &amp; \
                           (F.col('@timestamp') &lt; self.runtime.stop)

        if self.hosts is not None:
            host_filter = (F.col('client_request_host')
                           == self.manual_conf.hosts[0])
            if len(self.manual_conf.hosts) &gt; 1:
                for h in self.manual_conf.hosts[1:]:
                    host_filter = host_filter | (
                        F.col('client_request_host') == h
                    )
            filter_condition = filter_condition &amp; host_filter

        self.logs_df = self.es_storage.get(
            self.runtime.start,
            self.runtime.stop,
            filter_condition=filter_condition,
            extra_config={
                'es.mapping.include': ','.join(
                    self.group_by_cols + self.feature_manager.active_columns
                )
            },
            columns_to_keep=list(
                self.group_by_cols + self.feature_manager.active_columns
            )
        ).select(
            *self.group_by_cols, *self.feature_manager.active_columns
        ).persist(self.spark_conf.storage_level)

        self.logger.info('Will be retrieving {} rows'.format(
            self.logs_df.count()
        )
        )

        if self.save_logs_dir:
            log_name = f'/{self.runtime.start.strftime("%Y-%m-%d-%H%M%S")}' \
                       f'_' + \
                       f'{self.runtime.stop.strftime("%Y-%m-%d-%H%M%S")}'
            if self.runtime.target:
                log_name += f'_{"_".join(self.runtime.target)}' \
                    if isinstance(self.runtime.target, list) \
                    else f'_{self.runtime.target}'
            self.save_logs(self.logs_df, self.save_logs_dir + log_name)

    def set_up_es(self):

        from es_retriever.es.storage import EsStorage
        from es_retriever.config import Config

        self.es_config = Config(
            es_url=self.els_conf.host,
            es_user=self.els_conf.user,
            es_pass=self.els_conf.password,
            es_base_index=self.els_conf.base_index,
            es_index_type=self.els_conf.index_type,
        )
        # todo: fix this in es-retriever: fix setup to include jars
        conf = self.es_config.spark_conf.copy()
        conf['spark.jars'] = self.spark_conf.jars
        self.es_config.spark_conf = conf
        self.es_storage = EsStorage(self.es_config, init_session=False)
        self.es_storage.spark_conf = conf
        self.es_storage.session_getter = self.es_session_getter
        self.es_storage.session_getter()

    def es_session_getter(self):
        from pyspark.sql import SparkSession
        from pyspark import SparkConf

        conf = SparkConf()
        conf.set('spark.logConf', 'true')
        conf.set('spark.jars', self.spark_conf.jars)
        conf.set('spark.driver.memory', '6G')
        conf.set(
            'spark.sql.session.timeZone', self.spark_conf.session_timezone
        )
        conf.set('spark.sql.shuffle.partitions',
                 self.spark_conf.shuffle_partitions)

        spark = SparkSession.builder \
            .config(conf=conf) \
            .appName('Baskerville Spark') \
            .getOrCreate()

        if self.spark_conf.log_level:
            spark.sparkContext.setLogLevel(self.spark_conf.log_level)

        spark.conf.set('spark.jars', self.spark_conf.jars)

        for k, v in self.es_config.es_read_conf.items():
            spark.conf.set(k, v)
        spark.conf.set("es.port", "9200")
        return spark

    def save_logs(self, spark_df, save_logs_path):
        spark_df.coalesce(1).write.mode('overwrite').format('json').save(
            save_logs_path)</code></pre>
       </details>
       <h3>
        Ancestors
       </h3>
       <ul class="hlist">
        <li>
         <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
          SparkPipelineBase
         </a>
        </li>
        <li>
         <a href="base.html#baskerville.models.base.PipelineBase" title="baskerville.models.base.PipelineBase">
          PipelineBase
         </a>
        </li>
       </ul>
       <h3>
        Methods
       </h3>
       <dl>
        <dt id="baskerville.models.pipelines.ElasticsearchPipeline.es_session_getter">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            es_session_getter
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def es_session_getter(self):
    from pyspark.sql import SparkSession
    from pyspark import SparkConf

    conf = SparkConf()
    conf.set('spark.logConf', 'true')
    conf.set('spark.jars', self.spark_conf.jars)
    conf.set('spark.driver.memory', '6G')
    conf.set(
        'spark.sql.session.timeZone', self.spark_conf.session_timezone
    )
    conf.set('spark.sql.shuffle.partitions',
             self.spark_conf.shuffle_partitions)

    spark = SparkSession.builder \
        .config(conf=conf) \
        .appName('Baskerville Spark') \
        .getOrCreate()

    if self.spark_conf.log_level:
        spark.sparkContext.setLogLevel(self.spark_conf.log_level)

    spark.conf.set('spark.jars', self.spark_conf.jars)

    for k, v in self.es_config.es_read_conf.items():
        spark.conf.set(k, v)
    spark.conf.set("es.port", "9200")
    return spark</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.pipelines.ElasticsearchPipeline.initialize">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            initialize
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Start sessions, initialize cache/features/model/dfs.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def initialize(self):
    """
    Start sessions, initialize cache/features/model/dfs.
    :return:
    """
    super().initialize()
    self.set_up_es()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.pipelines.ElasticsearchPipeline.save_logs">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            save_logs
           </span>
          </span>
          (
          <span>
           self, spark_df, save_logs_path)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def save_logs(self, spark_df, save_logs_path):
    spark_df.coalesce(1).write.mode('overwrite').format('json').save(
        save_logs_path)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.pipelines.ElasticsearchPipeline.set_up_es">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            set_up_es
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def set_up_es(self):

    from es_retriever.es.storage import EsStorage
    from es_retriever.config import Config

    self.es_config = Config(
        es_url=self.els_conf.host,
        es_user=self.els_conf.user,
        es_pass=self.els_conf.password,
        es_base_index=self.els_conf.base_index,
        es_index_type=self.els_conf.index_type,
    )
    # todo: fix this in es-retriever: fix setup to include jars
    conf = self.es_config.spark_conf.copy()
    conf['spark.jars'] = self.spark_conf.jars
    self.es_config.spark_conf = conf
    self.es_storage = EsStorage(self.es_config, init_session=False)
    self.es_storage.spark_conf = conf
    self.es_storage.session_getter = self.es_session_getter
    self.es_storage.session_getter()</code></pre>
         </details>
        </dd>
       </dl>
       <h3>
        Inherited members
       </h3>
       <ul class="hlist">
        <li>
         <code>
          <b>
           <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
            SparkPipelineBase
           </a>
          </b>
         </code>
         :
         <ul class="hlist">
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_cache_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns">
             add_cache_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_calc_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns">
             add_calc_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns">
             add_post_groupby_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.create_runtime" title="baskerville.models.base_spark.SparkPipelineBase.create_runtime">
             create_runtime
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.cross_reference" title="baskerville.models.base_spark.SparkPipelineBase.cross_reference">
             cross_reference
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_calculation" title="baskerville.models.base_spark.SparkPipelineBase.feature_calculation">
             feature_calculation
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_extraction" title="baskerville.models.base_spark.SparkPipelineBase.feature_extraction">
             feature_extraction
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_update" title="baskerville.models.base_spark.SparkPipelineBase.feature_update">
             feature_update
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.features_to_dict" title="baskerville.models.base_spark.SparkPipelineBase.features_to_dict">
             features_to_dict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_cache" title="baskerville.models.base_spark.SparkPipelineBase.filter_cache">
             filter_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_columns" title="baskerville.models.base_spark.SparkPipelineBase.filter_columns">
             filter_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.finish_up" title="baskerville.models.base_spark.SparkPipelineBase.finish_up">
             finish_up
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by" title="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by">
             get_columns_to_filter_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_data" title="baskerville.models.base_spark.SparkPipelineBase.get_data">
             get_data
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs" title="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs">
             get_group_by_aggs
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations" title="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations">
             get_post_group_by_calculations
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.group_by" title="baskerville.models.base_spark.SparkPipelineBase.group_by">
             group_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns" title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns">
             handle_missing_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session" title="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session">
             instantiate_spark_session
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.label_or_predict" title="baskerville.models.base_spark.SparkPipelineBase.label_or_predict">
             label_or_predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.load_test" title="baskerville.models.base_spark.SparkPipelineBase.load_test">
             load_test
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.normalize_host_names" title="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names">
             normalize_host_names
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.predict" title="baskerville.models.base_spark.SparkPipelineBase.predict">
             predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.preprocessing" title="baskerville.models.base_spark.SparkPipelineBase.preprocessing">
             preprocessing
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.process_data" title="baskerville.models.base_spark.SparkPipelineBase.process_data">
             process_data
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.refresh_cache" title="baskerville.models.base_spark.SparkPipelineBase.refresh_cache">
             refresh_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.rename_columns" title="baskerville.models.base_spark.SparkPipelineBase.rename_columns">
             rename_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.reset" title="baskerville.models.base_spark.SparkPipelineBase.reset">
             reset
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.run" title="baskerville.models.base_spark.SparkPipelineBase.run">
             run
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save" title="baskerville.models.base_spark.SparkPipelineBase.save">
             save
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save_df_to_table" title="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table">
             save_df_to_table
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache" title="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache">
             set_up_request_set_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.trigger_challenge" title="baskerville.models.base_spark.SparkPipelineBase.trigger_challenge">
             trigger_challenge
            </a>
           </code>
          </li>
         </ul>
        </li>
       </ul>
      </dd>
      <dt id="baskerville.models.pipelines.KafkaPipeline">
       <code class="flex name class">
        <span>
         class
         <span class="ident">
          KafkaPipeline
         </span>
        </span>
        <span>
         (
        </span>
        <span>
         db_conf, engine_conf, kafka_conf, spark_conf, clean_up=True)
        </span>
       </code>
      </dt>
      <dd>
       <section class="desc">
        <p>
         A pipeline that processes data from a Kafka instance every x seconds.
        </p>
       </section>
       <details class="source">
        <summary>
         <span>
          Expand source code
         </span>
        </summary>
        <pre><code class="python">class KafkaPipeline(SparkPipelineBase):
    """
    A pipeline that processes data from a Kafka instance every x seconds.
    """

    def __init__(
            self,
            db_conf,
            engine_conf,
            kafka_conf,
            spark_conf,
            clean_up=True
    ):
        super(KafkaPipeline, self).__init__(
            db_conf, engine_conf, spark_conf, clean_up=clean_up,
        )

        self.kafka_conf = kafka_conf
        self.start = None
        self.ssc = None

    def initialize(self):
        super().initialize()
        self.ssc = StreamingContext(
            self.spark.sparkContext, self.engine_conf.time_bucket
        )

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            start=self.start,
            conf=self.engine_conf
        )

    def get_data(self):

        self.logs_df = self.logs_df.map(lambda l: json.loads(l[1])).toDF(
            self.data_parser.schema
        ).repartition(*self.group_by_cols).persist(
            self.spark_conf.storage_level)

        self.load_test()

    def run(self):
        self.create_runtime()

        kafkaParams = {
            # 'bootstrap.servers': self.kafka_conf.bootstrap_servers,
            'metadata.broker.list': self.kafka_conf.bootstrap_servers,
            'auto.offset.reset': 'largest',
            # 'security.protocol': self.kafka_conf.security_protocol,
            # 'ssl.truststore.location': self.kafka_conf
            # .ssl_truststore_location,
            # 'ssl.truststore.password': self.kafka_conf
            # .ssl_truststore_password,
            # 'ssl.keystore.type': 'JKS',
            # 'ssl.truststore.type': 'JKS',
            # 'ssl.keystore.location': self.kafka_conf.ssl_keystore_location,
            # 'ssl.keystore.password': self.kafka_conf.ssl_keystore_password,
            # 'ssl.key.password': self.kafka_conf.ssl_key_password,
            # 'ssl.endpoint.identification.algorithm':
            # self.kafka_conf.ssl_endpoint_identification_algorithm,

            'group.id': self.kafka_conf.consume_group,
            # 'auto.create.topics.enable': 'true'
        }

        kafkaStream = KafkaUtils.createDirectStream(
            self.ssc,
            [self.kafka_conf.logs_topic],
            kafkaParams=kafkaParams,
            # fromOffsets={TopicAndPartition(
            # self.kafka_conf.consume_topic, 0): 0}
        )

        # from pympler import muppy, summary, tracker
        # tr = tracker.SummaryTracker()

        def process_subsets(time, rdd):
            global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

            self.logger.info('Data until {}'.format(time))
            if not rdd.isEmpty():
                # print('*-' * 25, 'BEFORE', '*-' * 25)
                # all_objects = muppy.get_objects()
                # self.logger.debug(f'**** Length of all objects BEFORE:
                # {len(all_objects)}')
                # tr.print_diff()
                try:
                    # set dataframe to process later on
                    # todo: handle edge cases
                    # todo: what happens if we have a missing column here?
                    # todo: does the time this takes to complete affects the
                    # kafka messages consumption?
                    self.logs_df = rdd
                    self.get_data()
                    self.remaining_steps = list(self.step_to_action.keys())

                    for step, action in self.step_to_action.items():
                        self.logger.info('Starting step {}'.format(step))
                        action()
                        self.logger.info('Completed step {}'.format(step))
                        self.remaining_steps.remove(step)
                    # print('*'*50, 'AFTER', '*'*50)
                    # all_objects = muppy.get_objects()
                    # self.logger.debug(
                    #     f'**** Length of all objects AFTER:
                    #     {len(all_objects)}')
                    # tr.print_diff()
                    self.logger.debug(
                        f'self.spark.sparkContext.'
                        f'_jsc.getPersistentRDDs().items() '
                        f'{ len(self.spark.sparkContext._jsc.getPersistentRDDs().items())}')  # noqa
                    rdd.unpersist()
                    del rdd
                except Exception as e:
                    traceback.print_exc()
                    self.logger.error(e)
                finally:
                    self.reset()
            else:
                self.logger.info('Empty RDD...')

        kafkaStream.foreachRDD(process_subsets)

        self.ssc.start()
        self.ssc.awaitTermination()</code></pre>
       </details>
       <h3>
        Ancestors
       </h3>
       <ul class="hlist">
        <li>
         <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
          SparkPipelineBase
         </a>
        </li>
        <li>
         <a href="base.html#baskerville.models.base.PipelineBase" title="baskerville.models.base.PipelineBase">
          PipelineBase
         </a>
        </li>
       </ul>
       <h3>
        Inherited members
       </h3>
       <ul class="hlist">
        <li>
         <code>
          <b>
           <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
            SparkPipelineBase
           </a>
          </b>
         </code>
         :
         <ul class="hlist">
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_cache_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns">
             add_cache_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_calc_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns">
             add_calc_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns">
             add_post_groupby_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.create_runtime" title="baskerville.models.base_spark.SparkPipelineBase.create_runtime">
             create_runtime
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.cross_reference" title="baskerville.models.base_spark.SparkPipelineBase.cross_reference">
             cross_reference
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_calculation" title="baskerville.models.base_spark.SparkPipelineBase.feature_calculation">
             feature_calculation
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_extraction" title="baskerville.models.base_spark.SparkPipelineBase.feature_extraction">
             feature_extraction
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_update" title="baskerville.models.base_spark.SparkPipelineBase.feature_update">
             feature_update
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.features_to_dict" title="baskerville.models.base_spark.SparkPipelineBase.features_to_dict">
             features_to_dict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_cache" title="baskerville.models.base_spark.SparkPipelineBase.filter_cache">
             filter_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_columns" title="baskerville.models.base_spark.SparkPipelineBase.filter_columns">
             filter_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.finish_up" title="baskerville.models.base_spark.SparkPipelineBase.finish_up">
             finish_up
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by" title="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by">
             get_columns_to_filter_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_data" title="baskerville.models.base_spark.SparkPipelineBase.get_data">
             get_data
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs" title="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs">
             get_group_by_aggs
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations" title="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations">
             get_post_group_by_calculations
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.group_by" title="baskerville.models.base_spark.SparkPipelineBase.group_by">
             group_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns" title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns">
             handle_missing_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.initialize" title="baskerville.models.base_spark.SparkPipelineBase.initialize">
             initialize
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session" title="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session">
             instantiate_spark_session
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.label_or_predict" title="baskerville.models.base_spark.SparkPipelineBase.label_or_predict">
             label_or_predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.load_test" title="baskerville.models.base_spark.SparkPipelineBase.load_test">
             load_test
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.normalize_host_names" title="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names">
             normalize_host_names
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.predict" title="baskerville.models.base_spark.SparkPipelineBase.predict">
             predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.preprocessing" title="baskerville.models.base_spark.SparkPipelineBase.preprocessing">
             preprocessing
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.process_data" title="baskerville.models.base_spark.SparkPipelineBase.process_data">
             process_data
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.refresh_cache" title="baskerville.models.base_spark.SparkPipelineBase.refresh_cache">
             refresh_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.rename_columns" title="baskerville.models.base_spark.SparkPipelineBase.rename_columns">
             rename_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.reset" title="baskerville.models.base_spark.SparkPipelineBase.reset">
             reset
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.run" title="baskerville.models.base_spark.SparkPipelineBase.run">
             run
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save" title="baskerville.models.base_spark.SparkPipelineBase.save">
             save
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save_df_to_table" title="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table">
             save_df_to_table
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache" title="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache">
             set_up_request_set_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.trigger_challenge" title="baskerville.models.base_spark.SparkPipelineBase.trigger_challenge">
             trigger_challenge
            </a>
           </code>
          </li>
         </ul>
        </li>
       </ul>
      </dd>
      <dt id="baskerville.models.pipelines.RawLogPipeline">
       <code class="flex name class">
        <span>
         class
         <span class="ident">
          RawLogPipeline
         </span>
        </span>
        <span>
         (
        </span>
        <span>
         db_conf, engine_conf, spark_conf, clean_up=True)
        </span>
       </code>
      </dt>
      <dd>
       <section class="desc">
        <p>
         A pipeline that processes a list of raw files.
        </p>
       </section>
       <details class="source">
        <summary>
         <span>
          Expand source code
         </span>
        </summary>
        <pre><code class="python">class RawLogPipeline(SparkPipelineBase):
    """
    A pipeline that processes a list of raw files.
    """
    log_paths: list

    def __init__(self, db_conf, engine_conf, spark_conf, clean_up=True):
        self.log_paths = engine_conf.raw_log.paths
        super(RawLogPipeline, self).__init__(
            db_conf, engine_conf, spark_conf, clean_up
        )
        self.log_paths = engine_conf.raw_log.paths
        self.batch_i = 1
        self.batch_n = len(self.log_paths)
        self.current_log_path = None

    def run(self):
        for log in self.log_paths:
            self.logger.info(f'Processing {log}...')
            self.current_log_path = log

            self.create_runtime()
            self.get_data()
            self.process_data()
            self.reset()

            self.batch_i += 1

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            file_name=self.current_log_path,
            conf=self.engine_conf,
            comment=f'batch runtime {self.batch_i} of {self.batch_n}'
        )
        self.logger.info('Created runtime {}'.format(self.runtime.id))

    def get_data(self):
        """
        Gets the dataframe according to the configuration
        :return: None
        """

        self.logs_df = self.spark.read.json(
            self.current_log_path
        ).persist(self.spark_conf.storage_level)
        # .repartition(*self.group_by_cols)

        self.logger.info('Got dataframe of #{} records'.format(
            self.logs_df.count())
        )
        self.load_test()</code></pre>
       </details>
       <h3>
        Ancestors
       </h3>
       <ul class="hlist">
        <li>
         <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
          SparkPipelineBase
         </a>
        </li>
        <li>
         <a href="base.html#baskerville.models.base.PipelineBase" title="baskerville.models.base.PipelineBase">
          PipelineBase
         </a>
        </li>
       </ul>
       <h3>
        Methods
       </h3>
       <dl>
        <dt id="baskerville.models.pipelines.RawLogPipeline.get_data">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            get_data
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Gets the dataframe according to the configuration
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def get_data(self):
    """
    Gets the dataframe according to the configuration
    :return: None
    """

    self.logs_df = self.spark.read.json(
        self.current_log_path
    ).persist(self.spark_conf.storage_level)
    # .repartition(*self.group_by_cols)

    self.logger.info('Got dataframe of #{} records'.format(
        self.logs_df.count())
    )
    self.load_test()</code></pre>
         </details>
        </dd>
       </dl>
       <h3>
        Inherited members
       </h3>
       <ul class="hlist">
        <li>
         <code>
          <b>
           <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
            SparkPipelineBase
           </a>
          </b>
         </code>
         :
         <ul class="hlist">
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_cache_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns">
             add_cache_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_calc_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns">
             add_calc_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns">
             add_post_groupby_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.create_runtime" title="baskerville.models.base_spark.SparkPipelineBase.create_runtime">
             create_runtime
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.cross_reference" title="baskerville.models.base_spark.SparkPipelineBase.cross_reference">
             cross_reference
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_calculation" title="baskerville.models.base_spark.SparkPipelineBase.feature_calculation">
             feature_calculation
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_extraction" title="baskerville.models.base_spark.SparkPipelineBase.feature_extraction">
             feature_extraction
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_update" title="baskerville.models.base_spark.SparkPipelineBase.feature_update">
             feature_update
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.features_to_dict" title="baskerville.models.base_spark.SparkPipelineBase.features_to_dict">
             features_to_dict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_cache" title="baskerville.models.base_spark.SparkPipelineBase.filter_cache">
             filter_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_columns" title="baskerville.models.base_spark.SparkPipelineBase.filter_columns">
             filter_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.finish_up" title="baskerville.models.base_spark.SparkPipelineBase.finish_up">
             finish_up
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by" title="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by">
             get_columns_to_filter_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs" title="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs">
             get_group_by_aggs
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations" title="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations">
             get_post_group_by_calculations
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.group_by" title="baskerville.models.base_spark.SparkPipelineBase.group_by">
             group_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns" title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns">
             handle_missing_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.initialize" title="baskerville.models.base_spark.SparkPipelineBase.initialize">
             initialize
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session" title="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session">
             instantiate_spark_session
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.label_or_predict" title="baskerville.models.base_spark.SparkPipelineBase.label_or_predict">
             label_or_predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.load_test" title="baskerville.models.base_spark.SparkPipelineBase.load_test">
             load_test
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.normalize_host_names" title="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names">
             normalize_host_names
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.predict" title="baskerville.models.base_spark.SparkPipelineBase.predict">
             predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.preprocessing" title="baskerville.models.base_spark.SparkPipelineBase.preprocessing">
             preprocessing
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.process_data" title="baskerville.models.base_spark.SparkPipelineBase.process_data">
             process_data
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.refresh_cache" title="baskerville.models.base_spark.SparkPipelineBase.refresh_cache">
             refresh_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.rename_columns" title="baskerville.models.base_spark.SparkPipelineBase.rename_columns">
             rename_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.reset" title="baskerville.models.base_spark.SparkPipelineBase.reset">
             reset
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.run" title="baskerville.models.base_spark.SparkPipelineBase.run">
             run
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save" title="baskerville.models.base_spark.SparkPipelineBase.save">
             save
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save_df_to_table" title="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table">
             save_df_to_table
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache" title="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache">
             set_up_request_set_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.trigger_challenge" title="baskerville.models.base_spark.SparkPipelineBase.trigger_challenge">
             trigger_challenge
            </a>
           </code>
          </li>
         </ul>
        </li>
       </ul>
      </dd>
      <dt id="baskerville.models.pipelines.SparkStructuredStreamingRealTimePipeline">
       <code class="flex name class">
        <span>
         class
         <span class="ident">
          SparkStructuredStreamingRealTimePipeline
         </span>
        </span>
        <span>
         (
        </span>
        <span>
         db_conf, engine_conf, kafka_conf, spark_conf, clean_up=True)
        </span>
       </code>
      </dt>
      <dd>
       <section class="desc">
        <p>
         The base class for all pipelines that use spark. It initializes spark
session and provides basic implementation for some of the main methods
        </p>
       </section>
       <details class="source">
        <summary>
         <span>
          Expand source code
         </span>
        </summary>
        <pre><code class="python">class SparkStructuredStreamingRealTimePipeline(SparkPipelineBase):
    def __init__(
            self,
            db_conf,
            engine_conf,
            kafka_conf,
            spark_conf,
            clean_up=True
    ):
        super(SparkStructuredStreamingRealTimePipeline, self).__init__(
            db_conf,
            engine_conf,
            spark_conf,
            clean_up=clean_up,
        )
        self.engine_conf = engine_conf
        self.features_conf = engine_conf.features
        self.kafka_conf = kafka_conf
        self.log_parser = self.engine_conf.data_config.parser
        self.start = None

        self.ssc = StreamingContext(
            self.spark.sparkContext, self.engine_conf.time_bucket
        )

    def create_runtime(self):
        self.runtime = self.tools.create_runtime(
            start=self.start,
            dt_bucket=self.engine_conf.time_bucket
        )

    def get_data(self):
        self.logs_df = self.logs_df.map(lambda l: json.loads(l[1])).toDF(
            self.log_parser.schema
        )
        # since filtering is done in a next step, do not use actions here,
        # because it will cause calculations with the whole dataset
        # self.logger.info('Got dataframe of #{} records'.format(
        #     self.logs_df.count())
        # )

    def run(self):
        self.start = datetime.utcnow()
        self.create_runtime()

        # Subscribe to a pattern, read from the end of the stream:
        # kafkaStream = self.spark \
        #     .read \
        #     .format("kafka") \
        #     .option("startingOffsets", "earliest") \
        #     .option("kafka.bootstrap.servers", self.kafka_conf.url) \
        #     .option("subscribe", self.kafka_conf.consume_topic) \
        #     .option("auto.offset.reset", "earliest") \
        #     .load()

        # .option("kafka.partition.assignment.strategy", "range") \
        # .option("security.protocol", "SSL") \
        # .option("ssl.truststore.location",
        # "/deflect-analytics-ecosystem/
        # containers/kafka/local_cert/client.truststore.jks") \
        # .option("ssl.truststore.password", "kafkadocker") \
        # .option("ssl.keystore.location",
        # "/deflect-analytics-ecosystem/containers/kafka/
        # local_cert/kafka.server.keystore.jks") \
        # .option("ssl.keystore.password", "kafkadocker") \
        # .option("ssl.key.password", "kafkadocker") \
        # .load()

        # .option("subscribePattern", self.kafka_conf.consume_topic) \

        # kafkaStream = self.spark \
        #     .readStream \
        #     .format("kafka") \
        #     .option("kafka.bootstrap.servers", self.kafka_conf.zookeeper) \
        #     .option("startingOffsets", "earliest") \
        #     .option("subscribe", self.kafka_conf.consume_topic) \
        #     .option("auto.offset.reset", "earliest") \
        #     .option("security.protocol", "SSL") \
        #     .option("ssl.truststore.location",
        #             "/deflect-analytics-ecosystem/containers/
        #             kafka/local_cert/client.truststore.jks") \
        #     .option("ssl.truststore.password", "kafkadocker") \
        #     .option("ssl.keystore.location",
        #             "/deflect-analytics-ecosystem/
        #             containers/kafka/local_cert/kafka.server.keystore.jks") \
        #     .option("ssl.keystore.password", "kafkadocker") \
        #     .option("ssl.key.password", "kafkadocker") \
        #     .load()
        #
        # import pyspark.sql.functions as F
        # df1 = kafkaStream.selectExpr("CAST(value AS STRING)",
        #                     "CAST(timestamp AS TIMESTAMP)"
        #       ).select(F.from_json("value", self.data_parser.schema))
        #
        # q = df1.writeStream.format("console") \
        #         .option("truncate","false")\
        #         .start()\
        #         .awaitTermination()
        # # .option("kafka.partition.assignment.strategy", "range") \
        #
        # NOTE: make sure kafka 8 jar is in the spark.jars, won't work with 10
        # https://elephant.tech/spark-2-0-streaming-from-ssl-kafka-with-hdp-2-4/  # noqa

        # topicPartion = TopicAndPartition(self.kafka_conf.consume_topic, 0)

        kafkaStream = KafkaUtils.createDirectStream(
            self.ssc,
            [self.kafka_conf.logs_topic],
            {
                # 'bootstrap.servers': self.kafka_conf.zookeeper,
                'metadata.broker.list': self.kafka_conf.url,
                # "kafka.sasl.kerberos.service.name": "kafka",
                # "kafka.sasl.kerberos.service.name":
                # "/usr/lib/jvm/jdk1.8.0_162/jre/lib/security/cacerts",
                'group.id': self.kafka_conf.consume_group,
                # 'auto.offset.reset': 'largest',
                # 'security.protocol': self.kafka_conf.security_protocol,
                # "kafka.ssl.truststore.location":
                # self.kafka_conf.ssl_truststore_location,
                # "kafka.ssl.truststore.password":
                # self.kafka_conf.ssl_truststore_password
            }
            # fromOffset={topicPartion: int(0)}
        )

        # readDF = kafkaStream.selectExpr("CAST(key AS STRING)",
        #                            "CAST(value AS STRING)")

        # readDF.show()

        # query = readDF.writeStream.format("console").start()
        # import time
        # time.sleep(10)  # sleep 10 seconds
        # query.stop()
        # windowed_stream = kafkaStream.withWatermark("@timestamp",
        # f"{self.engine_conf.time_bucket} seconds").selectExpr(
        # "CAST(value AS STRING)",
        # "CAST(timestamp AS TIMESTAMP)").toDF('log', 'timestamp')
        #
        # q = kafkaStream.writeStream \
        #     .format("console") \
        #     .option("truncate", "false")

        # q = windowed_stream.count()
        # print(q)
        #
        # windowed_stream.start().awaitTermination()

        def process_subsets(time, rdd):
            self.logger.info('Data until {}'.format(time))
            if not rdd.isEmpty():
                # set dataframe to process later on
                self.logs_df = rdd
                self._get_df()
                self.remaining_steps = list(self.step_to_action.keys())

                for step, action in self.step_to_action.items():
                    self.logger.info('Starting step {}'.format(step))
                    action()
                    self.logger.info('Completed step {}'.format(step))
                    self.remaining_steps.remove(step)
            else:
                self.logger.info('Empty RDD...')
            self.reset()

        kafkaStream.foreachRDD(process_subsets)

        self.ssc.start()

        self.ssc.awaitTermination()</code></pre>
       </details>
       <h3>
        Ancestors
       </h3>
       <ul class="hlist">
        <li>
         <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
          SparkPipelineBase
         </a>
        </li>
        <li>
         <a href="base.html#baskerville.models.base.PipelineBase" title="baskerville.models.base.PipelineBase">
          PipelineBase
         </a>
        </li>
       </ul>
       <h3>
        Inherited members
       </h3>
       <ul class="hlist">
        <li>
         <code>
          <b>
           <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
            SparkPipelineBase
           </a>
          </b>
         </code>
         :
         <ul class="hlist">
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_cache_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns">
             add_cache_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_calc_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns">
             add_calc_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns">
             add_post_groupby_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.create_runtime" title="baskerville.models.base_spark.SparkPipelineBase.create_runtime">
             create_runtime
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.cross_reference" title="baskerville.models.base_spark.SparkPipelineBase.cross_reference">
             cross_reference
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_calculation" title="baskerville.models.base_spark.SparkPipelineBase.feature_calculation">
             feature_calculation
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_extraction" title="baskerville.models.base_spark.SparkPipelineBase.feature_extraction">
             feature_extraction
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.feature_update" title="baskerville.models.base_spark.SparkPipelineBase.feature_update">
             feature_update
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.features_to_dict" title="baskerville.models.base_spark.SparkPipelineBase.features_to_dict">
             features_to_dict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_cache" title="baskerville.models.base_spark.SparkPipelineBase.filter_cache">
             filter_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.filter_columns" title="baskerville.models.base_spark.SparkPipelineBase.filter_columns">
             filter_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.finish_up" title="baskerville.models.base_spark.SparkPipelineBase.finish_up">
             finish_up
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by" title="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by">
             get_columns_to_filter_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_data" title="baskerville.models.base_spark.SparkPipelineBase.get_data">
             get_data
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs" title="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs">
             get_group_by_aggs
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations" title="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations">
             get_post_group_by_calculations
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.group_by" title="baskerville.models.base_spark.SparkPipelineBase.group_by">
             group_by
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns" title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns">
             handle_missing_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.initialize" title="baskerville.models.base_spark.SparkPipelineBase.initialize">
             initialize
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session" title="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session">
             instantiate_spark_session
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.label_or_predict" title="baskerville.models.base_spark.SparkPipelineBase.label_or_predict">
             label_or_predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.load_test" title="baskerville.models.base_spark.SparkPipelineBase.load_test">
             load_test
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.normalize_host_names" title="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names">
             normalize_host_names
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.predict" title="baskerville.models.base_spark.SparkPipelineBase.predict">
             predict
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.preprocessing" title="baskerville.models.base_spark.SparkPipelineBase.preprocessing">
             preprocessing
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.process_data" title="baskerville.models.base_spark.SparkPipelineBase.process_data">
             process_data
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.refresh_cache" title="baskerville.models.base_spark.SparkPipelineBase.refresh_cache">
             refresh_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.rename_columns" title="baskerville.models.base_spark.SparkPipelineBase.rename_columns">
             rename_columns
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.reset" title="baskerville.models.base_spark.SparkPipelineBase.reset">
             reset
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.run" title="baskerville.models.base_spark.SparkPipelineBase.run">
             run
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save" title="baskerville.models.base_spark.SparkPipelineBase.save">
             save
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.save_df_to_table" title="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table">
             save_df_to_table
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache" title="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache">
             set_up_request_set_cache
            </a>
           </code>
          </li>
          <li>
           <code>
            <a href="base_spark.html#baskerville.models.base_spark.SparkPipelineBase.trigger_challenge" title="baskerville.models.base_spark.SparkPipelineBase.trigger_challenge">
             trigger_challenge
            </a>
           </code>
          </li>
         </ul>
        </li>
       </ul>
      </dd>
     </dl>
    </section>
   </article>
   <nav id="sidebar">
    <h1>
     Index
    </h1>
    <div class="toc">
     <ul>
     </ul>
    </div>
    <ul id="index">
     <li>
      <h3>
       Super-module
      </h3>
      <ul>
       <li>
        <code>
         <a href="index.html" title="baskerville.models">
          baskerville.models
         </a>
        </code>
       </li>
      </ul>
     </li>
     <li>
      <h3>
       <a href="#header-classes">
        Classes
       </a>
      </h3>
      <ul>
       <li>
        <h4>
         <code>
          <a href="#baskerville.models.pipelines.ElasticsearchPipeline" title="baskerville.models.pipelines.ElasticsearchPipeline">
           ElasticsearchPipeline
          </a>
         </code>
        </h4>
        <ul class="">
         <li>
          <code>
           <a href="#baskerville.models.pipelines.ElasticsearchPipeline.es_session_getter" title="baskerville.models.pipelines.ElasticsearchPipeline.es_session_getter">
            es_session_getter
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.pipelines.ElasticsearchPipeline.initialize" title="baskerville.models.pipelines.ElasticsearchPipeline.initialize">
            initialize
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.pipelines.ElasticsearchPipeline.save_logs" title="baskerville.models.pipelines.ElasticsearchPipeline.save_logs">
            save_logs
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.pipelines.ElasticsearchPipeline.set_up_es" title="baskerville.models.pipelines.ElasticsearchPipeline.set_up_es">
            set_up_es
           </a>
          </code>
         </li>
        </ul>
       </li>
       <li>
        <h4>
         <code>
          <a href="#baskerville.models.pipelines.KafkaPipeline" title="baskerville.models.pipelines.KafkaPipeline">
           KafkaPipeline
          </a>
         </code>
        </h4>
       </li>
       <li>
        <h4>
         <code>
          <a href="#baskerville.models.pipelines.RawLogPipeline" title="baskerville.models.pipelines.RawLogPipeline">
           RawLogPipeline
          </a>
         </code>
        </h4>
        <ul class="">
         <li>
          <code>
           <a href="#baskerville.models.pipelines.RawLogPipeline.get_data" title="baskerville.models.pipelines.RawLogPipeline.get_data">
            get_data
           </a>
          </code>
         </li>
        </ul>
       </li>
       <li>
        <h4>
         <code>
          <a href="#baskerville.models.pipelines.SparkStructuredStreamingRealTimePipeline" title="baskerville.models.pipelines.SparkStructuredStreamingRealTimePipeline">
           SparkStructuredStreamingRealTimePipeline
          </a>
         </code>
        </h4>
       </li>
      </ul>
     </li>
    </ul>
   </nav>
  </main>
  <footer id="footer">
   <p>
    Generated by
    <a href="https://pdoc3.github.io/pdoc">
     <cite>
      pdoc
     </cite>
     0.7.2
    </a>
    .
   </p>
   <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">
    <img alt="Creative Commons Licence" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"/>
   </a>
   <br/>
   This work is copyright (c) 2020, eQualit.ie inc., and is licensed under a
   <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">
    Creative Commons Attribution 4.0 International License
   </a>
   .
  </footer>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad()
  </script>
 </body>
</html>