<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, minimum-scale=1" name="viewport"/>
  <meta content="pdoc 0.7.2" name="generator"/>
  <title>
   baskerville.models.request_set_cache API documentation
  </title>
  <meta content="" name="description"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet"/>
  <style>
   .flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}
  </style>
  <style media="screen and (min-width: 700px)">
   @media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}
  </style>
  <style media="print">
   @media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}
  </style>
 </head>
 <body>
  <main>
   <article id="content">
    <header>
     <h1 class="title">
      Module
      <code>
       baskerville.models.request_set_cache
      </code>
     </h1>
    </header>
    <section id="section-intro">
     <details class="source">
      <summary>
       <span>
        Expand source code
       </span>
      </summary>
      <pre><code class="python"># Copyright (c) 2020, eQualit.ie inc.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.


import datetime
import gc
import os

from py4j.protocol import Py4JJavaError

from baskerville.spark import get_spark_session
from baskerville.spark.helpers import StorageLevel
from pyspark.sql import functions as F

from baskerville.util.file_manager import FileManager
from baskerville.util.helpers import get_logger, Singleton


class RequestSetSparkCache(Singleton):

    def __init__(
            self,
            cache_config,
            table_name,
            columns_to_keep,
            expire_if_longer_than=3600,
            logger=None,
            session_getter=get_spark_session,
            group_by_fields=('target', 'ip'),
            format_='parquet',
            path='request_set_cache'
    ):
        self.__cache = None
        self.__persistent_cache = None
        self.schema = None
        self.cache_config = cache_config
        self.table_name = table_name
        self.columns_to_keep = columns_to_keep
        self.expire_if_longer_than = expire_if_longer_than
        self.logger = logger if logger else get_logger(self.__class__.__name__)
        self.session_getter = session_getter
        self.group_by_fields = group_by_fields
        self.format_ = format_
        self.storage_level = StorageLevel.CUSTOM
        self.column_renamings = {
            'first_ever_request': 'start',
            'old_subset_count': 'subset_count',
            'old_features': 'features',
                            'old_num_requests': 'num_requests',
        }
        self._count = 0
        self._last_updated = datetime.datetime.utcnow()
        self._changed = False
        self.file_manager = FileManager(path, self.session_getter())

        self.file_name = os.path.join(
            path, f'{self.__class__.__name__}.{self.format_}')
        self.temp_file_name = os.path.join(
            path, f'{self.__class__.__name__}temp.{self.format_}')

        if self.file_manager.path_exists(self.file_name):
            self.file_manager.delete_path(self.file_name)
        if self.file_manager.path_exists(self.temp_file_name):
            self.file_manager.delete_path(self.temp_file_name)

    @property
    def cache(self):
        return self.__cache

    @property
    def persistent_cache(self):
        return self.__persistent_cache

    @property
    def persistent_cache_file(self):
        return self.file_name

    def _get_load_q(self):
        return f'''(SELECT *
                    from {self.table_name}
                    where id in (select max(id)
                    from {self.table_name}
                    group by {', '.join(self.group_by_fields)} )
                    ) as {self.table_name}'''

    def options(self, **kwargs):
        self.options = kwargs
        return self

    def _load(self, update_date=None, hosts=None, extra_filters=None):
        """
        Loads the request_sets already in the database
        :return:
        :rtype: pyspark.sql.Dataframe
        """
        where = ()
        if update_date:
            where = (
                    (F.col("updated_at") &gt;= update_date) |
                    (F.col("created_at") &gt;= update_date)
            )
        if not isinstance(where, F.Column):
            where = (F.col('id').isNotNull())

        if isinstance(extra_filters, F.Column):
            where = where &amp; extra_filters

        spark = self.session_getter()

        df = spark.read.format('jdbc').options(
            url=self.cache_config['db_url'],
            driver=self.cache_config['db_driver'],
            dbtable=self._get_load_q(),
            user=self.cache_config['user'],
            password=self.cache_config['password'],
            fetchsize=1000,
            max_connections=200,
        ).load(
        ).where(
            where
        ).select(
            *self.columns_to_keep
        )

        if hosts is not None:
            df = df.join(F.broadcast(hosts), ['target'], 'leftsemi')

        self._changed = True

        return df

    def write(self):
        """
        Write persistent cache to file
        :return: None
        """
        self.__cache.write.mode('overwrite').partitionBy(
            *self.group_by_fields
        ).format(self.format_).save(self.persistent_cache_file)

    def load(self, update_date=None, hosts=None, extra_filters=None):
        """
        Load cache from database and store it in the configured file format
        :param update_date:
        :param hosts:
        :param extra_filters:
        :return:
        """
        self.__cache = self._load(
            update_date=update_date,
            hosts=hosts,
            extra_filters=extra_filters
        ).persist(self.storage_level)

        self.write()

        return self

    def load_empty(self, schema):
        """
        Instantiate an empty cache from the specified schema
        :param schema:
        :return:
        """
        self.schema = schema
        spark = self.session_getter()
        self.__cache = spark.createDataFrame([], schema)
        self.__persistent_cache = spark.createDataFrame([], schema)

    def append(self, df):
        self.__cache = self.__cache.union(
            df.select(*self.columns_to_keep)
        )

        return self

    def update_cache(self, df):
        self.__cache = self.__cache.select('*').where(
            F.col('id') not in df.select('id').distinct()
        )

        self.__cache = self.append(df)

        return self

    def update_df(
            self, df_to_update, join_cols=('target', 'ip'), select_cols=('*',)
    ):
        self._changed = True

        if "*" in select_cols:
            select_cols = self.cache.columns

        # add null columns if nothing in cache
        if self.count() == 0:
            for c in select_cols:
                if c not in df_to_update.columns:
                    df_to_update = df_to_update.withColumn(c, F.lit(None))
            return df_to_update

        # https://issues.apache.org/jira/browse/SPARK-10925
        df = df_to_update.rdd.toDF(df_to_update.schema).alias('a').join(
            F.broadcast(self.cache.select(*select_cols).alias('cache')),
            list(join_cols),
            how='left_outer'
        ).persist(self.storage_level)

        # update nulls and filter drop duplicate columns
        for c in select_cols:
            # if we have duplicate columns, take the new column as truth
            # (the cache)
            if df.columns.count(c) &gt; 1:
                if c not in join_cols:
                    df_col = f'a.{c}'
                    cache_col = f'cache.{c}'
                    renamed_col = f'renamed_{c}'

                    df = df.withColumn(
                        renamed_col,
                        F.when(
                            F.col(df_col).isNull(), F.col(cache_col)
                        ).otherwise(F.col(df_col))
                    )
                    df = df.select(*[i for i in df.columns
                                     if i not in [cache_col, df_col, c]
                                     ])
                    df = df.withColumnRenamed(renamed_col, c)
        df.unpersist(blocking=True)
        return df

    def filter_by(self, df, columns=None):
        """

        :param df:
        :param columns:
        :return:
        """
        import os
        if not columns:
            columns = df.columns

        if os.path.isdir(self.persistent_cache_file):
            self.__cache = self.session_getter().read.format(
                self.format_
            ).load(self.persistent_cache_file).join(
                F.broadcast(df),
                on=columns,
                how='inner'
            ).drop(
                'a.ip'
            ).persist(self.storage_level)
        else:
            if self.__cache:
                self.__cache = self.__cache.join(
                    F.broadcast(df),
                    on=columns,
                    how='inner'
                ).drop(
                    'a.ip'
                ).persist(self.storage_level)
            else:
                self.load_empty(self.schema)

    def update_self(
            self,
            source_df,
            join_cols=('target', 'ip'),
            select_cols=('*',),
            expire=True
    ):
        """

        :param source_df:
        :param join_cols:
        :param select_cols:
        :param expire:
        :return:
        """
        # if os.path.exists(self.persistent_cache_file):
        #     shutil.rmtree(self.persistent_cache_file)
        #     # time.sleep(1)

        to_drop = [
            'prediction', 'r', 'score', 'to_update', 'id', 'id_runtime',
            'features', 'start', 'stop', 'subset_count', 'num_requests',
            'total_seconds', 'time_bucket', 'model_version', 'to_update',
            'label', 'id_attribute'
        ]
        now = datetime.datetime.utcnow()
        source_df = source_df.persist(self.storage_level).alias('sd')

        self.logger.debug(f'Source_df count = {source_df.count()}')

        # read the whole thing again
        if self.file_manager.path_exists(self.file_name):
            self.__persistent_cache = self.session_getter().read.format(
                self.format_
            ).load(
                self.file_name
            ).persist(self.storage_level)

        # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/
        self.__persistent_cache = F.broadcast(
            source_df.rdd.toDF(source_df.schema)
        ).join(
            self.__persistent_cache.select(*select_cols).alias('pc'),
            list(join_cols),
            how='full_outer'
        ).persist(self.storage_level)

        # mark rows to update
        self.__persistent_cache = self.__persistent_cache.withColumn(
            'to_update',
            F.col('features').isNotNull()
        )

        # update cache columns
        for cache_col, df_col in self.column_renamings.items():
            self.__persistent_cache = self.__persistent_cache.withColumn(
                cache_col,
                F.when(
                    F.col('to_update') == True, F.col(df_col)  # noqa
                ).otherwise(
                    F.when(
                        F.col(cache_col).isNotNull(),
                        F.col(cache_col)
                    )
                )
            )
        self.__persistent_cache = self.__persistent_cache.withColumn(
            'updated_at',
            F.when(
                F.col('to_update') == True, now  # noqa
            ).otherwise(F.col('updated_at'))
        )
        # drop cols that do not belong in the cache
        self.__persistent_cache = self.__persistent_cache.drop(*to_drop)

        # remove old rows
        if expire:
            update_date = now - datetime.timedelta(
                seconds=self.expire_if_longer_than
            )
            self.__persistent_cache = self.__persistent_cache.select(
                '*'
            ).where(F.col('updated_at') &gt;= update_date)

        # write back to parquet - different file/folder though
        # because self.parquet_name is already in use
        # rename temp to self.parquet_name
        if self.file_manager.path_exists(self.temp_file_name):
            self.file_manager.delete_path(self.temp_file_name)

        self.__persistent_cache.write.mode(
            'overwrite'
        ).format(
            self.format_
        ).save(self.temp_file_name)

        self.logger.debug(
            f'# Number of rows in persistent cache: '
            f'{self.__persistent_cache.count()}'
        )

        # we don't need anything in memory anymore
        source_df.unpersist(blocking=True)
        source_df = None
        del source_df
        self.empty_all()

        # rename temp to self.parquet_name
        if self.file_manager.path_exists(self.file_name):
            self.file_manager.delete_path(self.file_name)

        self.file_manager.rename_path(self.temp_file_name, self.file_name)

    def refresh(self, update_date, hosts, extra_filters=None):
        df = self._load(
            update_date=update_date, hosts=hosts, extra_filters=extra_filters
        )

        self.append(
            df
        ).deduplicate()

        return self

    def deduplicate(self):
        self.__cache = self.__cache.dropDuplicates()
        # self._count = self.cache.count()
        # self._last_updated = datetime.datetime.now()
        # self._changed = False

    def alias(self, name):
        self.__cache = self.__cache.alias(name)
        return self

    def show(self, n=20, t=False):
        self.__cache.show(n, t)

    def select(self, what):
        return self.__cache.select(what)

    def count(self):
        if self.__cache:
            try:
                return self.cache.count()
            except Py4JJavaError:
                import traceback
                traceback.print_exc()
                self.logger.debug(
                    'Just hit the cache issue.. trying to refresh')
                # self.cache.createOrReplaceTempView("current_cache")
                # self.session_getter().catalog.refreshTable("current_cache")
                return self.cache.count()

        return 0

    def clean(self, now, expire_if_longer_than=3600):
        """
        Remove request_sets with seconds since update &gt; expire_if_longer_than
        :param datetime.datetime now: utc datetime
        :param int expire_if_longer_than: seconds
        :param bool allow_null: allow request_sets with null update_date (newly
        created)
        :return: None
        """
        update_date = now - datetime.timedelta(seconds=expire_if_longer_than)
        self.__cache = self.__cache.select('*').where((
            (F.col("updated_at") &gt;= F.lit(update_date)) |
            (F.col("created_at") &gt;= F.lit(update_date))
        ))

        return self

    def empty(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        self.__cache = None

    def empty_all(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        if self.__persistent_cache is not None:
            self.__persistent_cache.unpersist(blocking=True)

        self.__cache = None
        self.__persistent_cache = None
        gc.collect()
        self.session_getter().sparkContext._jvm.System.gc()

    def persist(self):
        self.__cache = self.__cache.persist(self.storage_level)
        # self.__cache.createOrReplaceTempView(self.__class__.__name__)
        # spark = self.session_getter()
        # spark.catalog.cacheTable(self.__class__.__name__)

    def __len__(self):
        return self.count()</code></pre>
     </details>
    </section>
    <section>
    </section>
    <section>
    </section>
    <section>
    </section>
    <section>
     <h2 class="section-title" id="header-classes">
      Classes
     </h2>
     <dl>
      <dt id="baskerville.models.request_set_cache.RequestSetSparkCache">
       <code class="flex name class">
        <span>
         class
         <span class="ident">
          RequestSetSparkCache
         </span>
        </span>
        <span>
         (
        </span>
        <span>
         cache_config, table_name, columns_to_keep, expire_if_longer_than=3600, logger=None, session_getter=&lt;function get_spark_session&gt;, group_by_fields=('target', 'ip'), format_='parquet', path='request_set_cache')
        </span>
       </code>
      </dt>
      <dd>
       <section class="desc">
       </section>
       <details class="source">
        <summary>
         <span>
          Expand source code
         </span>
        </summary>
        <pre><code class="python">class RequestSetSparkCache(Singleton):

    def __init__(
            self,
            cache_config,
            table_name,
            columns_to_keep,
            expire_if_longer_than=3600,
            logger=None,
            session_getter=get_spark_session,
            group_by_fields=('target', 'ip'),
            format_='parquet',
            path='request_set_cache'
    ):
        self.__cache = None
        self.__persistent_cache = None
        self.schema = None
        self.cache_config = cache_config
        self.table_name = table_name
        self.columns_to_keep = columns_to_keep
        self.expire_if_longer_than = expire_if_longer_than
        self.logger = logger if logger else get_logger(self.__class__.__name__)
        self.session_getter = session_getter
        self.group_by_fields = group_by_fields
        self.format_ = format_
        self.storage_level = StorageLevel.CUSTOM
        self.column_renamings = {
            'first_ever_request': 'start',
            'old_subset_count': 'subset_count',
            'old_features': 'features',
                            'old_num_requests': 'num_requests',
        }
        self._count = 0
        self._last_updated = datetime.datetime.utcnow()
        self._changed = False
        self.file_manager = FileManager(path, self.session_getter())

        self.file_name = os.path.join(
            path, f'{self.__class__.__name__}.{self.format_}')
        self.temp_file_name = os.path.join(
            path, f'{self.__class__.__name__}temp.{self.format_}')

        if self.file_manager.path_exists(self.file_name):
            self.file_manager.delete_path(self.file_name)
        if self.file_manager.path_exists(self.temp_file_name):
            self.file_manager.delete_path(self.temp_file_name)

    @property
    def cache(self):
        return self.__cache

    @property
    def persistent_cache(self):
        return self.__persistent_cache

    @property
    def persistent_cache_file(self):
        return self.file_name

    def _get_load_q(self):
        return f'''(SELECT *
                    from {self.table_name}
                    where id in (select max(id)
                    from {self.table_name}
                    group by {', '.join(self.group_by_fields)} )
                    ) as {self.table_name}'''

    def options(self, **kwargs):
        self.options = kwargs
        return self

    def _load(self, update_date=None, hosts=None, extra_filters=None):
        """
        Loads the request_sets already in the database
        :return:
        :rtype: pyspark.sql.Dataframe
        """
        where = ()
        if update_date:
            where = (
                    (F.col("updated_at") &gt;= update_date) |
                    (F.col("created_at") &gt;= update_date)
            )
        if not isinstance(where, F.Column):
            where = (F.col('id').isNotNull())

        if isinstance(extra_filters, F.Column):
            where = where &amp; extra_filters

        spark = self.session_getter()

        df = spark.read.format('jdbc').options(
            url=self.cache_config['db_url'],
            driver=self.cache_config['db_driver'],
            dbtable=self._get_load_q(),
            user=self.cache_config['user'],
            password=self.cache_config['password'],
            fetchsize=1000,
            max_connections=200,
        ).load(
        ).where(
            where
        ).select(
            *self.columns_to_keep
        )

        if hosts is not None:
            df = df.join(F.broadcast(hosts), ['target'], 'leftsemi')

        self._changed = True

        return df

    def write(self):
        """
        Write persistent cache to file
        :return: None
        """
        self.__cache.write.mode('overwrite').partitionBy(
            *self.group_by_fields
        ).format(self.format_).save(self.persistent_cache_file)

    def load(self, update_date=None, hosts=None, extra_filters=None):
        """
        Load cache from database and store it in the configured file format
        :param update_date:
        :param hosts:
        :param extra_filters:
        :return:
        """
        self.__cache = self._load(
            update_date=update_date,
            hosts=hosts,
            extra_filters=extra_filters
        ).persist(self.storage_level)

        self.write()

        return self

    def load_empty(self, schema):
        """
        Instantiate an empty cache from the specified schema
        :param schema:
        :return:
        """
        self.schema = schema
        spark = self.session_getter()
        self.__cache = spark.createDataFrame([], schema)
        self.__persistent_cache = spark.createDataFrame([], schema)

    def append(self, df):
        self.__cache = self.__cache.union(
            df.select(*self.columns_to_keep)
        )

        return self

    def update_cache(self, df):
        self.__cache = self.__cache.select('*').where(
            F.col('id') not in df.select('id').distinct()
        )

        self.__cache = self.append(df)

        return self

    def update_df(
            self, df_to_update, join_cols=('target', 'ip'), select_cols=('*',)
    ):
        self._changed = True

        if "*" in select_cols:
            select_cols = self.cache.columns

        # add null columns if nothing in cache
        if self.count() == 0:
            for c in select_cols:
                if c not in df_to_update.columns:
                    df_to_update = df_to_update.withColumn(c, F.lit(None))
            return df_to_update

        # https://issues.apache.org/jira/browse/SPARK-10925
        df = df_to_update.rdd.toDF(df_to_update.schema).alias('a').join(
            F.broadcast(self.cache.select(*select_cols).alias('cache')),
            list(join_cols),
            how='left_outer'
        ).persist(self.storage_level)

        # update nulls and filter drop duplicate columns
        for c in select_cols:
            # if we have duplicate columns, take the new column as truth
            # (the cache)
            if df.columns.count(c) &gt; 1:
                if c not in join_cols:
                    df_col = f'a.{c}'
                    cache_col = f'cache.{c}'
                    renamed_col = f'renamed_{c}'

                    df = df.withColumn(
                        renamed_col,
                        F.when(
                            F.col(df_col).isNull(), F.col(cache_col)
                        ).otherwise(F.col(df_col))
                    )
                    df = df.select(*[i for i in df.columns
                                     if i not in [cache_col, df_col, c]
                                     ])
                    df = df.withColumnRenamed(renamed_col, c)
        df.unpersist(blocking=True)
        return df

    def filter_by(self, df, columns=None):
        """

        :param df:
        :param columns:
        :return:
        """
        import os
        if not columns:
            columns = df.columns

        if os.path.isdir(self.persistent_cache_file):
            self.__cache = self.session_getter().read.format(
                self.format_
            ).load(self.persistent_cache_file).join(
                F.broadcast(df),
                on=columns,
                how='inner'
            ).drop(
                'a.ip'
            ).persist(self.storage_level)
        else:
            if self.__cache:
                self.__cache = self.__cache.join(
                    F.broadcast(df),
                    on=columns,
                    how='inner'
                ).drop(
                    'a.ip'
                ).persist(self.storage_level)
            else:
                self.load_empty(self.schema)

    def update_self(
            self,
            source_df,
            join_cols=('target', 'ip'),
            select_cols=('*',),
            expire=True
    ):
        """

        :param source_df:
        :param join_cols:
        :param select_cols:
        :param expire:
        :return:
        """
        # if os.path.exists(self.persistent_cache_file):
        #     shutil.rmtree(self.persistent_cache_file)
        #     # time.sleep(1)

        to_drop = [
            'prediction', 'r', 'score', 'to_update', 'id', 'id_runtime',
            'features', 'start', 'stop', 'subset_count', 'num_requests',
            'total_seconds', 'time_bucket', 'model_version', 'to_update',
            'label', 'id_attribute'
        ]
        now = datetime.datetime.utcnow()
        source_df = source_df.persist(self.storage_level).alias('sd')

        self.logger.debug(f'Source_df count = {source_df.count()}')

        # read the whole thing again
        if self.file_manager.path_exists(self.file_name):
            self.__persistent_cache = self.session_getter().read.format(
                self.format_
            ).load(
                self.file_name
            ).persist(self.storage_level)

        # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/
        self.__persistent_cache = F.broadcast(
            source_df.rdd.toDF(source_df.schema)
        ).join(
            self.__persistent_cache.select(*select_cols).alias('pc'),
            list(join_cols),
            how='full_outer'
        ).persist(self.storage_level)

        # mark rows to update
        self.__persistent_cache = self.__persistent_cache.withColumn(
            'to_update',
            F.col('features').isNotNull()
        )

        # update cache columns
        for cache_col, df_col in self.column_renamings.items():
            self.__persistent_cache = self.__persistent_cache.withColumn(
                cache_col,
                F.when(
                    F.col('to_update') == True, F.col(df_col)  # noqa
                ).otherwise(
                    F.when(
                        F.col(cache_col).isNotNull(),
                        F.col(cache_col)
                    )
                )
            )
        self.__persistent_cache = self.__persistent_cache.withColumn(
            'updated_at',
            F.when(
                F.col('to_update') == True, now  # noqa
            ).otherwise(F.col('updated_at'))
        )
        # drop cols that do not belong in the cache
        self.__persistent_cache = self.__persistent_cache.drop(*to_drop)

        # remove old rows
        if expire:
            update_date = now - datetime.timedelta(
                seconds=self.expire_if_longer_than
            )
            self.__persistent_cache = self.__persistent_cache.select(
                '*'
            ).where(F.col('updated_at') &gt;= update_date)

        # write back to parquet - different file/folder though
        # because self.parquet_name is already in use
        # rename temp to self.parquet_name
        if self.file_manager.path_exists(self.temp_file_name):
            self.file_manager.delete_path(self.temp_file_name)

        self.__persistent_cache.write.mode(
            'overwrite'
        ).format(
            self.format_
        ).save(self.temp_file_name)

        self.logger.debug(
            f'# Number of rows in persistent cache: '
            f'{self.__persistent_cache.count()}'
        )

        # we don't need anything in memory anymore
        source_df.unpersist(blocking=True)
        source_df = None
        del source_df
        self.empty_all()

        # rename temp to self.parquet_name
        if self.file_manager.path_exists(self.file_name):
            self.file_manager.delete_path(self.file_name)

        self.file_manager.rename_path(self.temp_file_name, self.file_name)

    def refresh(self, update_date, hosts, extra_filters=None):
        df = self._load(
            update_date=update_date, hosts=hosts, extra_filters=extra_filters
        )

        self.append(
            df
        ).deduplicate()

        return self

    def deduplicate(self):
        self.__cache = self.__cache.dropDuplicates()
        # self._count = self.cache.count()
        # self._last_updated = datetime.datetime.now()
        # self._changed = False

    def alias(self, name):
        self.__cache = self.__cache.alias(name)
        return self

    def show(self, n=20, t=False):
        self.__cache.show(n, t)

    def select(self, what):
        return self.__cache.select(what)

    def count(self):
        if self.__cache:
            try:
                return self.cache.count()
            except Py4JJavaError:
                import traceback
                traceback.print_exc()
                self.logger.debug(
                    'Just hit the cache issue.. trying to refresh')
                # self.cache.createOrReplaceTempView("current_cache")
                # self.session_getter().catalog.refreshTable("current_cache")
                return self.cache.count()

        return 0

    def clean(self, now, expire_if_longer_than=3600):
        """
        Remove request_sets with seconds since update &gt; expire_if_longer_than
        :param datetime.datetime now: utc datetime
        :param int expire_if_longer_than: seconds
        :param bool allow_null: allow request_sets with null update_date (newly
        created)
        :return: None
        """
        update_date = now - datetime.timedelta(seconds=expire_if_longer_than)
        self.__cache = self.__cache.select('*').where((
            (F.col("updated_at") &gt;= F.lit(update_date)) |
            (F.col("created_at") &gt;= F.lit(update_date))
        ))

        return self

    def empty(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        self.__cache = None

    def empty_all(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        if self.__persistent_cache is not None:
            self.__persistent_cache.unpersist(blocking=True)

        self.__cache = None
        self.__persistent_cache = None
        gc.collect()
        self.session_getter().sparkContext._jvm.System.gc()

    def persist(self):
        self.__cache = self.__cache.persist(self.storage_level)
        # self.__cache.createOrReplaceTempView(self.__class__.__name__)
        # spark = self.session_getter()
        # spark.catalog.cacheTable(self.__class__.__name__)

    def __len__(self):
        return self.count()</code></pre>
       </details>
       <h3>
        Ancestors
       </h3>
       <ul class="hlist">
        <li>
         <a href="../util/helpers.html#baskerville.util.helpers.Singleton" title="baskerville.util.helpers.Singleton">
          Singleton
         </a>
        </li>
       </ul>
       <h3>
        Instance variables
       </h3>
       <dl>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.cache">
         <code class="name">
          var
          <span class="ident">
           cache
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">@property
def cache(self):
    return self.__cache</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache">
         <code class="name">
          var
          <span class="ident">
           persistent_cache
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">@property
def persistent_cache(self):
    return self.__persistent_cache</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache_file">
         <code class="name">
          var
          <span class="ident">
           persistent_cache_file
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">@property
def persistent_cache_file(self):
    return self.file_name</code></pre>
         </details>
        </dd>
       </dl>
       <h3>
        Methods
       </h3>
       <dl>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.alias">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            alias
           </span>
          </span>
          (
          <span>
           self, name)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def alias(self, name):
    self.__cache = self.__cache.alias(name)
    return self</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.append">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            append
           </span>
          </span>
          (
          <span>
           self, df)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def append(self, df):
    self.__cache = self.__cache.union(
        df.select(*self.columns_to_keep)
    )

    return self</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.clean">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            clean
           </span>
          </span>
          (
          <span>
           self, now, expire_if_longer_than=3600)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Remove request_sets with seconds since update &gt; expire_if_longer_than
:param datetime.datetime now: utc datetime
:param int expire_if_longer_than: seconds
:param bool allow_null: allow request_sets with null update_date (newly
created)
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def clean(self, now, expire_if_longer_than=3600):
    """
    Remove request_sets with seconds since update &gt; expire_if_longer_than
    :param datetime.datetime now: utc datetime
    :param int expire_if_longer_than: seconds
    :param bool allow_null: allow request_sets with null update_date (newly
    created)
    :return: None
    """
    update_date = now - datetime.timedelta(seconds=expire_if_longer_than)
    self.__cache = self.__cache.select('*').where((
        (F.col("updated_at") &gt;= F.lit(update_date)) |
        (F.col("created_at") &gt;= F.lit(update_date))
    ))

    return self</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.count">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            count
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def count(self):
    if self.__cache:
        try:
            return self.cache.count()
        except Py4JJavaError:
            import traceback
            traceback.print_exc()
            self.logger.debug(
                'Just hit the cache issue.. trying to refresh')
            # self.cache.createOrReplaceTempView("current_cache")
            # self.session_getter().catalog.refreshTable("current_cache")
            return self.cache.count()

    return 0</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.deduplicate">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            deduplicate
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def deduplicate(self):
    self.__cache = self.__cache.dropDuplicates()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.empty">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            empty
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def empty(self):
    if self.__cache is not None:
        self.__cache.unpersist(blocking=True)
    self.__cache = None</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.empty_all">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            empty_all
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def empty_all(self):
    if self.__cache is not None:
        self.__cache.unpersist(blocking=True)
    if self.__persistent_cache is not None:
        self.__persistent_cache.unpersist(blocking=True)

    self.__cache = None
    self.__persistent_cache = None
    gc.collect()
    self.session_getter().sparkContext._jvm.System.gc()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.filter_by">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            filter_by
           </span>
          </span>
          (
          <span>
           self, df, columns=None)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           :param df:
:param columns:
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def filter_by(self, df, columns=None):
    """

    :param df:
    :param columns:
    :return:
    """
    import os
    if not columns:
        columns = df.columns

    if os.path.isdir(self.persistent_cache_file):
        self.__cache = self.session_getter().read.format(
            self.format_
        ).load(self.persistent_cache_file).join(
            F.broadcast(df),
            on=columns,
            how='inner'
        ).drop(
            'a.ip'
        ).persist(self.storage_level)
    else:
        if self.__cache:
            self.__cache = self.__cache.join(
                F.broadcast(df),
                on=columns,
                how='inner'
            ).drop(
                'a.ip'
            ).persist(self.storage_level)
        else:
            self.load_empty(self.schema)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.load">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            load
           </span>
          </span>
          (
          <span>
           self, update_date=None, hosts=None, extra_filters=None)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Load cache from database and store it in the configured file format
:param update_date:
:param hosts:
:param extra_filters:
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def load(self, update_date=None, hosts=None, extra_filters=None):
    """
    Load cache from database and store it in the configured file format
    :param update_date:
    :param hosts:
    :param extra_filters:
    :return:
    """
    self.__cache = self._load(
        update_date=update_date,
        hosts=hosts,
        extra_filters=extra_filters
    ).persist(self.storage_level)

    self.write()

    return self</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.load_empty">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            load_empty
           </span>
          </span>
          (
          <span>
           self, schema)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Instantiate an empty cache from the specified schema
:param schema:
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def load_empty(self, schema):
    """
    Instantiate an empty cache from the specified schema
    :param schema:
    :return:
    """
    self.schema = schema
    spark = self.session_getter()
    self.__cache = spark.createDataFrame([], schema)
    self.__persistent_cache = spark.createDataFrame([], schema)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.options">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            options
           </span>
          </span>
          (
          <span>
           self, **kwargs)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def options(self, **kwargs):
    self.options = kwargs
    return self</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.persist">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            persist
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def persist(self):
    self.__cache = self.__cache.persist(self.storage_level)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.refresh">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            refresh
           </span>
          </span>
          (
          <span>
           self, update_date, hosts, extra_filters=None)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def refresh(self, update_date, hosts, extra_filters=None):
    df = self._load(
        update_date=update_date, hosts=hosts, extra_filters=extra_filters
    )

    self.append(
        df
    ).deduplicate()

    return self</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.select">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            select
           </span>
          </span>
          (
          <span>
           self, what)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def select(self, what):
    return self.__cache.select(what)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.show">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            show
           </span>
          </span>
          (
          <span>
           self, n=20, t=False)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def show(self, n=20, t=False):
    self.__cache.show(n, t)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.update_cache">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            update_cache
           </span>
          </span>
          (
          <span>
           self, df)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def update_cache(self, df):
    self.__cache = self.__cache.select('*').where(
        F.col('id') not in df.select('id').distinct()
    )

    self.__cache = self.append(df)

    return self</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.update_df">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            update_df
           </span>
          </span>
          (
          <span>
           self, df_to_update, join_cols=('target', 'ip'), select_cols=('*',))
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def update_df(
        self, df_to_update, join_cols=('target', 'ip'), select_cols=('*',)
):
    self._changed = True

    if "*" in select_cols:
        select_cols = self.cache.columns

    # add null columns if nothing in cache
    if self.count() == 0:
        for c in select_cols:
            if c not in df_to_update.columns:
                df_to_update = df_to_update.withColumn(c, F.lit(None))
        return df_to_update

    # https://issues.apache.org/jira/browse/SPARK-10925
    df = df_to_update.rdd.toDF(df_to_update.schema).alias('a').join(
        F.broadcast(self.cache.select(*select_cols).alias('cache')),
        list(join_cols),
        how='left_outer'
    ).persist(self.storage_level)

    # update nulls and filter drop duplicate columns
    for c in select_cols:
        # if we have duplicate columns, take the new column as truth
        # (the cache)
        if df.columns.count(c) &gt; 1:
            if c not in join_cols:
                df_col = f'a.{c}'
                cache_col = f'cache.{c}'
                renamed_col = f'renamed_{c}'

                df = df.withColumn(
                    renamed_col,
                    F.when(
                        F.col(df_col).isNull(), F.col(cache_col)
                    ).otherwise(F.col(df_col))
                )
                df = df.select(*[i for i in df.columns
                                 if i not in [cache_col, df_col, c]
                                 ])
                df = df.withColumnRenamed(renamed_col, c)
    df.unpersist(blocking=True)
    return df</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.update_self">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            update_self
           </span>
          </span>
          (
          <span>
           self, source_df, join_cols=('target', 'ip'), select_cols=('*',), expire=True)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           :param source_df:
:param join_cols:
:param select_cols:
:param expire:
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def update_self(
        self,
        source_df,
        join_cols=('target', 'ip'),
        select_cols=('*',),
        expire=True
):
    """

    :param source_df:
    :param join_cols:
    :param select_cols:
    :param expire:
    :return:
    """
    # if os.path.exists(self.persistent_cache_file):
    #     shutil.rmtree(self.persistent_cache_file)
    #     # time.sleep(1)

    to_drop = [
        'prediction', 'r', 'score', 'to_update', 'id', 'id_runtime',
        'features', 'start', 'stop', 'subset_count', 'num_requests',
        'total_seconds', 'time_bucket', 'model_version', 'to_update',
        'label', 'id_attribute'
    ]
    now = datetime.datetime.utcnow()
    source_df = source_df.persist(self.storage_level).alias('sd')

    self.logger.debug(f'Source_df count = {source_df.count()}')

    # read the whole thing again
    if self.file_manager.path_exists(self.file_name):
        self.__persistent_cache = self.session_getter().read.format(
            self.format_
        ).load(
            self.file_name
        ).persist(self.storage_level)

    # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/
    self.__persistent_cache = F.broadcast(
        source_df.rdd.toDF(source_df.schema)
    ).join(
        self.__persistent_cache.select(*select_cols).alias('pc'),
        list(join_cols),
        how='full_outer'
    ).persist(self.storage_level)

    # mark rows to update
    self.__persistent_cache = self.__persistent_cache.withColumn(
        'to_update',
        F.col('features').isNotNull()
    )

    # update cache columns
    for cache_col, df_col in self.column_renamings.items():
        self.__persistent_cache = self.__persistent_cache.withColumn(
            cache_col,
            F.when(
                F.col('to_update') == True, F.col(df_col)  # noqa
            ).otherwise(
                F.when(
                    F.col(cache_col).isNotNull(),
                    F.col(cache_col)
                )
            )
        )
    self.__persistent_cache = self.__persistent_cache.withColumn(
        'updated_at',
        F.when(
            F.col('to_update') == True, now  # noqa
        ).otherwise(F.col('updated_at'))
    )
    # drop cols that do not belong in the cache
    self.__persistent_cache = self.__persistent_cache.drop(*to_drop)

    # remove old rows
    if expire:
        update_date = now - datetime.timedelta(
            seconds=self.expire_if_longer_than
        )
        self.__persistent_cache = self.__persistent_cache.select(
            '*'
        ).where(F.col('updated_at') &gt;= update_date)

    # write back to parquet - different file/folder though
    # because self.parquet_name is already in use
    # rename temp to self.parquet_name
    if self.file_manager.path_exists(self.temp_file_name):
        self.file_manager.delete_path(self.temp_file_name)

    self.__persistent_cache.write.mode(
        'overwrite'
    ).format(
        self.format_
    ).save(self.temp_file_name)

    self.logger.debug(
        f'# Number of rows in persistent cache: '
        f'{self.__persistent_cache.count()}'
    )

    # we don't need anything in memory anymore
    source_df.unpersist(blocking=True)
    source_df = None
    del source_df
    self.empty_all()

    # rename temp to self.parquet_name
    if self.file_manager.path_exists(self.file_name):
        self.file_manager.delete_path(self.file_name)

    self.file_manager.rename_path(self.temp_file_name, self.file_name)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.request_set_cache.RequestSetSparkCache.write">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            write
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Write persistent cache to file
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def write(self):
    """
    Write persistent cache to file
    :return: None
    """
    self.__cache.write.mode('overwrite').partitionBy(
        *self.group_by_fields
    ).format(self.format_).save(self.persistent_cache_file)</code></pre>
         </details>
        </dd>
       </dl>
      </dd>
     </dl>
    </section>
   </article>
   <nav id="sidebar">
    <h1>
     Index
    </h1>
    <div class="toc">
     <ul>
     </ul>
    </div>
    <ul id="index">
     <li>
      <h3>
       Super-module
      </h3>
      <ul>
       <li>
        <code>
         <a href="index.html" title="baskerville.models">
          baskerville.models
         </a>
        </code>
       </li>
      </ul>
     </li>
     <li>
      <h3>
       <a href="#header-classes">
        Classes
       </a>
      </h3>
      <ul>
       <li>
        <h4>
         <code>
          <a href="#baskerville.models.request_set_cache.RequestSetSparkCache" title="baskerville.models.request_set_cache.RequestSetSparkCache">
           RequestSetSparkCache
          </a>
         </code>
        </h4>
        <ul class="">
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.alias" title="baskerville.models.request_set_cache.RequestSetSparkCache.alias">
            alias
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.append" title="baskerville.models.request_set_cache.RequestSetSparkCache.append">
            append
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.cache" title="baskerville.models.request_set_cache.RequestSetSparkCache.cache">
            cache
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.clean" title="baskerville.models.request_set_cache.RequestSetSparkCache.clean">
            clean
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.count" title="baskerville.models.request_set_cache.RequestSetSparkCache.count">
            count
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.deduplicate" title="baskerville.models.request_set_cache.RequestSetSparkCache.deduplicate">
            deduplicate
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.empty" title="baskerville.models.request_set_cache.RequestSetSparkCache.empty">
            empty
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.empty_all" title="baskerville.models.request_set_cache.RequestSetSparkCache.empty_all">
            empty_all
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.filter_by" title="baskerville.models.request_set_cache.RequestSetSparkCache.filter_by">
            filter_by
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.load" title="baskerville.models.request_set_cache.RequestSetSparkCache.load">
            load
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.load_empty" title="baskerville.models.request_set_cache.RequestSetSparkCache.load_empty">
            load_empty
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.options" title="baskerville.models.request_set_cache.RequestSetSparkCache.options">
            options
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.persist" title="baskerville.models.request_set_cache.RequestSetSparkCache.persist">
            persist
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache" title="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache">
            persistent_cache
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache_file" title="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache_file">
            persistent_cache_file
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.refresh" title="baskerville.models.request_set_cache.RequestSetSparkCache.refresh">
            refresh
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.select" title="baskerville.models.request_set_cache.RequestSetSparkCache.select">
            select
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.show" title="baskerville.models.request_set_cache.RequestSetSparkCache.show">
            show
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.update_cache" title="baskerville.models.request_set_cache.RequestSetSparkCache.update_cache">
            update_cache
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.update_df" title="baskerville.models.request_set_cache.RequestSetSparkCache.update_df">
            update_df
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.update_self" title="baskerville.models.request_set_cache.RequestSetSparkCache.update_self">
            update_self
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.request_set_cache.RequestSetSparkCache.write" title="baskerville.models.request_set_cache.RequestSetSparkCache.write">
            write
           </a>
          </code>
         </li>
        </ul>
       </li>
      </ul>
     </li>
    </ul>
   </nav>
  </main>
  <footer id="footer">
   <p>
    Generated by
    <a href="https://pdoc3.github.io/pdoc">
     <cite>
      pdoc
     </cite>
     0.7.2
    </a>
    .
   </p>
   <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">
    <img alt="Creative Commons Licence" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"/>
   </a>
   <br/>
   This work is copyright (c) 2020, eQualit.ie inc., and is licensed under a
   <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">
    Creative Commons Attribution 4.0 International License
   </a>
   .
  </footer>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad()
  </script>
 </body>
</html>