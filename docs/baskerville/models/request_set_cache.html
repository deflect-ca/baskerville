<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>baskerville.models.request_set_cache API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>baskerville.models.request_set_cache</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import datetime
import gc
import os
import shutil

from baskerville.spark import get_spark_session
from baskerville.spark.helpers import StorageLevel
from pyspark.sql import functions as F

from baskerville.util.helpers import get_logger, get_default_data_path

#
# class CacheRotator(object, metaclass=Iterator):
#
#     def __init__(self, base_name=&#39;cache&#39;, remove=True, format_=&#39;parquet&#39;):
#         self.base_name = base_name
#         self.remove = remove
#         self.format_ = format_
#         self.n_range = 10
#         self.current_name = self.get_file_name(0)
#         if self.remove:
#             self.remove_file()
#
#     def remove_file(self):
#         if os.path.exists(self.current_name):
#             shutil.rmtree(self.current_name)
#
#     def get_file_name(self, postfix):
#         return f&#39;{get_default_data_path()}/{self.base_name}_{postfix}.{self.format_}&#39;
#
#     def __iter__(self):
#         for i in range(0, self.n_range):
#             self.remove_file()
#             self.current_name = self.get_file_name(i)
#             yield self.current_name
#
#     def __next__(self):
#         for i in range(0, self.n_range):
#             self.remove_file()
#             self.current_name = self.get_file_name(i)
#             yield self.current_name


class RequestSetSparkCache(object):

    def __init__(
            self,
            cache_config,
            table_name,
            columns_to_keep,
            expire_if_longer_than=3600,
            logger=None,
            session_getter=get_spark_session,
            group_by_fields=(&#39;target&#39;, &#39;ip&#39;),
            format_=&#39;parquet&#39;
    ):
        self.__cache = None
        self.__persistent_cache = None
        self.schema = None
        self.cache_config = cache_config
        self.table_name = table_name
        self.columns_to_keep = columns_to_keep
        self.expire_if_longer_than = expire_if_longer_than
        self.logger = logger if logger else get_logger(self.__class__.__name__)
        self.session_getter = session_getter
        self.group_by_fields = group_by_fields
        self.format_ = format_
        self.storage_level = StorageLevel.CUSTOM
        self.column_renamings = {
                            &#39;first_ever_request&#39;: &#39;start&#39;,
                            &#39;old_subset_count&#39;: &#39;subset_count&#39;,
                            &#39;old_features&#39;: &#39;features&#39;,
                            &#39;old_num_requests&#39;: &#39;num_requests&#39;,
                           }
        self._count = 0
        self._last_updated = datetime.datetime.utcnow()
        self._changed = False
        self.file_name = f&#39;{get_default_data_path()}/&#39; \
            f&#39;{self.__class__.__name__}.{self.format_}&#39;
        self.temp_file_name = f&#39;{get_default_data_path()}/&#39; \
            f&#39;{self.__class__.__name__}temp.{self.format_}&#39;

        if os.path.exists(self.file_name):
            shutil.rmtree(self.file_name)
        if os.path.exists(self.temp_file_name):
            shutil.rmtree(self.temp_file_name)

    @property
    def cache(self):
        return self.__cache

    @property
    def persistent_cache(self):
        return self.__persistent_cache

    @property
    def persistent_cache_file(self):
        return self.file_name

    def _get_load_q(self):
        return f&#39;&#39;&#39;(SELECT * 
                    from {self.table_name} 
                    where id in (select max(id) 
                    from {self.table_name} 
                    group by {&#39;, &#39;.join(self.group_by_fields)} )
                    ) as {self.table_name}&#39;&#39;&#39;

    def options(self, **kwargs):
        self.options = kwargs
        return self

    def _load(self, update_date=None, hosts=None, extra_filters=None):
        &#34;&#34;&#34;
        Loads the request_sets already in the database
        :return:
        :rtype: pyspark.sql.Dataframe
        &#34;&#34;&#34;
        where = ()
        if update_date:
            where = (
                    (F.col(&#34;updated_at&#34;) &gt;= update_date) |
                    (F.col(&#34;created_at&#34;) &gt;= update_date)
            )
        if not isinstance(where, F.Column):
            where = (F.col(&#39;id&#39;).isNotNull())

        if isinstance(extra_filters, F.Column):
            where = where &amp; extra_filters

        spark = self.session_getter()

        df = spark.read.format(&#39;jdbc&#39;).options(
            url=self.cache_config[&#39;db_url&#39;],
            driver=self.cache_config[&#39;db_driver&#39;],
            dbtable=self._get_load_q(),
            user=self.cache_config[&#39;user&#39;],
            password=self.cache_config[&#39;password&#39;],
            fetchsize=1000,
            max_connections=200,
        ).load(
        ).where(
            where
        ).select(
            *self.columns_to_keep
        )

        if hosts is not None:
            df = df.join(F.broadcast(hosts), [&#39;target&#39;], &#39;leftsemi&#39;)

        self._changed = True

        return df

    def write(self):
        &#34;&#34;&#34;
        Write persistent cache to file
        :return: None
        &#34;&#34;&#34;
        self.__cache.write.mode(&#39;overwrite&#39;).partitionBy(
            *self.group_by_fields
        ).format(self.format_).save(self.persistent_cache_file)

    def load(self, update_date=None, hosts=None, extra_filters=None):
        &#34;&#34;&#34;
        Load cache from database and store it in the configured file format
        :param update_date:
        :param hosts:
        :param extra_filters:
        :return:
        &#34;&#34;&#34;
        self.__cache = self._load(
            update_date=update_date,
            hosts=hosts,
            extra_filters=extra_filters
        ).persist(self.storage_level)

        self.write()

        return self

    def load_empty(self, schema):
        &#34;&#34;&#34;
        Instantiate an empty cache from the specified schema
        :param schema:
        :return:
        &#34;&#34;&#34;
        self.schema = schema
        spark = self.session_getter()
        self.__cache = spark.createDataFrame([], schema)
        self.__persistent_cache = spark.createDataFrame([], schema)

    def append(self, df):
        self.__cache = self.__cache.union(
            df.select(*self.columns_to_keep)
        )

        return self

    def update_cache(self, df):
        self.__cache = self.__cache.select(&#39;*&#39;).where(
            F.col(&#39;id&#39;) not in df.select(&#39;id&#39;).distinct()
        )

        self.__cache = self.append(df)

        return self

    def update_df(
            self, df_to_update, join_cols=(&#39;target&#39;, &#39;ip&#39;), select_cols=(&#39;*&#39;,)
    ):
        self._changed = True

        if &#34;*&#34; in select_cols:
            select_cols = self.cache.columns

        # add null columns if nothing in cache
        if self.count() == 0:
            for c in select_cols:
                if c not in df_to_update.columns:
                    df_to_update = df_to_update.withColumn(c, F.lit(None))
            return df_to_update

        # https://issues.apache.org/jira/browse/SPARK-10925
        df = df_to_update.rdd.toDF(df_to_update.schema).alias(&#39;a&#39;).join(
            F.broadcast(self.cache.select(*select_cols).alias(&#39;cache&#39;)),
            list(join_cols),
            how=&#39;left_outer&#39;
        ).persist(self.storage_level)

        # update nulls and filter drop duplicate columns
        for c in select_cols:
            # if we have duplicate columns, take the new column as truth
            # (the cache)
            if df.columns.count(c) &gt; 1:
                if c not in join_cols:
                    df_col = f&#39;a.{c}&#39;
                    cache_col = f&#39;cache.{c}&#39;
                    renamed_col = f&#39;renamed_{c}&#39;

                    df = df.withColumn(
                        renamed_col,
                        F.when(
                            F.col(df_col).isNull(), F.col(cache_col)
                        ).otherwise(F.col(df_col))
                    )
                    df = df.select(*[i for i in df.columns
                                     if i not in [cache_col, df_col, c]
                                     ])
                    df = df.withColumnRenamed(renamed_col, c)
        df.unpersist(blocking=True)
        return df

    def filter_by(self, df, columns=None):
        &#34;&#34;&#34;

        :param df:
        :param columns:
        :return:
        &#34;&#34;&#34;
        import os
        if not columns:
            columns = df.columns

        if os.path.isdir(self.persistent_cache_file):
            self.__cache = self.session_getter().read.format(
                self.format_
            ).load(self.persistent_cache_file).join(
                F.broadcast(df),
                on=columns,
                how=&#39;inner&#39;
            ).drop(
                &#39;a.ip&#39;
            ).persist(self.storage_level)
        else:
            if self.__cache:
                self.__cache = self.__cache.join(
                    F.broadcast(df),
                    on=columns,
                    how=&#39;inner&#39;
                ).drop(
                    &#39;a.ip&#39;
                ).persist(self.storage_level)
            else:
                self.load_empty(self.schema)

    def update_self(
            self,
            source_df,
            join_cols=(&#39;target&#39;, &#39;ip&#39;),
            select_cols=(&#39;*&#39;,),
            expire=True
    ):
        &#34;&#34;&#34;

        :param source_df:
        :param join_cols:
        :param select_cols:
        :param expire:
        :return:
        &#34;&#34;&#34;
        # if os.path.exists(self.persistent_cache_file):
        #     shutil.rmtree(self.persistent_cache_file)
        #     # time.sleep(1)

        to_drop = [
            &#39;prediction&#39;, &#39;r&#39;, &#39;to_update&#39;, &#39;id&#39;, &#39;id_runtime&#39;, &#39;features&#39;,
            &#39;start&#39;, &#39;stop&#39;, &#39;subset_count&#39;, &#39;num_requests&#39;, &#39;total_seconds&#39;,
            &#39;time_bucket&#39;, &#39;model_version&#39;, &#39;to_update&#39;, &#39;label&#39;, &#39;id_attribute&#39;
        ]
        now = datetime.datetime.utcnow()
        source_df = source_df.persist(self.storage_level).alias(&#39;sd&#39;)

        self.logger.debug(f&#39;Source_df count = {source_df.count()}&#39;)

        # read the whole thing again
        if os.path.isdir(self.persistent_cache_file):
            self.__persistent_cache = self.session_getter().read.format(
                self.format_
            ).load(
                self.persistent_cache_file
            ).persist(self.storage_level)

        # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/
        self.__persistent_cache = F.broadcast(
            source_df.rdd.toDF(source_df.schema)
        ).join(
            self.__persistent_cache.select(*select_cols).alias(&#39;pc&#39;),
            list(join_cols),
            how=&#39;full_outer&#39;
        ).persist(self.storage_level)

        # mark rows to update
        self.__persistent_cache = self.__persistent_cache.withColumn(
            &#39;to_update&#39;,
            F.col(&#39;features&#39;).isNotNull()
        )

        # update cache columns
        for cache_col, df_col in self.column_renamings.items():
            self.__persistent_cache = self.__persistent_cache.withColumn(
                cache_col,
                F.when(
                    F.col(&#39;to_update&#39;) == True, F.col(df_col)  # noqa
                ).otherwise(
                    F.when(
                        F.col(cache_col).isNotNull(),
                        F.col(cache_col)
                    )
                )
            )
        self.__persistent_cache = self.__persistent_cache.withColumn(
            &#39;updated_at&#39;,
            F.when(
                F.col(&#39;to_update&#39;) == True, now  # noqa
            ).otherwise(F.col(&#39;updated_at&#39;))
        )
        # drop cols that do not belong in the cache
        self.__persistent_cache = self.__persistent_cache.drop(*to_drop)

        # remove old rows
        if expire:
            update_date = now - datetime.timedelta(
                seconds=self.expire_if_longer_than
            )
            self.__persistent_cache = self.__persistent_cache.select(
                &#39;*&#39;
            ).where(F.col(&#39;updated_at&#39;) &gt;= update_date)

        # write back to parquet - different file/folder though
        # because self.parquet_name is already in use
        # rename temp to self.parquet_name
        self.__persistent_cache.write.mode(
            &#39;overwrite&#39;
        ).format(
            self.format_
        ).save(self.temp_file_name)

        self.logger.debug(
            f&#39;# Number of rows in persistent cache: &#39;
            f&#39;{self.__persistent_cache.count()}&#39;
        )

        # we don&#39;t need anything in memory anymore
        source_df.unpersist(blocking=True)
        source_df = None
        del source_df
        self.empty_all()

        # rename temp to self.parquet_name
        if os.path.exists(self.file_name):
            shutil.rmtree(self.file_name)
        os.rename(self.temp_file_name, self.file_name)

    def refresh(self, update_date, hosts, extra_filters=None):
        df = self._load(
            update_date=update_date, hosts=hosts, extra_filters=extra_filters
        )

        self.append(
           df
        ).deduplicate()

        return self

    def deduplicate(self):
        self.__cache = self.__cache.dropDuplicates()
        # self._count = self.cache.count()
        # self._last_updated = datetime.datetime.now()
        # self._changed = False

    def alias(self, name):
        self.__cache = self.__cache.alias(name)
        return self

    def show(self, n=20, t=False):
        self.__cache.show(n, t)

    def select(self, what):
        return self.__cache.select(what)

    def count(self):
        if self.__cache:
            try:
                return self.cache.count()
            except:
                import traceback
                traceback.print_exc()
                self.logger.debug(&#39;Just hit the cache issue.. trying to refresh&#39;)
                # self.cache.createOrReplaceTempView(&#34;current_cache&#34;)
                # self.session_getter().catalog.refreshTable(&#34;current_cache&#34;)
                return self.cache.count()

        return 0

    def clean(self, now, expire_if_longer_than=3600):
        &#34;&#34;&#34;
        Remove request_sets with seconds since update &gt; expire_if_longer_than
        :param datetime.datetime now: utc datetime
        :param int expire_if_longer_than: seconds
        :param bool allow_null: allow request_sets with null update_date (newly
        created)
        :return: None
        &#34;&#34;&#34;
        update_date = now - datetime.timedelta(seconds=expire_if_longer_than)
        self.__cache = self.__cache.select(&#39;*&#39;).where((
                (F.col(&#34;updated_at&#34;) &gt;= F.lit(update_date)) |
                (F.col(&#34;created_at&#34;) &gt;= F.lit(update_date))
        ))

        return self

    def empty(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        self.__cache = None

    def empty_all(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        if self.__persistent_cache is not None:
            self.__persistent_cache.unpersist(blocking=True)

        self.__cache = None
        self.__persistent_cache = None
        gc.collect()
        self.session_getter().sparkContext._jvm.System.gc()

    def persist(self):
        self.__cache = self.__cache.persist(self.storage_level)
        # self.__cache.createOrReplaceTempView(self.__class__.__name__)
        # spark = self.session_getter()
        # spark.catalog.cacheTable(self.__class__.__name__)

    def __len__(self):
        return self.count()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache"><code class="flex name class">
<span>class <span class="ident">RequestSetSparkCache</span></span>
<span>(</span><span>cache_config, table_name, columns_to_keep, expire_if_longer_than=3600, logger=None, session_getter=&lt;function get_spark_session&gt;, group_by_fields=('target', 'ip'), format_='parquet')</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RequestSetSparkCache(object):

    def __init__(
            self,
            cache_config,
            table_name,
            columns_to_keep,
            expire_if_longer_than=3600,
            logger=None,
            session_getter=get_spark_session,
            group_by_fields=(&#39;target&#39;, &#39;ip&#39;),
            format_=&#39;parquet&#39;
    ):
        self.__cache = None
        self.__persistent_cache = None
        self.schema = None
        self.cache_config = cache_config
        self.table_name = table_name
        self.columns_to_keep = columns_to_keep
        self.expire_if_longer_than = expire_if_longer_than
        self.logger = logger if logger else get_logger(self.__class__.__name__)
        self.session_getter = session_getter
        self.group_by_fields = group_by_fields
        self.format_ = format_
        self.storage_level = StorageLevel.CUSTOM
        self.column_renamings = {
                            &#39;first_ever_request&#39;: &#39;start&#39;,
                            &#39;old_subset_count&#39;: &#39;subset_count&#39;,
                            &#39;old_features&#39;: &#39;features&#39;,
                            &#39;old_num_requests&#39;: &#39;num_requests&#39;,
                           }
        self._count = 0
        self._last_updated = datetime.datetime.utcnow()
        self._changed = False
        self.file_name = f&#39;{get_default_data_path()}/&#39; \
            f&#39;{self.__class__.__name__}.{self.format_}&#39;
        self.temp_file_name = f&#39;{get_default_data_path()}/&#39; \
            f&#39;{self.__class__.__name__}temp.{self.format_}&#39;

        if os.path.exists(self.file_name):
            shutil.rmtree(self.file_name)
        if os.path.exists(self.temp_file_name):
            shutil.rmtree(self.temp_file_name)

    @property
    def cache(self):
        return self.__cache

    @property
    def persistent_cache(self):
        return self.__persistent_cache

    @property
    def persistent_cache_file(self):
        return self.file_name

    def _get_load_q(self):
        return f&#39;&#39;&#39;(SELECT * 
                    from {self.table_name} 
                    where id in (select max(id) 
                    from {self.table_name} 
                    group by {&#39;, &#39;.join(self.group_by_fields)} )
                    ) as {self.table_name}&#39;&#39;&#39;

    def options(self, **kwargs):
        self.options = kwargs
        return self

    def _load(self, update_date=None, hosts=None, extra_filters=None):
        &#34;&#34;&#34;
        Loads the request_sets already in the database
        :return:
        :rtype: pyspark.sql.Dataframe
        &#34;&#34;&#34;
        where = ()
        if update_date:
            where = (
                    (F.col(&#34;updated_at&#34;) &gt;= update_date) |
                    (F.col(&#34;created_at&#34;) &gt;= update_date)
            )
        if not isinstance(where, F.Column):
            where = (F.col(&#39;id&#39;).isNotNull())

        if isinstance(extra_filters, F.Column):
            where = where &amp; extra_filters

        spark = self.session_getter()

        df = spark.read.format(&#39;jdbc&#39;).options(
            url=self.cache_config[&#39;db_url&#39;],
            driver=self.cache_config[&#39;db_driver&#39;],
            dbtable=self._get_load_q(),
            user=self.cache_config[&#39;user&#39;],
            password=self.cache_config[&#39;password&#39;],
            fetchsize=1000,
            max_connections=200,
        ).load(
        ).where(
            where
        ).select(
            *self.columns_to_keep
        )

        if hosts is not None:
            df = df.join(F.broadcast(hosts), [&#39;target&#39;], &#39;leftsemi&#39;)

        self._changed = True

        return df

    def write(self):
        &#34;&#34;&#34;
        Write persistent cache to file
        :return: None
        &#34;&#34;&#34;
        self.__cache.write.mode(&#39;overwrite&#39;).partitionBy(
            *self.group_by_fields
        ).format(self.format_).save(self.persistent_cache_file)

    def load(self, update_date=None, hosts=None, extra_filters=None):
        &#34;&#34;&#34;
        Load cache from database and store it in the configured file format
        :param update_date:
        :param hosts:
        :param extra_filters:
        :return:
        &#34;&#34;&#34;
        self.__cache = self._load(
            update_date=update_date,
            hosts=hosts,
            extra_filters=extra_filters
        ).persist(self.storage_level)

        self.write()

        return self

    def load_empty(self, schema):
        &#34;&#34;&#34;
        Instantiate an empty cache from the specified schema
        :param schema:
        :return:
        &#34;&#34;&#34;
        self.schema = schema
        spark = self.session_getter()
        self.__cache = spark.createDataFrame([], schema)
        self.__persistent_cache = spark.createDataFrame([], schema)

    def append(self, df):
        self.__cache = self.__cache.union(
            df.select(*self.columns_to_keep)
        )

        return self

    def update_cache(self, df):
        self.__cache = self.__cache.select(&#39;*&#39;).where(
            F.col(&#39;id&#39;) not in df.select(&#39;id&#39;).distinct()
        )

        self.__cache = self.append(df)

        return self

    def update_df(
            self, df_to_update, join_cols=(&#39;target&#39;, &#39;ip&#39;), select_cols=(&#39;*&#39;,)
    ):
        self._changed = True

        if &#34;*&#34; in select_cols:
            select_cols = self.cache.columns

        # add null columns if nothing in cache
        if self.count() == 0:
            for c in select_cols:
                if c not in df_to_update.columns:
                    df_to_update = df_to_update.withColumn(c, F.lit(None))
            return df_to_update

        # https://issues.apache.org/jira/browse/SPARK-10925
        df = df_to_update.rdd.toDF(df_to_update.schema).alias(&#39;a&#39;).join(
            F.broadcast(self.cache.select(*select_cols).alias(&#39;cache&#39;)),
            list(join_cols),
            how=&#39;left_outer&#39;
        ).persist(self.storage_level)

        # update nulls and filter drop duplicate columns
        for c in select_cols:
            # if we have duplicate columns, take the new column as truth
            # (the cache)
            if df.columns.count(c) &gt; 1:
                if c not in join_cols:
                    df_col = f&#39;a.{c}&#39;
                    cache_col = f&#39;cache.{c}&#39;
                    renamed_col = f&#39;renamed_{c}&#39;

                    df = df.withColumn(
                        renamed_col,
                        F.when(
                            F.col(df_col).isNull(), F.col(cache_col)
                        ).otherwise(F.col(df_col))
                    )
                    df = df.select(*[i for i in df.columns
                                     if i not in [cache_col, df_col, c]
                                     ])
                    df = df.withColumnRenamed(renamed_col, c)
        df.unpersist(blocking=True)
        return df

    def filter_by(self, df, columns=None):
        &#34;&#34;&#34;

        :param df:
        :param columns:
        :return:
        &#34;&#34;&#34;
        import os
        if not columns:
            columns = df.columns

        if os.path.isdir(self.persistent_cache_file):
            self.__cache = self.session_getter().read.format(
                self.format_
            ).load(self.persistent_cache_file).join(
                F.broadcast(df),
                on=columns,
                how=&#39;inner&#39;
            ).drop(
                &#39;a.ip&#39;
            ).persist(self.storage_level)
        else:
            if self.__cache:
                self.__cache = self.__cache.join(
                    F.broadcast(df),
                    on=columns,
                    how=&#39;inner&#39;
                ).drop(
                    &#39;a.ip&#39;
                ).persist(self.storage_level)
            else:
                self.load_empty(self.schema)

    def update_self(
            self,
            source_df,
            join_cols=(&#39;target&#39;, &#39;ip&#39;),
            select_cols=(&#39;*&#39;,),
            expire=True
    ):
        &#34;&#34;&#34;

        :param source_df:
        :param join_cols:
        :param select_cols:
        :param expire:
        :return:
        &#34;&#34;&#34;
        # if os.path.exists(self.persistent_cache_file):
        #     shutil.rmtree(self.persistent_cache_file)
        #     # time.sleep(1)

        to_drop = [
            &#39;prediction&#39;, &#39;r&#39;, &#39;to_update&#39;, &#39;id&#39;, &#39;id_runtime&#39;, &#39;features&#39;,
            &#39;start&#39;, &#39;stop&#39;, &#39;subset_count&#39;, &#39;num_requests&#39;, &#39;total_seconds&#39;,
            &#39;time_bucket&#39;, &#39;model_version&#39;, &#39;to_update&#39;, &#39;label&#39;, &#39;id_attribute&#39;
        ]
        now = datetime.datetime.utcnow()
        source_df = source_df.persist(self.storage_level).alias(&#39;sd&#39;)

        self.logger.debug(f&#39;Source_df count = {source_df.count()}&#39;)

        # read the whole thing again
        if os.path.isdir(self.persistent_cache_file):
            self.__persistent_cache = self.session_getter().read.format(
                self.format_
            ).load(
                self.persistent_cache_file
            ).persist(self.storage_level)

        # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/
        self.__persistent_cache = F.broadcast(
            source_df.rdd.toDF(source_df.schema)
        ).join(
            self.__persistent_cache.select(*select_cols).alias(&#39;pc&#39;),
            list(join_cols),
            how=&#39;full_outer&#39;
        ).persist(self.storage_level)

        # mark rows to update
        self.__persistent_cache = self.__persistent_cache.withColumn(
            &#39;to_update&#39;,
            F.col(&#39;features&#39;).isNotNull()
        )

        # update cache columns
        for cache_col, df_col in self.column_renamings.items():
            self.__persistent_cache = self.__persistent_cache.withColumn(
                cache_col,
                F.when(
                    F.col(&#39;to_update&#39;) == True, F.col(df_col)  # noqa
                ).otherwise(
                    F.when(
                        F.col(cache_col).isNotNull(),
                        F.col(cache_col)
                    )
                )
            )
        self.__persistent_cache = self.__persistent_cache.withColumn(
            &#39;updated_at&#39;,
            F.when(
                F.col(&#39;to_update&#39;) == True, now  # noqa
            ).otherwise(F.col(&#39;updated_at&#39;))
        )
        # drop cols that do not belong in the cache
        self.__persistent_cache = self.__persistent_cache.drop(*to_drop)

        # remove old rows
        if expire:
            update_date = now - datetime.timedelta(
                seconds=self.expire_if_longer_than
            )
            self.__persistent_cache = self.__persistent_cache.select(
                &#39;*&#39;
            ).where(F.col(&#39;updated_at&#39;) &gt;= update_date)

        # write back to parquet - different file/folder though
        # because self.parquet_name is already in use
        # rename temp to self.parquet_name
        self.__persistent_cache.write.mode(
            &#39;overwrite&#39;
        ).format(
            self.format_
        ).save(self.temp_file_name)

        self.logger.debug(
            f&#39;# Number of rows in persistent cache: &#39;
            f&#39;{self.__persistent_cache.count()}&#39;
        )

        # we don&#39;t need anything in memory anymore
        source_df.unpersist(blocking=True)
        source_df = None
        del source_df
        self.empty_all()

        # rename temp to self.parquet_name
        if os.path.exists(self.file_name):
            shutil.rmtree(self.file_name)
        os.rename(self.temp_file_name, self.file_name)

    def refresh(self, update_date, hosts, extra_filters=None):
        df = self._load(
            update_date=update_date, hosts=hosts, extra_filters=extra_filters
        )

        self.append(
           df
        ).deduplicate()

        return self

    def deduplicate(self):
        self.__cache = self.__cache.dropDuplicates()
        # self._count = self.cache.count()
        # self._last_updated = datetime.datetime.now()
        # self._changed = False

    def alias(self, name):
        self.__cache = self.__cache.alias(name)
        return self

    def show(self, n=20, t=False):
        self.__cache.show(n, t)

    def select(self, what):
        return self.__cache.select(what)

    def count(self):
        if self.__cache:
            try:
                return self.cache.count()
            except:
                import traceback
                traceback.print_exc()
                self.logger.debug(&#39;Just hit the cache issue.. trying to refresh&#39;)
                # self.cache.createOrReplaceTempView(&#34;current_cache&#34;)
                # self.session_getter().catalog.refreshTable(&#34;current_cache&#34;)
                return self.cache.count()

        return 0

    def clean(self, now, expire_if_longer_than=3600):
        &#34;&#34;&#34;
        Remove request_sets with seconds since update &gt; expire_if_longer_than
        :param datetime.datetime now: utc datetime
        :param int expire_if_longer_than: seconds
        :param bool allow_null: allow request_sets with null update_date (newly
        created)
        :return: None
        &#34;&#34;&#34;
        update_date = now - datetime.timedelta(seconds=expire_if_longer_than)
        self.__cache = self.__cache.select(&#39;*&#39;).where((
                (F.col(&#34;updated_at&#34;) &gt;= F.lit(update_date)) |
                (F.col(&#34;created_at&#34;) &gt;= F.lit(update_date))
        ))

        return self

    def empty(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        self.__cache = None

    def empty_all(self):
        if self.__cache is not None:
            self.__cache.unpersist(blocking=True)
        if self.__persistent_cache is not None:
            self.__persistent_cache.unpersist(blocking=True)

        self.__cache = None
        self.__persistent_cache = None
        gc.collect()
        self.session_getter().sparkContext._jvm.System.gc()

    def persist(self):
        self.__cache = self.__cache.persist(self.storage_level)
        # self.__cache.createOrReplaceTempView(self.__class__.__name__)
        # spark = self.session_getter()
        # spark.catalog.cacheTable(self.__class__.__name__)

    def __len__(self):
        return self.count()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.cache"><code class="name">var <span class="ident">cache</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def cache(self):
    return self.__cache</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache"><code class="name">var <span class="ident">persistent_cache</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def persistent_cache(self):
    return self.__persistent_cache</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache_file"><code class="name">var <span class="ident">persistent_cache_file</span></code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def persistent_cache_file(self):
    return self.file_name</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.alias"><code class="name flex">
<span>def <span class="ident">alias</span></span>(<span>self, name)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def alias(self, name):
    self.__cache = self.__cache.alias(name)
    return self</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.append"><code class="name flex">
<span>def <span class="ident">append</span></span>(<span>self, df)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def append(self, df):
    self.__cache = self.__cache.union(
        df.select(*self.columns_to_keep)
    )

    return self</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.clean"><code class="name flex">
<span>def <span class="ident">clean</span></span>(<span>self, now, expire_if_longer_than=3600)</span>
</code></dt>
<dd>
<section class="desc"><p>Remove request_sets with seconds since update &gt; expire_if_longer_than
:param datetime.datetime now: utc datetime
:param int expire_if_longer_than: seconds
:param bool allow_null: allow request_sets with null update_date (newly
created)
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean(self, now, expire_if_longer_than=3600):
    &#34;&#34;&#34;
    Remove request_sets with seconds since update &gt; expire_if_longer_than
    :param datetime.datetime now: utc datetime
    :param int expire_if_longer_than: seconds
    :param bool allow_null: allow request_sets with null update_date (newly
    created)
    :return: None
    &#34;&#34;&#34;
    update_date = now - datetime.timedelta(seconds=expire_if_longer_than)
    self.__cache = self.__cache.select(&#39;*&#39;).where((
            (F.col(&#34;updated_at&#34;) &gt;= F.lit(update_date)) |
            (F.col(&#34;created_at&#34;) &gt;= F.lit(update_date))
    ))

    return self</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.count"><code class="name flex">
<span>def <span class="ident">count</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def count(self):
    if self.__cache:
        try:
            return self.cache.count()
        except:
            import traceback
            traceback.print_exc()
            self.logger.debug(&#39;Just hit the cache issue.. trying to refresh&#39;)
            # self.cache.createOrReplaceTempView(&#34;current_cache&#34;)
            # self.session_getter().catalog.refreshTable(&#34;current_cache&#34;)
            return self.cache.count()

    return 0</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.deduplicate"><code class="name flex">
<span>def <span class="ident">deduplicate</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deduplicate(self):
    self.__cache = self.__cache.dropDuplicates()</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.empty"><code class="name flex">
<span>def <span class="ident">empty</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def empty(self):
    if self.__cache is not None:
        self.__cache.unpersist(blocking=True)
    self.__cache = None</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.empty_all"><code class="name flex">
<span>def <span class="ident">empty_all</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def empty_all(self):
    if self.__cache is not None:
        self.__cache.unpersist(blocking=True)
    if self.__persistent_cache is not None:
        self.__persistent_cache.unpersist(blocking=True)

    self.__cache = None
    self.__persistent_cache = None
    gc.collect()
    self.session_getter().sparkContext._jvm.System.gc()</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.filter_by"><code class="name flex">
<span>def <span class="ident">filter_by</span></span>(<span>self, df, columns=None)</span>
</code></dt>
<dd>
<section class="desc"><p>:param df:
:param columns:
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_by(self, df, columns=None):
    &#34;&#34;&#34;

    :param df:
    :param columns:
    :return:
    &#34;&#34;&#34;
    import os
    if not columns:
        columns = df.columns

    if os.path.isdir(self.persistent_cache_file):
        self.__cache = self.session_getter().read.format(
            self.format_
        ).load(self.persistent_cache_file).join(
            F.broadcast(df),
            on=columns,
            how=&#39;inner&#39;
        ).drop(
            &#39;a.ip&#39;
        ).persist(self.storage_level)
    else:
        if self.__cache:
            self.__cache = self.__cache.join(
                F.broadcast(df),
                on=columns,
                how=&#39;inner&#39;
            ).drop(
                &#39;a.ip&#39;
            ).persist(self.storage_level)
        else:
            self.load_empty(self.schema)</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, update_date=None, hosts=None, extra_filters=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Load cache from database and store it in the configured file format
:param update_date:
:param hosts:
:param extra_filters:
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load(self, update_date=None, hosts=None, extra_filters=None):
    &#34;&#34;&#34;
    Load cache from database and store it in the configured file format
    :param update_date:
    :param hosts:
    :param extra_filters:
    :return:
    &#34;&#34;&#34;
    self.__cache = self._load(
        update_date=update_date,
        hosts=hosts,
        extra_filters=extra_filters
    ).persist(self.storage_level)

    self.write()

    return self</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.load_empty"><code class="name flex">
<span>def <span class="ident">load_empty</span></span>(<span>self, schema)</span>
</code></dt>
<dd>
<section class="desc"><p>Instantiate an empty cache from the specified schema
:param schema:
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_empty(self, schema):
    &#34;&#34;&#34;
    Instantiate an empty cache from the specified schema
    :param schema:
    :return:
    &#34;&#34;&#34;
    self.schema = schema
    spark = self.session_getter()
    self.__cache = spark.createDataFrame([], schema)
    self.__persistent_cache = spark.createDataFrame([], schema)</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.options"><code class="name flex">
<span>def <span class="ident">options</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def options(self, **kwargs):
    self.options = kwargs
    return self</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.persist"><code class="name flex">
<span>def <span class="ident">persist</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def persist(self):
    self.__cache = self.__cache.persist(self.storage_level)</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.refresh"><code class="name flex">
<span>def <span class="ident">refresh</span></span>(<span>self, update_date, hosts, extra_filters=None)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh(self, update_date, hosts, extra_filters=None):
    df = self._load(
        update_date=update_date, hosts=hosts, extra_filters=extra_filters
    )

    self.append(
       df
    ).deduplicate()

    return self</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.select"><code class="name flex">
<span>def <span class="ident">select</span></span>(<span>self, what)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def select(self, what):
    return self.__cache.select(what)</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.show"><code class="name flex">
<span>def <span class="ident">show</span></span>(<span>self, n=20, t=False)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show(self, n=20, t=False):
    self.__cache.show(n, t)</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.update_cache"><code class="name flex">
<span>def <span class="ident">update_cache</span></span>(<span>self, df)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_cache(self, df):
    self.__cache = self.__cache.select(&#39;*&#39;).where(
        F.col(&#39;id&#39;) not in df.select(&#39;id&#39;).distinct()
    )

    self.__cache = self.append(df)

    return self</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.update_df"><code class="name flex">
<span>def <span class="ident">update_df</span></span>(<span>self, df_to_update, join_cols=('target', 'ip'), select_cols=('*',))</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_df(
        self, df_to_update, join_cols=(&#39;target&#39;, &#39;ip&#39;), select_cols=(&#39;*&#39;,)
):
    self._changed = True

    if &#34;*&#34; in select_cols:
        select_cols = self.cache.columns

    # add null columns if nothing in cache
    if self.count() == 0:
        for c in select_cols:
            if c not in df_to_update.columns:
                df_to_update = df_to_update.withColumn(c, F.lit(None))
        return df_to_update

    # https://issues.apache.org/jira/browse/SPARK-10925
    df = df_to_update.rdd.toDF(df_to_update.schema).alias(&#39;a&#39;).join(
        F.broadcast(self.cache.select(*select_cols).alias(&#39;cache&#39;)),
        list(join_cols),
        how=&#39;left_outer&#39;
    ).persist(self.storage_level)

    # update nulls and filter drop duplicate columns
    for c in select_cols:
        # if we have duplicate columns, take the new column as truth
        # (the cache)
        if df.columns.count(c) &gt; 1:
            if c not in join_cols:
                df_col = f&#39;a.{c}&#39;
                cache_col = f&#39;cache.{c}&#39;
                renamed_col = f&#39;renamed_{c}&#39;

                df = df.withColumn(
                    renamed_col,
                    F.when(
                        F.col(df_col).isNull(), F.col(cache_col)
                    ).otherwise(F.col(df_col))
                )
                df = df.select(*[i for i in df.columns
                                 if i not in [cache_col, df_col, c]
                                 ])
                df = df.withColumnRenamed(renamed_col, c)
    df.unpersist(blocking=True)
    return df</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.update_self"><code class="name flex">
<span>def <span class="ident">update_self</span></span>(<span>self, source_df, join_cols=('target', 'ip'), select_cols=('*',), expire=True)</span>
</code></dt>
<dd>
<section class="desc"><p>:param source_df:
:param join_cols:
:param select_cols:
:param expire:
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_self(
        self,
        source_df,
        join_cols=(&#39;target&#39;, &#39;ip&#39;),
        select_cols=(&#39;*&#39;,),
        expire=True
):
    &#34;&#34;&#34;

    :param source_df:
    :param join_cols:
    :param select_cols:
    :param expire:
    :return:
    &#34;&#34;&#34;
    # if os.path.exists(self.persistent_cache_file):
    #     shutil.rmtree(self.persistent_cache_file)
    #     # time.sleep(1)

    to_drop = [
        &#39;prediction&#39;, &#39;r&#39;, &#39;to_update&#39;, &#39;id&#39;, &#39;id_runtime&#39;, &#39;features&#39;,
        &#39;start&#39;, &#39;stop&#39;, &#39;subset_count&#39;, &#39;num_requests&#39;, &#39;total_seconds&#39;,
        &#39;time_bucket&#39;, &#39;model_version&#39;, &#39;to_update&#39;, &#39;label&#39;, &#39;id_attribute&#39;
    ]
    now = datetime.datetime.utcnow()
    source_df = source_df.persist(self.storage_level).alias(&#39;sd&#39;)

    self.logger.debug(f&#39;Source_df count = {source_df.count()}&#39;)

    # read the whole thing again
    if os.path.isdir(self.persistent_cache_file):
        self.__persistent_cache = self.session_getter().read.format(
            self.format_
        ).load(
            self.persistent_cache_file
        ).persist(self.storage_level)

    # http://www.learnbymarketing.com/1100/pyspark-joins-by-example/
    self.__persistent_cache = F.broadcast(
        source_df.rdd.toDF(source_df.schema)
    ).join(
        self.__persistent_cache.select(*select_cols).alias(&#39;pc&#39;),
        list(join_cols),
        how=&#39;full_outer&#39;
    ).persist(self.storage_level)

    # mark rows to update
    self.__persistent_cache = self.__persistent_cache.withColumn(
        &#39;to_update&#39;,
        F.col(&#39;features&#39;).isNotNull()
    )

    # update cache columns
    for cache_col, df_col in self.column_renamings.items():
        self.__persistent_cache = self.__persistent_cache.withColumn(
            cache_col,
            F.when(
                F.col(&#39;to_update&#39;) == True, F.col(df_col)  # noqa
            ).otherwise(
                F.when(
                    F.col(cache_col).isNotNull(),
                    F.col(cache_col)
                )
            )
        )
    self.__persistent_cache = self.__persistent_cache.withColumn(
        &#39;updated_at&#39;,
        F.when(
            F.col(&#39;to_update&#39;) == True, now  # noqa
        ).otherwise(F.col(&#39;updated_at&#39;))
    )
    # drop cols that do not belong in the cache
    self.__persistent_cache = self.__persistent_cache.drop(*to_drop)

    # remove old rows
    if expire:
        update_date = now - datetime.timedelta(
            seconds=self.expire_if_longer_than
        )
        self.__persistent_cache = self.__persistent_cache.select(
            &#39;*&#39;
        ).where(F.col(&#39;updated_at&#39;) &gt;= update_date)

    # write back to parquet - different file/folder though
    # because self.parquet_name is already in use
    # rename temp to self.parquet_name
    self.__persistent_cache.write.mode(
        &#39;overwrite&#39;
    ).format(
        self.format_
    ).save(self.temp_file_name)

    self.logger.debug(
        f&#39;# Number of rows in persistent cache: &#39;
        f&#39;{self.__persistent_cache.count()}&#39;
    )

    # we don&#39;t need anything in memory anymore
    source_df.unpersist(blocking=True)
    source_df = None
    del source_df
    self.empty_all()

    # rename temp to self.parquet_name
    if os.path.exists(self.file_name):
        shutil.rmtree(self.file_name)
    os.rename(self.temp_file_name, self.file_name)</code></pre>
</details>
</dd>
<dt id="baskerville.models.request_set_cache.RequestSetSparkCache.write"><code class="name flex">
<span>def <span class="ident">write</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Write persistent cache to file
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write(self):
    &#34;&#34;&#34;
    Write persistent cache to file
    :return: None
    &#34;&#34;&#34;
    self.__cache.write.mode(&#39;overwrite&#39;).partitionBy(
        *self.group_by_fields
    ).format(self.format_).save(self.persistent_cache_file)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="baskerville.models" href="index.html">baskerville.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache" href="#baskerville.models.request_set_cache.RequestSetSparkCache">RequestSetSparkCache</a></code></h4>
<ul class="">
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.alias" href="#baskerville.models.request_set_cache.RequestSetSparkCache.alias">alias</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.append" href="#baskerville.models.request_set_cache.RequestSetSparkCache.append">append</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.cache" href="#baskerville.models.request_set_cache.RequestSetSparkCache.cache">cache</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.clean" href="#baskerville.models.request_set_cache.RequestSetSparkCache.clean">clean</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.count" href="#baskerville.models.request_set_cache.RequestSetSparkCache.count">count</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.deduplicate" href="#baskerville.models.request_set_cache.RequestSetSparkCache.deduplicate">deduplicate</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.empty" href="#baskerville.models.request_set_cache.RequestSetSparkCache.empty">empty</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.empty_all" href="#baskerville.models.request_set_cache.RequestSetSparkCache.empty_all">empty_all</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.filter_by" href="#baskerville.models.request_set_cache.RequestSetSparkCache.filter_by">filter_by</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.load" href="#baskerville.models.request_set_cache.RequestSetSparkCache.load">load</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.load_empty" href="#baskerville.models.request_set_cache.RequestSetSparkCache.load_empty">load_empty</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.options" href="#baskerville.models.request_set_cache.RequestSetSparkCache.options">options</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.persist" href="#baskerville.models.request_set_cache.RequestSetSparkCache.persist">persist</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache" href="#baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache">persistent_cache</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache_file" href="#baskerville.models.request_set_cache.RequestSetSparkCache.persistent_cache_file">persistent_cache_file</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.refresh" href="#baskerville.models.request_set_cache.RequestSetSparkCache.refresh">refresh</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.select" href="#baskerville.models.request_set_cache.RequestSetSparkCache.select">select</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.show" href="#baskerville.models.request_set_cache.RequestSetSparkCache.show">show</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.update_cache" href="#baskerville.models.request_set_cache.RequestSetSparkCache.update_cache">update_cache</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.update_df" href="#baskerville.models.request_set_cache.RequestSetSparkCache.update_df">update_df</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.update_self" href="#baskerville.models.request_set_cache.RequestSetSparkCache.update_self">update_self</a></code></li>
<li><code><a title="baskerville.models.request_set_cache.RequestSetSparkCache.write" href="#baskerville.models.request_set_cache.RequestSetSparkCache.write">write</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>