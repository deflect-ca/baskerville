<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.2" />
<title>baskerville.models.base_spark API documentation</title>
<meta name="description" content="" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>baskerville.models.base_spark</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import datetime
import itertools

from baskerville.models.base import PipelineBase
from baskerville.models.feature_manager import FeatureManager
from baskerville.models.model_manager import ModelManager
from baskerville.spark.helpers import DictAccumulatorParam, col_to_json
from baskerville.util.helpers import TimeBucket
from pyspark.sql import types as T, DataFrame

from baskerville.features.helpers import StrippedFeature, \
    extract_features_in_order
from baskerville.spark import get_or_create_spark_session
from dateutil.tz import tzutc
from collections import OrderedDict, defaultdict

from baskerville.util.baskerville_tools import BaskervilleDBTools
from baskerville.util.enums import Step

from pyspark.sql import functions as F

from baskerville.db.models import RequestSet
from baskerville.models.request_set_cache import RequestSetSparkCache

# Broadcasts
CLF = None
SCL = None
MF = None
FEATURE_NAMES = None
ACTIVE_FEATURES = None

# Accumulators
CLIENT_PREDICTION_ACCUMULATOR = None
CLIENT_REQUEST_SET_COUNT = None


# TODO: double-check that defining the udf here doesn&#39;t break the accumulator &amp;
# the broadcast stuff
def predict_dict(target, dict_features, update_metrics=False):
    &#34;&#34;&#34;
    Scale the feature values and use the model to predict
    :param dict[str, float] dict_features: the feature dictionary
    :return: 0 if normal, 1 if abnormal, -1 if something went wrong
    &#34;&#34;&#34;
    global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT
    import numpy as np
    import json

    prediction = 0, 0.

    if isinstance(dict_features, str):
        dict_features = json.loads(dict_features)

    x_test = [extract_features_in_order(dict_features, MF.value)]
    try:
        x_test = SCL.value.transform(x_test)
        y = CLF.value.decision_function(x_test)
        prediction = np.sign(y)[0]
        r = np.float32(np.absolute(y)[0])
        prediction = float(prediction), float(r)
    except ValueError:
        import traceback
        traceback.print_exc()
        print(&#39;Cannot predict:&#39;, x_test)

    if update_metrics:
        CLIENT_REQUEST_SET_COUNT += {target: 1}
        CLIENT_PREDICTION_ACCUMULATOR += {target: prediction[0]}

    return prediction


prediction_schema = T.StructType([
    T.StructField(&#34;prediction&#34;, T.FloatType(), False),
    T.StructField(&#34;r&#34;, T.FloatType(), False)
])
udf_predict_dict = F.udf(predict_dict, prediction_schema)


class SparkPipelineBase(PipelineBase):
    &#34;&#34;&#34;
    The base class for all pipelines that use spark. It initializes spark
    session and provides basic implementation for some of the main methods
    &#34;&#34;&#34;

    def __init__(self,
                 db_conf,
                 engine_conf,
                 spark_conf,
                 clean_up=True,
                 group_by_cols=(&#39;client_request_host&#39;, &#39;client_ip&#39;),
                 *args,
                 **kwargs
                 ):

        super().__init__(db_conf, engine_conf, clean_up)
        self.start_time = datetime.datetime.utcnow()
        self.request_set_cache = None
        self.spark = None
        self.tools = None
        self.metrics_registry = None
        self.spark_conf = spark_conf
        self.data_parser = self.engine_conf.data_config.parser
        self.group_by_cols = list(set(group_by_cols))
        self.group_by_aggs = None
        self.post_group_by_aggs = None
        self.columns_to_filter_by = None
        self._can_predict = False
        self._is_initialized = False
        self.drop_if_missing_filter = None
        self.cols_to_drop = None
        self.cache_columns = [
            &#39;target&#39;,
            &#39;ip&#39;,
            &#39;first_ever_request&#39;,
            &#39;old_subset_count&#39;,
            &#39;old_features&#39;,
            &#39;old_num_requests&#39;,
        ]
        self.cache_config = {
            &#39;db_url&#39;: self.db_url,
            &#39;db_driver&#39;: self.spark_conf.db_driver,
            &#39;user&#39;: self.db_conf.user,
            &#39;password&#39;: self.db_conf.password
        }
        self.step_to_action = OrderedDict(
            zip([
                Step.preprocessing,
                Step.group_by,
                Step.feature_calculation,
                Step.label_or_predict,
                Step.save
            ], [
                self.preprocessing,
                self.group_by,
                self.feature_calculation,
                self.label_or_predict,
                self.save
            ]))

        self.remaining_steps = list(self.step_to_action.keys())

        self.time_bucket = TimeBucket(self.engine_conf.time_bucket)
        self.model_manager = ModelManager(self.db_conf, self.engine_conf)
        self.feature_manager = FeatureManager(self.engine_conf)

    def load_test(self):
        &#34;&#34;&#34;
        If the user has set the load_test configuration, then multiply the
        traffic by `self.engine_conf.load_test` times to do load testing.
        :return:
        &#34;&#34;&#34;
        if self.engine_conf.load_test:
            df = self.logs_df.persist(self.spark_conf.storage_level)

            for i in range(self.engine_conf.load_test - 1):
                df = df.withColumn(
                    &#39;client_ip&#39;, F.round(F.rand(42)).cast(&#39;string&#39;)
                )
                self.logs_df = self.logs_df.union(df).persist(
                    self.spark_conf.storage_level
                )

            df.unpersist()
            del df
            self.logger.info(
                f&#39;---- Count after multiplication: {self.logs_df.count()}&#39;
            )

    def reset(self):
        &#34;&#34;&#34;
        Unpersist rdds and dataframes and call GC - see broadcast memory
        release issue
        :return:
        &#34;&#34;&#34;
        global CLIENT_REQUEST_SET_COUNT, CLIENT_PREDICTION_ACCUMULATOR
        import gc

        for (id, rdd_) in list(
                self.spark.sparkContext._jsc.getPersistentRDDs().items()
        )[:]:
            rdd_.unpersist()
            del rdd_

        self.spark.catalog.clearCache()
        # self.spark.sparkContext._jvm.System.gc()
        gc.collect()

        # CLIENT_REQUEST_SET_COUNT.value = defaultdict(int)
        # CLIENT_PREDICTION_ACCUMULATOR.value = defaultdict(int)

    def set_broadcasts(self):
        global CLF, SCL, MF, FEATURE_NAMES, ACTIVE_FEATURES

        if self.model_manager.ml_model:
            # We can use the classifier and scaler as objects since they are
            # picklable.
            CLF = self.model_manager.get_classifier_broadcast()
            SCL = self.model_manager.get_scaler_broadcast()
            MF = self.model_manager.get_model_features_broadcast()
        FEATURE_NAMES = self.spark.sparkContext.broadcast(
            self.feature_manager.active_feature_names)
        ACTIVE_FEATURES = self.spark.sparkContext.broadcast([  # #WST
            StrippedFeature(f.feature_name, f.update_row)
            for f in self.feature_manager.active_features
        ])

    def set_accumulators(self):
        global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

        # todo: conditional set
        CLIENT_PREDICTION_ACCUMULATOR = self.spark.sparkContext.accumulator(
            defaultdict(float), DictAccumulatorParam(defaultdict(float))
        )
        CLIENT_REQUEST_SET_COUNT = self.spark.sparkContext.accumulator(
            defaultdict(int), DictAccumulatorParam(defaultdict(int))
        )

    def initialize(self):
        &#34;&#34;&#34;
        Set the basics:
        - Connect to the database
        - Initialize spark session
        - Get active model and scaler and set them to broadcast variables
        - Get active features with their active columns, update columns etc and
        set the relevant broadcast variables
        - Set the _can_predict flag
        - Instantiate the accumulators (for metrics)
        - Instantiate request set cache.
        :return:
        &#34;&#34;&#34;

        # initialize db access tools
        self.tools = BaskervilleDBTools(self.db_conf)
        self.tools.connect_to_db()

        # initialize spark session
        self.spark = self.instantiate_spark_session()

        # todo: use a class for the feature related stuff
        self.model_manager.initialize(self.spark, self.tools)
        self.feature_manager.initialize(self.model_manager)

        self._can_predict = self.feature_manager.feature_config_is_valid()
        self.drop_if_missing_filter = self.data_parser.drop_if_missing_filter()

        # set broadcasts
        self.set_broadcasts()
        self.set_accumulators()

        # set up cache
        self.request_set_cache = self.set_up_request_set_cache()

        # gather calculations
        self.group_by_aggs = self.get_group_by_aggs()
        self.columns_to_filter_by = self.get_columns_to_filter_by()
        self.cols_to_drop = set(
            self.feature_manager.active_feature_names +
            self.feature_manager.active_columns +
            list(self.group_by_aggs.keys()) +
            self.feature_manager.update_feature_cols
        ).difference(RequestSet.columns)

        self._is_initialized = True

    def get_columns_to_filter_by(self):
        &#34;&#34;&#34;
        Gathers all the columns that need to be present in the dataframe
        for the processing to complete.
        group_by_cols: the columns to group data on
        active_columns: the columns that the active features have declared as
        necessary
        timestamp_column: the time column - all logs need to have a time column
        :return: a set of the column names that need to be present in the
        dataframe
        :rtype: set[str]
        &#34;&#34;&#34;
        cols = self.group_by_cols + self.feature_manager.active_columns
        cols.append(self.engine_conf.data_config.timestamp_column)
        return set(cols)

    def get_group_by_aggs(self):
        &#34;&#34;&#34;
        Gathers all the group by arguments:
        basic_aggs:
            - first_request
            - last_request
            - num_requests
        column_aggs: the columns the features need for computation are gathered
         as lists
        feature_aggs: the columns the features need for computation
        Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
        The basic aggs have a priority over the feature and column aggs.
        The feature aggs have a priority over the column aggs (if a feature
        has explicitly asked for a computation for a specific column it relies
        upon, then the computation will be stored instead of the column
        aggregation as list)

        :return: a dictionary with the name of the group by aggregation columns
        as keys and the respective Column aggregation as values
        :rtype: dict[string, pyspark.Column]
        &#34;&#34;&#34;
        basic_aggs = {
            &#39;first_request&#39;: F.min(F.col(&#39;@timestamp&#39;)).alias(&#39;first_request&#39;),
            &#39;last_request&#39;: F.max(F.col(&#39;@timestamp&#39;)).alias(&#39;last_request&#39;),
            &#39;num_requests&#39;: F.count(F.col(&#39;@timestamp&#39;)).alias(&#39;num_requests&#39;)
        }

        column_aggs = {
            c: F.collect_list(F.col(c)).alias(c)
            for c in self.feature_manager.active_columns
        }

        feature_aggs = self.feature_manager.get_feature_group_by_aggs()

        basic_aggs.update(
            {k: v for k, v in feature_aggs.items() if k not in basic_aggs}
        )
        basic_aggs.update(
            {k: v for k, v in column_aggs.items() if k not in basic_aggs}
        )

        return basic_aggs

    def get_post_group_by_calculations(self):
        &#34;&#34;&#34;
        Gathers the columns and computations to be performed after the grouping
        of the data (logs_df)
        Basic post group by columns:
        - `id_runtime`
        - `time_bucket`
        - `start`
        - `stop`
        - `subset_count`

        if there is an ML Model defined:
        - `model_version`
        - `classifier`
        - `scaler`
        - `model_features`

        Each feature can also define post group by calculations using the
        post_group_by_calcs dict.

        :return: A dictionary with the name of the result columns as keys and
        their respective computations as values
        :rtype: dict[string, pyspark.Column]
        &#34;&#34;&#34;
        if self.post_group_by_aggs:
            return self.post_group_by_aggs

        post_group_by_columns = {
            &#39;id_runtime&#39;: F.lit(self.runtime.id),
            &#39;time_bucket&#39;: F.lit(self.time_bucket.sec),
            &#39;start&#39;: F.when(
                F.col(&#39;first_ever_request&#39;).isNotNull(),
                F.col(&#39;first_ever_request&#39;)
            ).otherwise(F.col(&#39;first_request&#39;)),
            &#39;stop&#39;: F.col(&#39;last_request&#39;),
            &#39;subset_count&#39;: F.when(
                F.col(&#39;old_subset_count&#39;).isNotNull(),
                F.col(&#39;old_subset_count&#39;)
            ).otherwise(F.lit(0))
        }

        if self.model_manager.ml_model:
            post_group_by_columns[&#39;model_version&#39;] = F.lit(
                self.model_manager.ml_model.id
            )

        # todo: what if a feature defines a column name that already exists?
        # e.g. like `subset_count`
        post_group_by_columns.update(
            self.feature_manager.post_group_by_calculations
        )

        return post_group_by_columns

    def __getitem__(self, name):
        if name == &#39;run&#39;:
            if not self._is_initialized:
                raise RuntimeError(
                    f&#39;__getitem__: {self.__class__.__name__} &#39;
                    f&#39;has not been initialized yet.&#39;
                )
        return getattr(self, name)

    def __getattribute__(self, name):
        if name == &#39;run&#39;:
            if not self._is_initialized:
                raise RuntimeError(
                    f&#39;__getattribute__:{self.__class__.__name__} &#39;
                    f&#39;has not been initialized yet.&#39;
                )

        return super().__getattribute__(name)

    def filter_cache(self):
        &#34;&#34;&#34;
        Use the current logs to find the past request sets - if any - in the
        request set cache
        :return:
        &#34;&#34;&#34;
        df = self.logs_df.select(
            F.col(&#39;client_request_host&#39;).alias(&#39;target&#39;),
            F.col(&#39;client_ip&#39;).alias(&#39;ip&#39;),
        ).distinct().alias(&#39;a&#39;).persist(self.spark_conf.storage_level)

        self.request_set_cache.filter_by(df)

        df.unpersist()
        del df

    def run(self):
        &#34;&#34;&#34;
        Runs the configured steps.
        :return:
        &#34;&#34;&#34;
        self.logger.info(
            f&#39;Spark UI accessible at:{self.spark.sparkContext.uiWebUrl()}&#39;
        )
        self.create_runtime()
        self.get_data()
        self.process_data()

    def process_data(self):
        &#34;&#34;&#34;
        Splits the data into time bucket lenght windows and executes all the steps
        :return:
        &#34;&#34;&#34;
        if self.logs_df.count() == 0:
            self.logger.info(&#39;No data in to process.&#39;)
        else:
            for window_df in self.get_window():
                self.logs_df = window_df
                self.logs_df = self.logs_df.repartition(
                    &#39;client_request_host&#39;,
                    &#39;client_ip&#39;
                ).persist(self.spark_conf.storage_level)
                self.remaining_steps = list(self.step_to_action.keys())
                for step, action in self.step_to_action.items():
                    self.logger.info(&#39;Starting step {}&#39;.format(step))
                    action()
                    self.logger.info(&#39;Completed step {}&#39;.format(step))
                    self.remaining_steps.remove(step)

    def get_window(self):

        from pyspark.sql import functions as F

        df = self.logs_df.withColumn(&#39;timestamp&#39;,
                                     F.col(&#39;@timestamp&#39;).cast(&#39;timestamp&#39;))
        df = df.sort(&#39;timestamp&#39;)
        current_window_start = df.agg({&#34;timestamp&#34;: &#34;min&#34;}).collect()[0][0]
        stop = df.agg({&#34;timestamp&#34;: &#34;max&#34;}).collect()[0][0]
        window_df = None
        current_end = current_window_start + self.time_bucket.td

        while True:
            if window_df:
                window_df.unpersist(blocking=True)
                del window_df
            filter_ = (
                    (F.col(&#39;timestamp&#39;) &gt;= current_window_start) &amp;
                    (F.col(&#39;timestamp&#39;) &lt; current_end)
            )
            window_df = df.where(filter_).persist(
                self.spark_conf.storage_level
            )
            if not window_df.rdd.isEmpty():
                print(f&#39;# Request sets = {window_df.count()}&#39;)
                yield window_df
            else:
                self.logger.info(f&#39;Empty window df for {str(filter_._jc)}&#39;)
            current_window_start = current_window_start + self.time_bucket.td
            current_end = current_window_start + self.time_bucket.td
            if current_window_start &gt;= stop:
                window_df.unpersist(blocking=True)
                del window_df
                break

    def create_runtime(self):
        &#34;&#34;&#34;
        Create a Runtime in the Baskerville database.
        :return:
        &#34;&#34;&#34;
        raise NotImplementedError(
            &#39;SparkPipelineBase does not have an implementation &#39;
            &#39;for _create_runtime.&#39;
        )

    def get_data(self):
        &#34;&#34;&#34;
        Get dataframe of log data
        :return:
        &#34;&#34;&#34;
        raise NotImplementedError(
            &#39;SparkPipelineBase does not have an implementation &#39;
            &#39;for _get_data.&#39;
        )

    def preprocessing(self):
        &#34;&#34;&#34;
        Fill missing values, add calculation cols, and filter.
        :return:
        &#34;&#34;&#34;

        self.handle_missing_columns()
        self.rename_columns()
        self.filter_columns()
        self.handle_missing_values()
        self.normalize_host_names()
        self.add_calc_columns()

    def group_by(self):
        &#34;&#34;&#34;
        Group the logs df by the given group-by columns (normally IP, host).
        :return: None
        &#34;&#34;&#34;
        self.logs_df = self.logs_df.groupBy(
            *self.group_by_cols
        ).agg(
            *self.group_by_aggs.values()
        )

    def feature_calculation(self):
        &#34;&#34;&#34;
        Add calculation cols, extract features, and update.
        :return:
        &#34;&#34;&#34;
        self.add_post_groupby_columns()
        self.feature_extraction()
        self.feature_update()

    def label_or_predict(self):
        &#34;&#34;&#34;
        Apply label from MISP or predict label.
        #todo: use separate steps for this
        :return:
        &#34;&#34;&#34;

        from pyspark.sql import functions as F
        from pyspark.sql.types import IntegerType

        if self.engine_conf.cross_reference:
            self.cross_reference()
        else:
            self.logs_df = self.logs_df.withColumn(
                &#39;label&#39;, F.lit(None).cast(IntegerType()))

        if not self._can_predict:
            self.logger.warn(
                &#39;Active features do not match model features, &#39;
                &#39;skipping prediction&#39;
            )
            self.logs_df = self.logs_df.withColumn(
                &#39;prediction&#39;, F.lit(None).cast(IntegerType()))
            self.logs_df = self.logs_df.withColumn(
                &#39;r&#39;, F.lit(None).cast(IntegerType()))
        elif not self.model_manager.ml_model:
            self.logger.warn(
                &#39;No ml model specified, &#39;
                &#39;skipping prediction&#39;
            )
            self.logs_df = self.logs_df.withColumn(
                &#39;prediction&#39;, F.lit(None).cast(IntegerType()))
            self.logs_df = self.logs_df.withColumn(
                &#39;r&#39;, F.lit(None).cast(IntegerType()))
        else:
            self.predict()

    def save(self):
        &#34;&#34;&#34;
        Update dataframe, save to database, and update cache.
        :return:
        &#34;&#34;&#34;
        request_set_columns = RequestSet.columns[:]
        not_common = {
            &#39;prediction&#39;, &#39;r&#39;, &#39;model_version&#39;, &#39;label&#39;, &#39;id_attribute&#39;,
            &#39;updated_at&#39;
        }.difference(self.logs_df.columns)

        for c in not_common:
            request_set_columns.remove(c)

        # filter the logs df with the request_set columns
        self.logs_df = self.logs_df.select(request_set_columns)

        # save request_sets
        self.logger.debug(&#39;Saving request_sets&#39;)
        self.save_df_to_table(
            self.logs_df.select(request_set_columns),
            RequestSet.__tablename__
        )
        self.refresh_cache()

    def refresh_cache(self):
        &#34;&#34;&#34;
        Update the cache with the current batch of logs in logs_df and clean up
        :return:
        &#34;&#34;&#34;
        self.request_set_cache.update_self(self.logs_df)
        self.logs_df.unpersist()
        self.logs_df = None
        # self.spark.catalog.clearCache()

    def finish_up(self):
        &#34;&#34;&#34;
        Try to gracefully stop by committing to database, emptying the cache,
        unpersist everything, disconnecting from db and clearing spark&#39;s cache
        :return: None
        &#34;&#34;&#34;
        if len(self.remaining_steps) == 0:
            request_set_count = self.tools.session.query(RequestSet).filter(
                RequestSet.id_runtime == self.runtime.id).count()
            self.runtime.processed = True
            self.runtime.n_request_sets = request_set_count
            self.tools.session.commit()
            self.logger.debug(&#39;finished updating runtime&#39;)
        try:
            self.request_set_cache.empty()
        except AttributeError:
            pass

        if hasattr(self, &#39;logs_df&#39;) and self.logs_df and isinstance(
                self.logs_df, DataFrame):
            self.logs_df.unpersist(blocking=True)
            self.logs_df = None

        if self.tools and self.tools.session:
            self.tools.session.commit()
            self.tools.disconnect_from_db()
        if self.spark:
            try:
                if self.spark_conf.spark_python_profile:
                    self.spark.sparkContext.show_profiles()
                self.spark.catalog.clearCache()
            except AttributeError:
                self.logger.debug(&#39;clearCache attr error&#39;)
                pass

    def instantiate_spark_session(self):
        &#34;&#34;&#34;
        #todo
        :return:
        &#34;&#34;&#34;
        return get_or_create_spark_session(self.spark_conf)

    def set_up_request_set_cache(self):
        &#34;&#34;&#34;
        Set up an instance of RequestSetSparkCache using the cache
        configuration. Also, load past (start_time - expiry_time)
        if cache_load_past is configured.
        :return:
        &#34;&#34;&#34;

        self.request_set_cache = RequestSetSparkCache(
            cache_config=self.cache_config,
            table_name=RequestSet.__tablename__,
            columns_to_keep=(
                &#39;target&#39;,
                &#39;ip&#39;,
                F.col(&#39;start&#39;).alias(&#39;first_ever_request&#39;),
                F.col(&#39;subset_count&#39;).alias(&#39;old_subset_count&#39;),
                F.col(&#39;features&#39;).alias(&#39;old_features&#39;),
                F.col(&#39;num_requests&#39;).alias(&#39;old_num_requests&#39;),
                F.col(&#39;updated_at&#39;)
            ),
            expire_if_longer_than=self.engine_conf.cache_expire_time

        )
        if self.engine_conf.cache_load_past:
            self.request_set_cache = self.request_set_cache.load(
                update_date=(
                    self.start_time - datetime.timedelta(
                        seconds=self.engine_conf.cache_expire_time
                    )
                ).replace(tzinfo=tzutc()),
                extra_filters=(
                        F.col(&#39;time_bucket&#39;) == self.time_bucket.sec
                )  # todo: &amp; (F.col(&#34;id_runtime&#34;) == self.runtime.id)?
            )
        else:
            schema = T.StructType([
                T.StructField(&#34;id&#34;, T.IntegerType(), False),
                T.StructField(&#34;target&#34;, T.StringType(), False),
                T.StructField(&#34;ip&#34;, T.StringType(), False),
                T.StructField(&#34;first_ever_request&#34;, T.TimestampType(), True),
                T.StructField(&#34;old_subset_count&#34;, T.IntegerType(), True),
                T.StructField(&#34;old_features&#34;,
                              T.MapType(T.StringType(), T.FloatType()), True),
                T.StructField(&#34;old_num_requests&#34;, T.IntegerType(), True),
                T.StructField(&#34;updated_at&#34;, T.TimestampType(), True)
            ])
            self.request_set_cache.load_empty(schema)

        self.logger.info(f&#39;In cache: {self.request_set_cache.count()}&#39;)

        return self.request_set_cache

    def handle_missing_columns(self):
        &#34;&#34;&#34;
        Check for missing columns and if any use the data parser to add them
        and fill them with defaults, if specified in the schema.
        :return:
        &#34;&#34;&#34;
        missing = self.data_parser.check_for_missing_columns(self.logs_df)
        if missing:
            self.logs_df = self.data_parser.add_missing_columns(
                self.logs_df, missing
            )

    def rename_columns(self):
        &#34;&#34;&#34;
        Some column names may cause issues with spark, e.g. `geo.ip.lat`, so
        the features that use those can declare in `columns_renamed` that those
        columns should be renamed to something else, e.g. `geo_ip_lat`
        :return:
        &#34;&#34;&#34;
        for k, v in self.feature_manager.column_renamings:
            self.logs_df = self.logs_df.withColumnRenamed(k, v)

    def filter_columns(self):
        &#34;&#34;&#34;
        Logs df may have columns that are not necessary for the analysis,
        filter them out to reduce the memory footprint.
        The absolutely essential columns are the group by columns and the
        timestamp column, or else the rest of the process will fail.
        And of course the columns the features need, the active columns.
        :return:None
        &#34;&#34;&#34;

        where = self.drop_if_missing_filter

        self.logs_df = self.logs_df.select(*self.columns_to_filter_by)
        if where is not None:
            self.logs_df = self.logs_df.where(where)

        # todo: metric for dropped logs
        print(f&#39;{self.logs_df.count()}&#39;)

    def handle_missing_values(self):
        self.logs_df = self.data_parser.fill_missing_values(self.logs_df)

    def normalize_host_names(self):
        &#34;&#34;&#34;
        From www.somedomain.tld keep somedomain
        # todo: improve this and remove udf
        # todo: keep original target in a separate field in db
        :return:
        &#34;&#34;&#34;
        from baskerville.spark.udfs import udf_normalize_host_name

        self.logs_df = self.logs_df.withColumn(
            &#39;client_request_host&#39;,
            udf_normalize_host_name(
                F.col(&#39;client_request_host&#39;).cast(T.StringType())
            )
        )

    def add_calc_columns(self):
        &#34;&#34;&#34;
        Each feature needs different calculations in order to be able to
        compute the feature value. Go through the features and apply the
        calculations. Each calculation can occur only once, calculations
        with the same name will be ignored.
        :return:
        &#34;&#34;&#34;

        self.logs_df = self.logs_df.withColumn(
            &#39;@timestamp&#39;, F.col(&#39;@timestamp&#39;).cast(&#39;timestamp&#39;)
        )

        for k, v in self.feature_manager.pre_group_by_calculations.items():
            self.logs_df = self.logs_df.withColumn(
                k, v
            )

        for f in self.feature_manager.active_features:
            self.logs_df = f.misc_compute(self.logs_df)

    def add_cache_columns(self):
        &#34;&#34;&#34;
        Add columns from the cache to facilitate the
        feature extraction, prediction, and save processes
        :return:
        &#34;&#34;&#34;
        self.logs_df = self.logs_df.alias(&#39;logs_df&#39;)
        self.filter_cache()
        self.logs_df = self.request_set_cache.update_df(
            self.logs_df, select_cols=self.cache_columns
        )
        self.logger.debug(
            f&#39;****** &gt; # of rows in cache: {self.request_set_cache.count()}&#39;)

    def add_post_groupby_columns(self):
        &#34;&#34;&#34;
        Add extra columns after the grouping of the logs to facilitate the
        feature extraction, prediction, and save processes
        Extra columns:
        * general:
        ----------
        - ip
        - target
        - id_runtime
        - time_bucket
        - start
        - subset_count

        * cache columns:
        ----------------
        - &#39;id&#39;,
        - &#39;first_ever_request&#39;,
        - &#39;old_subset_count&#39;,
        - &#39;old_features&#39;,
        - &#39;old_num_requests&#39;

        * model related:
        ----------------
        - model_version
        - classifier
        - scaler
        - model_features

        :return: None
        &#34;&#34;&#34;
        # todo: shouldn&#39;t this be a renaming?
        self.logs_df = self.logs_df.withColumn(&#39;ip&#39;, F.col(&#39;client_ip&#39;))
        self.logs_df = self.logs_df.withColumn(
            &#39;target&#39;, F.col(&#39;client_request_host&#39;)
        )
        self.add_cache_columns()

        for k, v in self.get_post_group_by_calculations().items():
            self.logs_df = self.logs_df.withColumn(k, v)

        self.logs_df = self.logs_df.drop(&#39;old_subset_count&#39;)

    def feature_extraction(self):
        &#34;&#34;&#34;
        For each feature compute the feature value and add it as a column in
        the dataframe
        :return: None
        &#34;&#34;&#34;

        for feature in self.feature_manager.active_features:
            self.logs_df = feature.compute(self.logs_df)

        self.logger.info(
            f&#39;Number of logs after feature extraction {self.logs_df.count()}&#39;
        )
        # self.logs_df = self.logs_df.cache()

    def feature_update(self):
        &#34;&#34;&#34;
        Update current batch&#39;s features with past features - if any - using
        the request set cache.
        :return:
        &#34;&#34;&#34;
        # convert current features to dict since the already saved request_sets
        # have the features as json
        self.features_to_dict(&#39;features&#39;)
        self.features_to_dict(&#39;old_features&#39;)
        self.logs_df = self.logs_df.drop(
            *self.feature_manager.active_feature_names).persist(
            self.spark_conf.storage_level)
        for f in self.feature_manager.updateable_active_features:
            self.logs_df = f.update(self.logs_df).cache()

        # self.logs_df = self.logs_df.persist(self.spark_conf.storage_level)
        self.logs_df = self.logs_df.withColumn(&#39;features&#39;, F.create_map(
            *list(
                itertools.chain(
                    *[
                        (F.lit(f.feature_name),
                         F.col(f.updated_feature_col_name))
                        for f in
                        self.feature_manager.updateable_active_features
                    ]
                )
            )
        ))

        # older way with a udf:
        # self.logs_df = self.logs_df.withColumn(
        #     &#39;features&#39;,
        #     udf_update_features(
        #         F.lit(cPickle.dumps([  # #WST
        #             StrippedFeature(f.feature_name, f.update_row)
        #             for f in self.feature_manager.active_features
        #         ]
        #         )),
        #         &#39;features&#39;,
        #         &#39;old_features&#39;,
        #         &#39;subset_count&#39;,
        #         &#39;start&#39;,
        #         &#39;last_request&#39;
        #     )
        # ).persist(self.spark_conf.storage_level)
        self.logs_df = self.logs_df.drop(&#39;old_features&#39;)

        self.logs_df = self.logs_df.withColumn(
            &#39;subset_count&#39;,
            F.col(&#39;subset_count&#39;) + F.lit(1)
        )

        self.logs_df = self.logs_df.withColumn(
            &#39;num_requests&#39;,
            F.when(
                F.col(&#39;old_num_requests&#39;) &gt; 0,
                F.col(&#39;old_num_requests&#39;) + F.col(&#39;num_requests&#39;)
            ).otherwise(F.col(&#39;num_requests&#39;))
        )
        self.logs_df = self.logs_df.drop(&#39;old_num_requests&#39;)
        diff = (F.unix_timestamp(&#39;last_request&#39;, format=&#34;YYYY-MM-DD %H:%M:%S&#34;)
                - F.unix_timestamp(
                    &#39;start&#39;, format=&#34;YYYY-MM-DD %H:%M:%S&#34;)
                ).cast(&#39;float&#39;)
        self.logs_df = self.logs_df.withColumn(&#39;total_seconds&#39;, diff)
        self.logs_df = self.logs_df.drop(*self.cols_to_drop)

    def cross_reference(self):
        &#34;&#34;&#34;
        Look up IPs in attributes table, and label as malicious (-1) if listed
        there.
        :return:
        &#34;&#34;&#34;
        from pyspark.sql import functions as F
        from baskerville.spark.udfs import udf_cross_reference_misp

        self.logs_df = self.logs_df.withColumn(
            &#39;cross_reference&#39;,
            udf_cross_reference_misp(&#39;ip&#39;,
                                     F.lit(json.dumps(self.db_conf.__dict__)))
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;label&#39;,
            F.when(F.col(&#39;cross_reference.label&#39;) != 0,
                   F.col(&#39;cross_reference.label&#39;)).otherwise(None)
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;id_attribute&#39;,
            F.when(F.col(&#39;cross_reference.id_attribute&#39;) != 0,
                   F.col(&#39;cross_reference.id_attribute&#39;)).otherwise(None)
        )

    def predict(self):
        &#34;&#34;&#34;
        Predict on the request_sets. Prediction on request_sets
        requires feature averaging where there is an existing request_set.
`        :return: None
        &#34;&#34;&#34;
        global CLF, SCL, MF, CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

        self.logs_df = self.logs_df.withColumn(
            &#39;y&#39;,
            udf_predict_dict(
                &#39;target&#39;,
                &#39;features&#39;,
                F.lit(self.engine_conf.metrics is not None)
                # todo: will fail if metrics missing in the conf file
            )
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;prediction&#39;, F.col(&#39;y.prediction&#39;)
        )
        self.logs_df = self.logs_df.withColumn(&#39;r&#39;, F.col(&#39;y.r&#39;))
        self.logs_df = self.logs_df.drop(&#39;y&#39;)

    def save_df_to_table(
            self, df, table_name, json_cols=(&#39;features&#39;,), mode=&#39;append&#39;
    ):
        &#34;&#34;&#34;
        Save the dataframe to the database. Jsonify any columns that need to
        be
        :param pyspark.Dataframe df: the dataframe to save
        :param str table_name: where to save the dataframe
        :param iterable json_cols: the name of the columns that need to be
        jsonified, e.g. `features`
        :param str mode: save mode, &#39;append&#39;, &#39;overwrite&#39; etc, see spark save
        options
        :return: None
        &#34;&#34;&#34;
        df = df.persist(self.spark_conf.storage_level)
        for c in json_cols:
            df = col_to_json(df, c)
        df.write.format(&#39;jdbc&#39;).options(
            url=self.db_url,
            driver=self.spark_conf.db_driver,
            dbtable=table_name,
            user=self.db_conf.user,
            password=self.db_conf.password,
            stringtype=&#39;unspecified&#39;,
            batchsize=100000,
            max_connections=1250,
            rewriteBatchedStatements=True,
            reWriteBatchedInserts=True,
            useServerPrepStmts=False,
        ).mode(mode).save()

    def features_to_dict(self, column):
        &#34;&#34;&#34;
        Convert the features to dictionary
        :param string column: the name of the column to use as output
        :return: None
        &#34;&#34;&#34;
        from pyspark.sql import functions as F

        self.logs_df = self.logs_df.withColumn(
            column,
            F.create_map(
                *list(
                    itertools.chain(
                        *[
                            (F.lit(f.feature_name), F.col(f.feature_name))
                            for f in self.feature_manager.active_features
                        ]
                    )
                ))
        )

    def register_metrics(self):
        &#34;&#34;&#34;
        Registers action hooks for:
        - step_to_action: enum metric to show the progress
        - get_data:
            - &#39;total_number_of_requests&#39;: increments the metric counter by
            the self.logs_df.count()
            - &#39;current_number_of_requests&#39;: sets the metric counter to the
            current self.logs_df.count()
        - filter_columns:
            - `total number of requests after filtering`
        - group_by:
            - `current_number_of_request_sets`:increments the metric counter by
            the self.logs_df.count()

        - save_df_to_table: average host prediction

        :return: None
        &#34;&#34;&#34;
        from baskerville.models.metrics.registry import metrics_registry
        from baskerville.util.enums import MetricClassEnum
        from baskerville.models.metrics.helpers import (
            incr_counter_for_logs_df,
            update_avg_hosts_counter,
            set_counter_for_logs_df
        )

        # wrap step_to_action
        self.step_to_action = metrics_registry.register_states(
            &#39;baskerville_steps&#39;,
            &#39;Tracks which step Baskerville is currently executing&#39;,
            self.step_to_action
        )

        # total number of requests in current df
        get_data = metrics_registry.register_action_hook(
            self.get_data,
            incr_counter_for_logs_df,
            metric_name=&#39;total_number_of_requests&#39;
        )
        # current number of requests in current df
        get_data = metrics_registry.register_action_hook(
            get_data,
            set_counter_for_logs_df,
            metric_cls=MetricClassEnum.gauge,
            metric_name=&#39;current_number_of_requests&#39;
        )
        # current number of requests in current df
        filter_columns = metrics_registry.register_action_hook(
            self.filter_columns,
            incr_counter_for_logs_df,
            metric_cls=MetricClassEnum.counter,
            metric_name=&#39;total_number_of_requests_after_filtering&#39;
        )
        # number of request_sets in current df after group by
        group_by = metrics_registry.register_action_hook(
            self.group_by,
            incr_counter_for_logs_df,
            metric_name=&#39;current_number_of_request_sets&#39;
        )

        # set the average prediction for each host, per batch
        save_df_to_table = metrics_registry.register_action_hook(
            self.save_df_to_table,
            update_avg_hosts_counter,
            metric_cls=MetricClassEnum.gauge,
            metric_name=&#39;avg_host_prediction&#39;,
            labelnames=[&#39;target&#39;]
        )

        # set wrapped methods
        setattr(self, &#39;get_data&#39;, get_data)
        setattr(self, &#39;filter_columns&#39;, filter_columns)
        setattr(self, &#39;group_by&#39;, group_by)
        setattr(self, &#39;save_df_to_table&#39;, save_df_to_table)

        self.logger.info(&#39;Registered metrics.&#39;)

        if self.engine_conf.metrics.exported_dashboard_file:
            from baskerville.models.metrics.dashboard_exporter import \
                DashboardExporter
            dashboard = DashboardExporter(&#39;Baskerville Metrics&#39;)
            dashboard.export(self.engine_conf.metrics.exported_dashboard_file)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="baskerville.models.base_spark.predict_dict"><code class="name flex">
<span>def <span class="ident">predict_dict</span></span>(<span>target, dict_features, update_metrics=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Scale the feature values and use the model to predict
:param dict[str, float] dict_features: the feature dictionary
:return: 0 if normal, 1 if abnormal, -1 if something went wrong</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_dict(target, dict_features, update_metrics=False):
    &#34;&#34;&#34;
    Scale the feature values and use the model to predict
    :param dict[str, float] dict_features: the feature dictionary
    :return: 0 if normal, 1 if abnormal, -1 if something went wrong
    &#34;&#34;&#34;
    global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT
    import numpy as np
    import json

    prediction = 0, 0.

    if isinstance(dict_features, str):
        dict_features = json.loads(dict_features)

    x_test = [extract_features_in_order(dict_features, MF.value)]
    try:
        x_test = SCL.value.transform(x_test)
        y = CLF.value.decision_function(x_test)
        prediction = np.sign(y)[0]
        r = np.float32(np.absolute(y)[0])
        prediction = float(prediction), float(r)
    except ValueError:
        import traceback
        traceback.print_exc()
        print(&#39;Cannot predict:&#39;, x_test)

    if update_metrics:
        CLIENT_REQUEST_SET_COUNT += {target: 1}
        CLIENT_PREDICTION_ACCUMULATOR += {target: prediction[0]}

    return prediction</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.udf_predict_dict"><code class="name flex">
<span>def <span class="ident">udf_predict_dict</span></span>(<span>target, dict_features, update_metrics=False)</span>
</code></dt>
<dd>
<section class="desc"><p>Scale the feature values and use the model to predict
:param dict[str, float] dict_features: the feature dictionary
:return: 0 if normal, 1 if abnormal, -1 if something went wrong</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_dict(target, dict_features, update_metrics=False):
    &#34;&#34;&#34;
    Scale the feature values and use the model to predict
    :param dict[str, float] dict_features: the feature dictionary
    :return: 0 if normal, 1 if abnormal, -1 if something went wrong
    &#34;&#34;&#34;
    global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT
    import numpy as np
    import json

    prediction = 0, 0.

    if isinstance(dict_features, str):
        dict_features = json.loads(dict_features)

    x_test = [extract_features_in_order(dict_features, MF.value)]
    try:
        x_test = SCL.value.transform(x_test)
        y = CLF.value.decision_function(x_test)
        prediction = np.sign(y)[0]
        r = np.float32(np.absolute(y)[0])
        prediction = float(prediction), float(r)
    except ValueError:
        import traceback
        traceback.print_exc()
        print(&#39;Cannot predict:&#39;, x_test)

    if update_metrics:
        CLIENT_REQUEST_SET_COUNT += {target: 1}
        CLIENT_PREDICTION_ACCUMULATOR += {target: prediction[0]}

    return prediction</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="baskerville.models.base_spark.SparkPipelineBase"><code class="flex name class">
<span>class <span class="ident">SparkPipelineBase</span></span>
<span>(</span><span>db_conf, engine_conf, spark_conf, clean_up=True, group_by_cols=('client_request_host', 'client_ip'), *args, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>The base class for all pipelines that use spark. It initializes spark
session and provides basic implementation for some of the main methods</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SparkPipelineBase(PipelineBase):
    &#34;&#34;&#34;
    The base class for all pipelines that use spark. It initializes spark
    session and provides basic implementation for some of the main methods
    &#34;&#34;&#34;

    def __init__(self,
                 db_conf,
                 engine_conf,
                 spark_conf,
                 clean_up=True,
                 group_by_cols=(&#39;client_request_host&#39;, &#39;client_ip&#39;),
                 *args,
                 **kwargs
                 ):

        super().__init__(db_conf, engine_conf, clean_up)
        self.start_time = datetime.datetime.utcnow()
        self.request_set_cache = None
        self.spark = None
        self.tools = None
        self.metrics_registry = None
        self.spark_conf = spark_conf
        self.data_parser = self.engine_conf.data_config.parser
        self.group_by_cols = list(set(group_by_cols))
        self.group_by_aggs = None
        self.post_group_by_aggs = None
        self.columns_to_filter_by = None
        self._can_predict = False
        self._is_initialized = False
        self.drop_if_missing_filter = None
        self.cols_to_drop = None
        self.cache_columns = [
            &#39;target&#39;,
            &#39;ip&#39;,
            &#39;first_ever_request&#39;,
            &#39;old_subset_count&#39;,
            &#39;old_features&#39;,
            &#39;old_num_requests&#39;,
        ]
        self.cache_config = {
            &#39;db_url&#39;: self.db_url,
            &#39;db_driver&#39;: self.spark_conf.db_driver,
            &#39;user&#39;: self.db_conf.user,
            &#39;password&#39;: self.db_conf.password
        }
        self.step_to_action = OrderedDict(
            zip([
                Step.preprocessing,
                Step.group_by,
                Step.feature_calculation,
                Step.label_or_predict,
                Step.save
            ], [
                self.preprocessing,
                self.group_by,
                self.feature_calculation,
                self.label_or_predict,
                self.save
            ]))

        self.remaining_steps = list(self.step_to_action.keys())

        self.time_bucket = TimeBucket(self.engine_conf.time_bucket)
        self.model_manager = ModelManager(self.db_conf, self.engine_conf)
        self.feature_manager = FeatureManager(self.engine_conf)

    def load_test(self):
        &#34;&#34;&#34;
        If the user has set the load_test configuration, then multiply the
        traffic by `self.engine_conf.load_test` times to do load testing.
        :return:
        &#34;&#34;&#34;
        if self.engine_conf.load_test:
            df = self.logs_df.persist(self.spark_conf.storage_level)

            for i in range(self.engine_conf.load_test - 1):
                df = df.withColumn(
                    &#39;client_ip&#39;, F.round(F.rand(42)).cast(&#39;string&#39;)
                )
                self.logs_df = self.logs_df.union(df).persist(
                    self.spark_conf.storage_level
                )

            df.unpersist()
            del df
            self.logger.info(
                f&#39;---- Count after multiplication: {self.logs_df.count()}&#39;
            )

    def reset(self):
        &#34;&#34;&#34;
        Unpersist rdds and dataframes and call GC - see broadcast memory
        release issue
        :return:
        &#34;&#34;&#34;
        global CLIENT_REQUEST_SET_COUNT, CLIENT_PREDICTION_ACCUMULATOR
        import gc

        for (id, rdd_) in list(
                self.spark.sparkContext._jsc.getPersistentRDDs().items()
        )[:]:
            rdd_.unpersist()
            del rdd_

        self.spark.catalog.clearCache()
        # self.spark.sparkContext._jvm.System.gc()
        gc.collect()

        # CLIENT_REQUEST_SET_COUNT.value = defaultdict(int)
        # CLIENT_PREDICTION_ACCUMULATOR.value = defaultdict(int)

    def set_broadcasts(self):
        global CLF, SCL, MF, FEATURE_NAMES, ACTIVE_FEATURES

        if self.model_manager.ml_model:
            # We can use the classifier and scaler as objects since they are
            # picklable.
            CLF = self.model_manager.get_classifier_broadcast()
            SCL = self.model_manager.get_scaler_broadcast()
            MF = self.model_manager.get_model_features_broadcast()
        FEATURE_NAMES = self.spark.sparkContext.broadcast(
            self.feature_manager.active_feature_names)
        ACTIVE_FEATURES = self.spark.sparkContext.broadcast([  # #WST
            StrippedFeature(f.feature_name, f.update_row)
            for f in self.feature_manager.active_features
        ])

    def set_accumulators(self):
        global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

        # todo: conditional set
        CLIENT_PREDICTION_ACCUMULATOR = self.spark.sparkContext.accumulator(
            defaultdict(float), DictAccumulatorParam(defaultdict(float))
        )
        CLIENT_REQUEST_SET_COUNT = self.spark.sparkContext.accumulator(
            defaultdict(int), DictAccumulatorParam(defaultdict(int))
        )

    def initialize(self):
        &#34;&#34;&#34;
        Set the basics:
        - Connect to the database
        - Initialize spark session
        - Get active model and scaler and set them to broadcast variables
        - Get active features with their active columns, update columns etc and
        set the relevant broadcast variables
        - Set the _can_predict flag
        - Instantiate the accumulators (for metrics)
        - Instantiate request set cache.
        :return:
        &#34;&#34;&#34;

        # initialize db access tools
        self.tools = BaskervilleDBTools(self.db_conf)
        self.tools.connect_to_db()

        # initialize spark session
        self.spark = self.instantiate_spark_session()

        # todo: use a class for the feature related stuff
        self.model_manager.initialize(self.spark, self.tools)
        self.feature_manager.initialize(self.model_manager)

        self._can_predict = self.feature_manager.feature_config_is_valid()
        self.drop_if_missing_filter = self.data_parser.drop_if_missing_filter()

        # set broadcasts
        self.set_broadcasts()
        self.set_accumulators()

        # set up cache
        self.request_set_cache = self.set_up_request_set_cache()

        # gather calculations
        self.group_by_aggs = self.get_group_by_aggs()
        self.columns_to_filter_by = self.get_columns_to_filter_by()
        self.cols_to_drop = set(
            self.feature_manager.active_feature_names +
            self.feature_manager.active_columns +
            list(self.group_by_aggs.keys()) +
            self.feature_manager.update_feature_cols
        ).difference(RequestSet.columns)

        self._is_initialized = True

    def get_columns_to_filter_by(self):
        &#34;&#34;&#34;
        Gathers all the columns that need to be present in the dataframe
        for the processing to complete.
        group_by_cols: the columns to group data on
        active_columns: the columns that the active features have declared as
        necessary
        timestamp_column: the time column - all logs need to have a time column
        :return: a set of the column names that need to be present in the
        dataframe
        :rtype: set[str]
        &#34;&#34;&#34;
        cols = self.group_by_cols + self.feature_manager.active_columns
        cols.append(self.engine_conf.data_config.timestamp_column)
        return set(cols)

    def get_group_by_aggs(self):
        &#34;&#34;&#34;
        Gathers all the group by arguments:
        basic_aggs:
            - first_request
            - last_request
            - num_requests
        column_aggs: the columns the features need for computation are gathered
         as lists
        feature_aggs: the columns the features need for computation
        Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
        The basic aggs have a priority over the feature and column aggs.
        The feature aggs have a priority over the column aggs (if a feature
        has explicitly asked for a computation for a specific column it relies
        upon, then the computation will be stored instead of the column
        aggregation as list)

        :return: a dictionary with the name of the group by aggregation columns
        as keys and the respective Column aggregation as values
        :rtype: dict[string, pyspark.Column]
        &#34;&#34;&#34;
        basic_aggs = {
            &#39;first_request&#39;: F.min(F.col(&#39;@timestamp&#39;)).alias(&#39;first_request&#39;),
            &#39;last_request&#39;: F.max(F.col(&#39;@timestamp&#39;)).alias(&#39;last_request&#39;),
            &#39;num_requests&#39;: F.count(F.col(&#39;@timestamp&#39;)).alias(&#39;num_requests&#39;)
        }

        column_aggs = {
            c: F.collect_list(F.col(c)).alias(c)
            for c in self.feature_manager.active_columns
        }

        feature_aggs = self.feature_manager.get_feature_group_by_aggs()

        basic_aggs.update(
            {k: v for k, v in feature_aggs.items() if k not in basic_aggs}
        )
        basic_aggs.update(
            {k: v for k, v in column_aggs.items() if k not in basic_aggs}
        )

        return basic_aggs

    def get_post_group_by_calculations(self):
        &#34;&#34;&#34;
        Gathers the columns and computations to be performed after the grouping
        of the data (logs_df)
        Basic post group by columns:
        - `id_runtime`
        - `time_bucket`
        - `start`
        - `stop`
        - `subset_count`

        if there is an ML Model defined:
        - `model_version`
        - `classifier`
        - `scaler`
        - `model_features`

        Each feature can also define post group by calculations using the
        post_group_by_calcs dict.

        :return: A dictionary with the name of the result columns as keys and
        their respective computations as values
        :rtype: dict[string, pyspark.Column]
        &#34;&#34;&#34;
        if self.post_group_by_aggs:
            return self.post_group_by_aggs

        post_group_by_columns = {
            &#39;id_runtime&#39;: F.lit(self.runtime.id),
            &#39;time_bucket&#39;: F.lit(self.time_bucket.sec),
            &#39;start&#39;: F.when(
                F.col(&#39;first_ever_request&#39;).isNotNull(),
                F.col(&#39;first_ever_request&#39;)
            ).otherwise(F.col(&#39;first_request&#39;)),
            &#39;stop&#39;: F.col(&#39;last_request&#39;),
            &#39;subset_count&#39;: F.when(
                F.col(&#39;old_subset_count&#39;).isNotNull(),
                F.col(&#39;old_subset_count&#39;)
            ).otherwise(F.lit(0))
        }

        if self.model_manager.ml_model:
            post_group_by_columns[&#39;model_version&#39;] = F.lit(
                self.model_manager.ml_model.id
            )

        # todo: what if a feature defines a column name that already exists?
        # e.g. like `subset_count`
        post_group_by_columns.update(
            self.feature_manager.post_group_by_calculations
        )

        return post_group_by_columns

    def __getitem__(self, name):
        if name == &#39;run&#39;:
            if not self._is_initialized:
                raise RuntimeError(
                    f&#39;__getitem__: {self.__class__.__name__} &#39;
                    f&#39;has not been initialized yet.&#39;
                )
        return getattr(self, name)

    def __getattribute__(self, name):
        if name == &#39;run&#39;:
            if not self._is_initialized:
                raise RuntimeError(
                    f&#39;__getattribute__:{self.__class__.__name__} &#39;
                    f&#39;has not been initialized yet.&#39;
                )

        return super().__getattribute__(name)

    def filter_cache(self):
        &#34;&#34;&#34;
        Use the current logs to find the past request sets - if any - in the
        request set cache
        :return:
        &#34;&#34;&#34;
        df = self.logs_df.select(
            F.col(&#39;client_request_host&#39;).alias(&#39;target&#39;),
            F.col(&#39;client_ip&#39;).alias(&#39;ip&#39;),
        ).distinct().alias(&#39;a&#39;).persist(self.spark_conf.storage_level)

        self.request_set_cache.filter_by(df)

        df.unpersist()
        del df

    def run(self):
        &#34;&#34;&#34;
        Runs the configured steps.
        :return:
        &#34;&#34;&#34;
        self.logger.info(
            f&#39;Spark UI accessible at:{self.spark.sparkContext.uiWebUrl()}&#39;
        )
        self.create_runtime()
        self.get_data()
        self.process_data()

    def process_data(self):
        &#34;&#34;&#34;
        Splits the data into time bucket lenght windows and executes all the steps
        :return:
        &#34;&#34;&#34;
        if self.logs_df.count() == 0:
            self.logger.info(&#39;No data in to process.&#39;)
        else:
            for window_df in self.get_window():
                self.logs_df = window_df
                self.logs_df = self.logs_df.repartition(
                    &#39;client_request_host&#39;,
                    &#39;client_ip&#39;
                ).persist(self.spark_conf.storage_level)
                self.remaining_steps = list(self.step_to_action.keys())
                for step, action in self.step_to_action.items():
                    self.logger.info(&#39;Starting step {}&#39;.format(step))
                    action()
                    self.logger.info(&#39;Completed step {}&#39;.format(step))
                    self.remaining_steps.remove(step)

    def get_window(self):

        from pyspark.sql import functions as F

        df = self.logs_df.withColumn(&#39;timestamp&#39;,
                                     F.col(&#39;@timestamp&#39;).cast(&#39;timestamp&#39;))
        df = df.sort(&#39;timestamp&#39;)
        current_window_start = df.agg({&#34;timestamp&#34;: &#34;min&#34;}).collect()[0][0]
        stop = df.agg({&#34;timestamp&#34;: &#34;max&#34;}).collect()[0][0]
        window_df = None
        current_end = current_window_start + self.time_bucket.td

        while True:
            if window_df:
                window_df.unpersist(blocking=True)
                del window_df
            filter_ = (
                    (F.col(&#39;timestamp&#39;) &gt;= current_window_start) &amp;
                    (F.col(&#39;timestamp&#39;) &lt; current_end)
            )
            window_df = df.where(filter_).persist(
                self.spark_conf.storage_level
            )
            if not window_df.rdd.isEmpty():
                print(f&#39;# Request sets = {window_df.count()}&#39;)
                yield window_df
            else:
                self.logger.info(f&#39;Empty window df for {str(filter_._jc)}&#39;)
            current_window_start = current_window_start + self.time_bucket.td
            current_end = current_window_start + self.time_bucket.td
            if current_window_start &gt;= stop:
                window_df.unpersist(blocking=True)
                del window_df
                break

    def create_runtime(self):
        &#34;&#34;&#34;
        Create a Runtime in the Baskerville database.
        :return:
        &#34;&#34;&#34;
        raise NotImplementedError(
            &#39;SparkPipelineBase does not have an implementation &#39;
            &#39;for _create_runtime.&#39;
        )

    def get_data(self):
        &#34;&#34;&#34;
        Get dataframe of log data
        :return:
        &#34;&#34;&#34;
        raise NotImplementedError(
            &#39;SparkPipelineBase does not have an implementation &#39;
            &#39;for _get_data.&#39;
        )

    def preprocessing(self):
        &#34;&#34;&#34;
        Fill missing values, add calculation cols, and filter.
        :return:
        &#34;&#34;&#34;

        self.handle_missing_columns()
        self.rename_columns()
        self.filter_columns()
        self.handle_missing_values()
        self.normalize_host_names()
        self.add_calc_columns()

    def group_by(self):
        &#34;&#34;&#34;
        Group the logs df by the given group-by columns (normally IP, host).
        :return: None
        &#34;&#34;&#34;
        self.logs_df = self.logs_df.groupBy(
            *self.group_by_cols
        ).agg(
            *self.group_by_aggs.values()
        )

    def feature_calculation(self):
        &#34;&#34;&#34;
        Add calculation cols, extract features, and update.
        :return:
        &#34;&#34;&#34;
        self.add_post_groupby_columns()
        self.feature_extraction()
        self.feature_update()

    def label_or_predict(self):
        &#34;&#34;&#34;
        Apply label from MISP or predict label.
        #todo: use separate steps for this
        :return:
        &#34;&#34;&#34;

        from pyspark.sql import functions as F
        from pyspark.sql.types import IntegerType

        if self.engine_conf.cross_reference:
            self.cross_reference()
        else:
            self.logs_df = self.logs_df.withColumn(
                &#39;label&#39;, F.lit(None).cast(IntegerType()))

        if not self._can_predict:
            self.logger.warn(
                &#39;Active features do not match model features, &#39;
                &#39;skipping prediction&#39;
            )
            self.logs_df = self.logs_df.withColumn(
                &#39;prediction&#39;, F.lit(None).cast(IntegerType()))
            self.logs_df = self.logs_df.withColumn(
                &#39;r&#39;, F.lit(None).cast(IntegerType()))
        elif not self.model_manager.ml_model:
            self.logger.warn(
                &#39;No ml model specified, &#39;
                &#39;skipping prediction&#39;
            )
            self.logs_df = self.logs_df.withColumn(
                &#39;prediction&#39;, F.lit(None).cast(IntegerType()))
            self.logs_df = self.logs_df.withColumn(
                &#39;r&#39;, F.lit(None).cast(IntegerType()))
        else:
            self.predict()

    def save(self):
        &#34;&#34;&#34;
        Update dataframe, save to database, and update cache.
        :return:
        &#34;&#34;&#34;
        request_set_columns = RequestSet.columns[:]
        not_common = {
            &#39;prediction&#39;, &#39;r&#39;, &#39;model_version&#39;, &#39;label&#39;, &#39;id_attribute&#39;,
            &#39;updated_at&#39;
        }.difference(self.logs_df.columns)

        for c in not_common:
            request_set_columns.remove(c)

        # filter the logs df with the request_set columns
        self.logs_df = self.logs_df.select(request_set_columns)

        # save request_sets
        self.logger.debug(&#39;Saving request_sets&#39;)
        self.save_df_to_table(
            self.logs_df.select(request_set_columns),
            RequestSet.__tablename__
        )
        self.refresh_cache()

    def refresh_cache(self):
        &#34;&#34;&#34;
        Update the cache with the current batch of logs in logs_df and clean up
        :return:
        &#34;&#34;&#34;
        self.request_set_cache.update_self(self.logs_df)
        self.logs_df.unpersist()
        self.logs_df = None
        # self.spark.catalog.clearCache()

    def finish_up(self):
        &#34;&#34;&#34;
        Try to gracefully stop by committing to database, emptying the cache,
        unpersist everything, disconnecting from db and clearing spark&#39;s cache
        :return: None
        &#34;&#34;&#34;
        if len(self.remaining_steps) == 0:
            request_set_count = self.tools.session.query(RequestSet).filter(
                RequestSet.id_runtime == self.runtime.id).count()
            self.runtime.processed = True
            self.runtime.n_request_sets = request_set_count
            self.tools.session.commit()
            self.logger.debug(&#39;finished updating runtime&#39;)
        try:
            self.request_set_cache.empty()
        except AttributeError:
            pass

        if hasattr(self, &#39;logs_df&#39;) and self.logs_df and isinstance(
                self.logs_df, DataFrame):
            self.logs_df.unpersist(blocking=True)
            self.logs_df = None

        if self.tools and self.tools.session:
            self.tools.session.commit()
            self.tools.disconnect_from_db()
        if self.spark:
            try:
                if self.spark_conf.spark_python_profile:
                    self.spark.sparkContext.show_profiles()
                self.spark.catalog.clearCache()
            except AttributeError:
                self.logger.debug(&#39;clearCache attr error&#39;)
                pass

    def instantiate_spark_session(self):
        &#34;&#34;&#34;
        #todo
        :return:
        &#34;&#34;&#34;
        return get_or_create_spark_session(self.spark_conf)

    def set_up_request_set_cache(self):
        &#34;&#34;&#34;
        Set up an instance of RequestSetSparkCache using the cache
        configuration. Also, load past (start_time - expiry_time)
        if cache_load_past is configured.
        :return:
        &#34;&#34;&#34;

        self.request_set_cache = RequestSetSparkCache(
            cache_config=self.cache_config,
            table_name=RequestSet.__tablename__,
            columns_to_keep=(
                &#39;target&#39;,
                &#39;ip&#39;,
                F.col(&#39;start&#39;).alias(&#39;first_ever_request&#39;),
                F.col(&#39;subset_count&#39;).alias(&#39;old_subset_count&#39;),
                F.col(&#39;features&#39;).alias(&#39;old_features&#39;),
                F.col(&#39;num_requests&#39;).alias(&#39;old_num_requests&#39;),
                F.col(&#39;updated_at&#39;)
            ),
            expire_if_longer_than=self.engine_conf.cache_expire_time

        )
        if self.engine_conf.cache_load_past:
            self.request_set_cache = self.request_set_cache.load(
                update_date=(
                    self.start_time - datetime.timedelta(
                        seconds=self.engine_conf.cache_expire_time
                    )
                ).replace(tzinfo=tzutc()),
                extra_filters=(
                        F.col(&#39;time_bucket&#39;) == self.time_bucket.sec
                )  # todo: &amp; (F.col(&#34;id_runtime&#34;) == self.runtime.id)?
            )
        else:
            schema = T.StructType([
                T.StructField(&#34;id&#34;, T.IntegerType(), False),
                T.StructField(&#34;target&#34;, T.StringType(), False),
                T.StructField(&#34;ip&#34;, T.StringType(), False),
                T.StructField(&#34;first_ever_request&#34;, T.TimestampType(), True),
                T.StructField(&#34;old_subset_count&#34;, T.IntegerType(), True),
                T.StructField(&#34;old_features&#34;,
                              T.MapType(T.StringType(), T.FloatType()), True),
                T.StructField(&#34;old_num_requests&#34;, T.IntegerType(), True),
                T.StructField(&#34;updated_at&#34;, T.TimestampType(), True)
            ])
            self.request_set_cache.load_empty(schema)

        self.logger.info(f&#39;In cache: {self.request_set_cache.count()}&#39;)

        return self.request_set_cache

    def handle_missing_columns(self):
        &#34;&#34;&#34;
        Check for missing columns and if any use the data parser to add them
        and fill them with defaults, if specified in the schema.
        :return:
        &#34;&#34;&#34;
        missing = self.data_parser.check_for_missing_columns(self.logs_df)
        if missing:
            self.logs_df = self.data_parser.add_missing_columns(
                self.logs_df, missing
            )

    def rename_columns(self):
        &#34;&#34;&#34;
        Some column names may cause issues with spark, e.g. `geo.ip.lat`, so
        the features that use those can declare in `columns_renamed` that those
        columns should be renamed to something else, e.g. `geo_ip_lat`
        :return:
        &#34;&#34;&#34;
        for k, v in self.feature_manager.column_renamings:
            self.logs_df = self.logs_df.withColumnRenamed(k, v)

    def filter_columns(self):
        &#34;&#34;&#34;
        Logs df may have columns that are not necessary for the analysis,
        filter them out to reduce the memory footprint.
        The absolutely essential columns are the group by columns and the
        timestamp column, or else the rest of the process will fail.
        And of course the columns the features need, the active columns.
        :return:None
        &#34;&#34;&#34;

        where = self.drop_if_missing_filter

        self.logs_df = self.logs_df.select(*self.columns_to_filter_by)
        if where is not None:
            self.logs_df = self.logs_df.where(where)

        # todo: metric for dropped logs
        print(f&#39;{self.logs_df.count()}&#39;)

    def handle_missing_values(self):
        self.logs_df = self.data_parser.fill_missing_values(self.logs_df)

    def normalize_host_names(self):
        &#34;&#34;&#34;
        From www.somedomain.tld keep somedomain
        # todo: improve this and remove udf
        # todo: keep original target in a separate field in db
        :return:
        &#34;&#34;&#34;
        from baskerville.spark.udfs import udf_normalize_host_name

        self.logs_df = self.logs_df.withColumn(
            &#39;client_request_host&#39;,
            udf_normalize_host_name(
                F.col(&#39;client_request_host&#39;).cast(T.StringType())
            )
        )

    def add_calc_columns(self):
        &#34;&#34;&#34;
        Each feature needs different calculations in order to be able to
        compute the feature value. Go through the features and apply the
        calculations. Each calculation can occur only once, calculations
        with the same name will be ignored.
        :return:
        &#34;&#34;&#34;

        self.logs_df = self.logs_df.withColumn(
            &#39;@timestamp&#39;, F.col(&#39;@timestamp&#39;).cast(&#39;timestamp&#39;)
        )

        for k, v in self.feature_manager.pre_group_by_calculations.items():
            self.logs_df = self.logs_df.withColumn(
                k, v
            )

        for f in self.feature_manager.active_features:
            self.logs_df = f.misc_compute(self.logs_df)

    def add_cache_columns(self):
        &#34;&#34;&#34;
        Add columns from the cache to facilitate the
        feature extraction, prediction, and save processes
        :return:
        &#34;&#34;&#34;
        self.logs_df = self.logs_df.alias(&#39;logs_df&#39;)
        self.filter_cache()
        self.logs_df = self.request_set_cache.update_df(
            self.logs_df, select_cols=self.cache_columns
        )
        self.logger.debug(
            f&#39;****** &gt; # of rows in cache: {self.request_set_cache.count()}&#39;)

    def add_post_groupby_columns(self):
        &#34;&#34;&#34;
        Add extra columns after the grouping of the logs to facilitate the
        feature extraction, prediction, and save processes
        Extra columns:
        * general:
        ----------
        - ip
        - target
        - id_runtime
        - time_bucket
        - start
        - subset_count

        * cache columns:
        ----------------
        - &#39;id&#39;,
        - &#39;first_ever_request&#39;,
        - &#39;old_subset_count&#39;,
        - &#39;old_features&#39;,
        - &#39;old_num_requests&#39;

        * model related:
        ----------------
        - model_version
        - classifier
        - scaler
        - model_features

        :return: None
        &#34;&#34;&#34;
        # todo: shouldn&#39;t this be a renaming?
        self.logs_df = self.logs_df.withColumn(&#39;ip&#39;, F.col(&#39;client_ip&#39;))
        self.logs_df = self.logs_df.withColumn(
            &#39;target&#39;, F.col(&#39;client_request_host&#39;)
        )
        self.add_cache_columns()

        for k, v in self.get_post_group_by_calculations().items():
            self.logs_df = self.logs_df.withColumn(k, v)

        self.logs_df = self.logs_df.drop(&#39;old_subset_count&#39;)

    def feature_extraction(self):
        &#34;&#34;&#34;
        For each feature compute the feature value and add it as a column in
        the dataframe
        :return: None
        &#34;&#34;&#34;

        for feature in self.feature_manager.active_features:
            self.logs_df = feature.compute(self.logs_df)

        self.logger.info(
            f&#39;Number of logs after feature extraction {self.logs_df.count()}&#39;
        )
        # self.logs_df = self.logs_df.cache()

    def feature_update(self):
        &#34;&#34;&#34;
        Update current batch&#39;s features with past features - if any - using
        the request set cache.
        :return:
        &#34;&#34;&#34;
        # convert current features to dict since the already saved request_sets
        # have the features as json
        self.features_to_dict(&#39;features&#39;)
        self.features_to_dict(&#39;old_features&#39;)
        self.logs_df = self.logs_df.drop(
            *self.feature_manager.active_feature_names).persist(
            self.spark_conf.storage_level)
        for f in self.feature_manager.updateable_active_features:
            self.logs_df = f.update(self.logs_df).cache()

        # self.logs_df = self.logs_df.persist(self.spark_conf.storage_level)
        self.logs_df = self.logs_df.withColumn(&#39;features&#39;, F.create_map(
            *list(
                itertools.chain(
                    *[
                        (F.lit(f.feature_name),
                         F.col(f.updated_feature_col_name))
                        for f in
                        self.feature_manager.updateable_active_features
                    ]
                )
            )
        ))

        # older way with a udf:
        # self.logs_df = self.logs_df.withColumn(
        #     &#39;features&#39;,
        #     udf_update_features(
        #         F.lit(cPickle.dumps([  # #WST
        #             StrippedFeature(f.feature_name, f.update_row)
        #             for f in self.feature_manager.active_features
        #         ]
        #         )),
        #         &#39;features&#39;,
        #         &#39;old_features&#39;,
        #         &#39;subset_count&#39;,
        #         &#39;start&#39;,
        #         &#39;last_request&#39;
        #     )
        # ).persist(self.spark_conf.storage_level)
        self.logs_df = self.logs_df.drop(&#39;old_features&#39;)

        self.logs_df = self.logs_df.withColumn(
            &#39;subset_count&#39;,
            F.col(&#39;subset_count&#39;) + F.lit(1)
        )

        self.logs_df = self.logs_df.withColumn(
            &#39;num_requests&#39;,
            F.when(
                F.col(&#39;old_num_requests&#39;) &gt; 0,
                F.col(&#39;old_num_requests&#39;) + F.col(&#39;num_requests&#39;)
            ).otherwise(F.col(&#39;num_requests&#39;))
        )
        self.logs_df = self.logs_df.drop(&#39;old_num_requests&#39;)
        diff = (F.unix_timestamp(&#39;last_request&#39;, format=&#34;YYYY-MM-DD %H:%M:%S&#34;)
                - F.unix_timestamp(
                    &#39;start&#39;, format=&#34;YYYY-MM-DD %H:%M:%S&#34;)
                ).cast(&#39;float&#39;)
        self.logs_df = self.logs_df.withColumn(&#39;total_seconds&#39;, diff)
        self.logs_df = self.logs_df.drop(*self.cols_to_drop)

    def cross_reference(self):
        &#34;&#34;&#34;
        Look up IPs in attributes table, and label as malicious (-1) if listed
        there.
        :return:
        &#34;&#34;&#34;
        from pyspark.sql import functions as F
        from baskerville.spark.udfs import udf_cross_reference_misp

        self.logs_df = self.logs_df.withColumn(
            &#39;cross_reference&#39;,
            udf_cross_reference_misp(&#39;ip&#39;,
                                     F.lit(json.dumps(self.db_conf.__dict__)))
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;label&#39;,
            F.when(F.col(&#39;cross_reference.label&#39;) != 0,
                   F.col(&#39;cross_reference.label&#39;)).otherwise(None)
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;id_attribute&#39;,
            F.when(F.col(&#39;cross_reference.id_attribute&#39;) != 0,
                   F.col(&#39;cross_reference.id_attribute&#39;)).otherwise(None)
        )

    def predict(self):
        &#34;&#34;&#34;
        Predict on the request_sets. Prediction on request_sets
        requires feature averaging where there is an existing request_set.
`        :return: None
        &#34;&#34;&#34;
        global CLF, SCL, MF, CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

        self.logs_df = self.logs_df.withColumn(
            &#39;y&#39;,
            udf_predict_dict(
                &#39;target&#39;,
                &#39;features&#39;,
                F.lit(self.engine_conf.metrics is not None)
                # todo: will fail if metrics missing in the conf file
            )
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;prediction&#39;, F.col(&#39;y.prediction&#39;)
        )
        self.logs_df = self.logs_df.withColumn(&#39;r&#39;, F.col(&#39;y.r&#39;))
        self.logs_df = self.logs_df.drop(&#39;y&#39;)

    def save_df_to_table(
            self, df, table_name, json_cols=(&#39;features&#39;,), mode=&#39;append&#39;
    ):
        &#34;&#34;&#34;
        Save the dataframe to the database. Jsonify any columns that need to
        be
        :param pyspark.Dataframe df: the dataframe to save
        :param str table_name: where to save the dataframe
        :param iterable json_cols: the name of the columns that need to be
        jsonified, e.g. `features`
        :param str mode: save mode, &#39;append&#39;, &#39;overwrite&#39; etc, see spark save
        options
        :return: None
        &#34;&#34;&#34;
        df = df.persist(self.spark_conf.storage_level)
        for c in json_cols:
            df = col_to_json(df, c)
        df.write.format(&#39;jdbc&#39;).options(
            url=self.db_url,
            driver=self.spark_conf.db_driver,
            dbtable=table_name,
            user=self.db_conf.user,
            password=self.db_conf.password,
            stringtype=&#39;unspecified&#39;,
            batchsize=100000,
            max_connections=1250,
            rewriteBatchedStatements=True,
            reWriteBatchedInserts=True,
            useServerPrepStmts=False,
        ).mode(mode).save()

    def features_to_dict(self, column):
        &#34;&#34;&#34;
        Convert the features to dictionary
        :param string column: the name of the column to use as output
        :return: None
        &#34;&#34;&#34;
        from pyspark.sql import functions as F

        self.logs_df = self.logs_df.withColumn(
            column,
            F.create_map(
                *list(
                    itertools.chain(
                        *[
                            (F.lit(f.feature_name), F.col(f.feature_name))
                            for f in self.feature_manager.active_features
                        ]
                    )
                ))
        )

    def register_metrics(self):
        &#34;&#34;&#34;
        Registers action hooks for:
        - step_to_action: enum metric to show the progress
        - get_data:
            - &#39;total_number_of_requests&#39;: increments the metric counter by
            the self.logs_df.count()
            - &#39;current_number_of_requests&#39;: sets the metric counter to the
            current self.logs_df.count()
        - filter_columns:
            - `total number of requests after filtering`
        - group_by:
            - `current_number_of_request_sets`:increments the metric counter by
            the self.logs_df.count()

        - save_df_to_table: average host prediction

        :return: None
        &#34;&#34;&#34;
        from baskerville.models.metrics.registry import metrics_registry
        from baskerville.util.enums import MetricClassEnum
        from baskerville.models.metrics.helpers import (
            incr_counter_for_logs_df,
            update_avg_hosts_counter,
            set_counter_for_logs_df
        )

        # wrap step_to_action
        self.step_to_action = metrics_registry.register_states(
            &#39;baskerville_steps&#39;,
            &#39;Tracks which step Baskerville is currently executing&#39;,
            self.step_to_action
        )

        # total number of requests in current df
        get_data = metrics_registry.register_action_hook(
            self.get_data,
            incr_counter_for_logs_df,
            metric_name=&#39;total_number_of_requests&#39;
        )
        # current number of requests in current df
        get_data = metrics_registry.register_action_hook(
            get_data,
            set_counter_for_logs_df,
            metric_cls=MetricClassEnum.gauge,
            metric_name=&#39;current_number_of_requests&#39;
        )
        # current number of requests in current df
        filter_columns = metrics_registry.register_action_hook(
            self.filter_columns,
            incr_counter_for_logs_df,
            metric_cls=MetricClassEnum.counter,
            metric_name=&#39;total_number_of_requests_after_filtering&#39;
        )
        # number of request_sets in current df after group by
        group_by = metrics_registry.register_action_hook(
            self.group_by,
            incr_counter_for_logs_df,
            metric_name=&#39;current_number_of_request_sets&#39;
        )

        # set the average prediction for each host, per batch
        save_df_to_table = metrics_registry.register_action_hook(
            self.save_df_to_table,
            update_avg_hosts_counter,
            metric_cls=MetricClassEnum.gauge,
            metric_name=&#39;avg_host_prediction&#39;,
            labelnames=[&#39;target&#39;]
        )

        # set wrapped methods
        setattr(self, &#39;get_data&#39;, get_data)
        setattr(self, &#39;filter_columns&#39;, filter_columns)
        setattr(self, &#39;group_by&#39;, group_by)
        setattr(self, &#39;save_df_to_table&#39;, save_df_to_table)

        self.logger.info(&#39;Registered metrics.&#39;)

        if self.engine_conf.metrics.exported_dashboard_file:
            from baskerville.models.metrics.dashboard_exporter import \
                DashboardExporter
            dashboard = DashboardExporter(&#39;Baskerville Metrics&#39;)
            dashboard.export(self.engine_conf.metrics.exported_dashboard_file)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="baskerville.models.base.PipelineBase" href="base.html#baskerville.models.base.PipelineBase">PipelineBase</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="baskerville.models.pipelines.ElasticsearchPipeline" href="pipelines.html#baskerville.models.pipelines.ElasticsearchPipeline">ElasticsearchPipeline</a></li>
<li><a title="baskerville.models.pipelines.RawLogPipeline" href="pipelines.html#baskerville.models.pipelines.RawLogPipeline">RawLogPipeline</a></li>
<li><a title="baskerville.models.pipelines.KafkaPipeline" href="pipelines.html#baskerville.models.pipelines.KafkaPipeline">KafkaPipeline</a></li>
<li><a title="baskerville.models.pipelines.SparkStructuredStreamingRealTimePipeline" href="pipelines.html#baskerville.models.pipelines.SparkStructuredStreamingRealTimePipeline">SparkStructuredStreamingRealTimePipeline</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns"><code class="name flex">
<span>def <span class="ident">add_cache_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Add columns from the cache to facilitate the
feature extraction, prediction, and save processes
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_cache_columns(self):
    &#34;&#34;&#34;
    Add columns from the cache to facilitate the
    feature extraction, prediction, and save processes
    :return:
    &#34;&#34;&#34;
    self.logs_df = self.logs_df.alias(&#39;logs_df&#39;)
    self.filter_cache()
    self.logs_df = self.request_set_cache.update_df(
        self.logs_df, select_cols=self.cache_columns
    )
    self.logger.debug(
        f&#39;****** &gt; # of rows in cache: {self.request_set_cache.count()}&#39;)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns"><code class="name flex">
<span>def <span class="ident">add_calc_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Each feature needs different calculations in order to be able to
compute the feature value. Go through the features and apply the
calculations. Each calculation can occur only once, calculations
with the same name will be ignored.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_calc_columns(self):
    &#34;&#34;&#34;
    Each feature needs different calculations in order to be able to
    compute the feature value. Go through the features and apply the
    calculations. Each calculation can occur only once, calculations
    with the same name will be ignored.
    :return:
    &#34;&#34;&#34;

    self.logs_df = self.logs_df.withColumn(
        &#39;@timestamp&#39;, F.col(&#39;@timestamp&#39;).cast(&#39;timestamp&#39;)
    )

    for k, v in self.feature_manager.pre_group_by_calculations.items():
        self.logs_df = self.logs_df.withColumn(
            k, v
        )

    for f in self.feature_manager.active_features:
        self.logs_df = f.misc_compute(self.logs_df)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns"><code class="name flex">
<span>def <span class="ident">add_post_groupby_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Add extra columns after the grouping of the logs to facilitate the
feature extraction, prediction, and save processes
Extra columns:
* general:</p>
<hr>
<ul>
<li>ip</li>
<li>target</li>
<li>id_runtime</li>
<li>time_bucket</li>
<li>start</li>
<li>subset_count</li>
</ul>
<h2 id="cache-columns">* cache columns:</h2>
<ul>
<li>'id',</li>
<li>'first_ever_request',</li>
<li>'old_subset_count',</li>
<li>'old_features',</li>
<li>'old_num_requests'</li>
</ul>
<h2 id="model-related">* model related:</h2>
<ul>
<li>model_version</li>
<li>classifier</li>
<li>scaler</li>
<li>model_features</li>
</ul>
<p>:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_post_groupby_columns(self):
    &#34;&#34;&#34;
    Add extra columns after the grouping of the logs to facilitate the
    feature extraction, prediction, and save processes
    Extra columns:
    * general:
    ----------
    - ip
    - target
    - id_runtime
    - time_bucket
    - start
    - subset_count

    * cache columns:
    ----------------
    - &#39;id&#39;,
    - &#39;first_ever_request&#39;,
    - &#39;old_subset_count&#39;,
    - &#39;old_features&#39;,
    - &#39;old_num_requests&#39;

    * model related:
    ----------------
    - model_version
    - classifier
    - scaler
    - model_features

    :return: None
    &#34;&#34;&#34;
    # todo: shouldn&#39;t this be a renaming?
    self.logs_df = self.logs_df.withColumn(&#39;ip&#39;, F.col(&#39;client_ip&#39;))
    self.logs_df = self.logs_df.withColumn(
        &#39;target&#39;, F.col(&#39;client_request_host&#39;)
    )
    self.add_cache_columns()

    for k, v in self.get_post_group_by_calculations().items():
        self.logs_df = self.logs_df.withColumn(k, v)

    self.logs_df = self.logs_df.drop(&#39;old_subset_count&#39;)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.create_runtime"><code class="name flex">
<span>def <span class="ident">create_runtime</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Create a Runtime in the Baskerville database.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_runtime(self):
    &#34;&#34;&#34;
    Create a Runtime in the Baskerville database.
    :return:
    &#34;&#34;&#34;
    raise NotImplementedError(
        &#39;SparkPipelineBase does not have an implementation &#39;
        &#39;for _create_runtime.&#39;
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.cross_reference"><code class="name flex">
<span>def <span class="ident">cross_reference</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Look up IPs in attributes table, and label as malicious (-1) if listed
there.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_reference(self):
    &#34;&#34;&#34;
    Look up IPs in attributes table, and label as malicious (-1) if listed
    there.
    :return:
    &#34;&#34;&#34;
    from pyspark.sql import functions as F
    from baskerville.spark.udfs import udf_cross_reference_misp

    self.logs_df = self.logs_df.withColumn(
        &#39;cross_reference&#39;,
        udf_cross_reference_misp(&#39;ip&#39;,
                                 F.lit(json.dumps(self.db_conf.__dict__)))
    )
    self.logs_df = self.logs_df.withColumn(
        &#39;label&#39;,
        F.when(F.col(&#39;cross_reference.label&#39;) != 0,
               F.col(&#39;cross_reference.label&#39;)).otherwise(None)
    )
    self.logs_df = self.logs_df.withColumn(
        &#39;id_attribute&#39;,
        F.when(F.col(&#39;cross_reference.id_attribute&#39;) != 0,
               F.col(&#39;cross_reference.id_attribute&#39;)).otherwise(None)
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.feature_calculation"><code class="name flex">
<span>def <span class="ident">feature_calculation</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Add calculation cols, extract features, and update.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_calculation(self):
    &#34;&#34;&#34;
    Add calculation cols, extract features, and update.
    :return:
    &#34;&#34;&#34;
    self.add_post_groupby_columns()
    self.feature_extraction()
    self.feature_update()</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.feature_extraction"><code class="name flex">
<span>def <span class="ident">feature_extraction</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>For each feature compute the feature value and add it as a column in
the dataframe
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_extraction(self):
    &#34;&#34;&#34;
    For each feature compute the feature value and add it as a column in
    the dataframe
    :return: None
    &#34;&#34;&#34;

    for feature in self.feature_manager.active_features:
        self.logs_df = feature.compute(self.logs_df)

    self.logger.info(
        f&#39;Number of logs after feature extraction {self.logs_df.count()}&#39;
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.feature_update"><code class="name flex">
<span>def <span class="ident">feature_update</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Update current batch's features with past features - if any - using
the request set cache.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def feature_update(self):
    &#34;&#34;&#34;
    Update current batch&#39;s features with past features - if any - using
    the request set cache.
    :return:
    &#34;&#34;&#34;
    # convert current features to dict since the already saved request_sets
    # have the features as json
    self.features_to_dict(&#39;features&#39;)
    self.features_to_dict(&#39;old_features&#39;)
    self.logs_df = self.logs_df.drop(
        *self.feature_manager.active_feature_names).persist(
        self.spark_conf.storage_level)
    for f in self.feature_manager.updateable_active_features:
        self.logs_df = f.update(self.logs_df).cache()

    # self.logs_df = self.logs_df.persist(self.spark_conf.storage_level)
    self.logs_df = self.logs_df.withColumn(&#39;features&#39;, F.create_map(
        *list(
            itertools.chain(
                *[
                    (F.lit(f.feature_name),
                     F.col(f.updated_feature_col_name))
                    for f in
                    self.feature_manager.updateable_active_features
                ]
            )
        )
    ))

    # older way with a udf:
    # self.logs_df = self.logs_df.withColumn(
    #     &#39;features&#39;,
    #     udf_update_features(
    #         F.lit(cPickle.dumps([  # #WST
    #             StrippedFeature(f.feature_name, f.update_row)
    #             for f in self.feature_manager.active_features
    #         ]
    #         )),
    #         &#39;features&#39;,
    #         &#39;old_features&#39;,
    #         &#39;subset_count&#39;,
    #         &#39;start&#39;,
    #         &#39;last_request&#39;
    #     )
    # ).persist(self.spark_conf.storage_level)
    self.logs_df = self.logs_df.drop(&#39;old_features&#39;)

    self.logs_df = self.logs_df.withColumn(
        &#39;subset_count&#39;,
        F.col(&#39;subset_count&#39;) + F.lit(1)
    )

    self.logs_df = self.logs_df.withColumn(
        &#39;num_requests&#39;,
        F.when(
            F.col(&#39;old_num_requests&#39;) &gt; 0,
            F.col(&#39;old_num_requests&#39;) + F.col(&#39;num_requests&#39;)
        ).otherwise(F.col(&#39;num_requests&#39;))
    )
    self.logs_df = self.logs_df.drop(&#39;old_num_requests&#39;)
    diff = (F.unix_timestamp(&#39;last_request&#39;, format=&#34;YYYY-MM-DD %H:%M:%S&#34;)
            - F.unix_timestamp(
                &#39;start&#39;, format=&#34;YYYY-MM-DD %H:%M:%S&#34;)
            ).cast(&#39;float&#39;)
    self.logs_df = self.logs_df.withColumn(&#39;total_seconds&#39;, diff)
    self.logs_df = self.logs_df.drop(*self.cols_to_drop)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.features_to_dict"><code class="name flex">
<span>def <span class="ident">features_to_dict</span></span>(<span>self, column)</span>
</code></dt>
<dd>
<section class="desc"><p>Convert the features to dictionary
:param string column: the name of the column to use as output
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def features_to_dict(self, column):
    &#34;&#34;&#34;
    Convert the features to dictionary
    :param string column: the name of the column to use as output
    :return: None
    &#34;&#34;&#34;
    from pyspark.sql import functions as F

    self.logs_df = self.logs_df.withColumn(
        column,
        F.create_map(
            *list(
                itertools.chain(
                    *[
                        (F.lit(f.feature_name), F.col(f.feature_name))
                        for f in self.feature_manager.active_features
                    ]
                )
            ))
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.filter_cache"><code class="name flex">
<span>def <span class="ident">filter_cache</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Use the current logs to find the past request sets - if any - in the
request set cache
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_cache(self):
    &#34;&#34;&#34;
    Use the current logs to find the past request sets - if any - in the
    request set cache
    :return:
    &#34;&#34;&#34;
    df = self.logs_df.select(
        F.col(&#39;client_request_host&#39;).alias(&#39;target&#39;),
        F.col(&#39;client_ip&#39;).alias(&#39;ip&#39;),
    ).distinct().alias(&#39;a&#39;).persist(self.spark_conf.storage_level)

    self.request_set_cache.filter_by(df)

    df.unpersist()
    del df</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.filter_columns"><code class="name flex">
<span>def <span class="ident">filter_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Logs df may have columns that are not necessary for the analysis,
filter them out to reduce the memory footprint.
The absolutely essential columns are the group by columns and the
timestamp column, or else the rest of the process will fail.
And of course the columns the features need, the active columns.
:return:None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_columns(self):
    &#34;&#34;&#34;
    Logs df may have columns that are not necessary for the analysis,
    filter them out to reduce the memory footprint.
    The absolutely essential columns are the group by columns and the
    timestamp column, or else the rest of the process will fail.
    And of course the columns the features need, the active columns.
    :return:None
    &#34;&#34;&#34;

    where = self.drop_if_missing_filter

    self.logs_df = self.logs_df.select(*self.columns_to_filter_by)
    if where is not None:
        self.logs_df = self.logs_df.where(where)

    # todo: metric for dropped logs
    print(f&#39;{self.logs_df.count()}&#39;)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.finish_up"><code class="name flex">
<span>def <span class="ident">finish_up</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Try to gracefully stop by committing to database, emptying the cache,
unpersist everything, disconnecting from db and clearing spark's cache
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finish_up(self):
    &#34;&#34;&#34;
    Try to gracefully stop by committing to database, emptying the cache,
    unpersist everything, disconnecting from db and clearing spark&#39;s cache
    :return: None
    &#34;&#34;&#34;
    if len(self.remaining_steps) == 0:
        request_set_count = self.tools.session.query(RequestSet).filter(
            RequestSet.id_runtime == self.runtime.id).count()
        self.runtime.processed = True
        self.runtime.n_request_sets = request_set_count
        self.tools.session.commit()
        self.logger.debug(&#39;finished updating runtime&#39;)
    try:
        self.request_set_cache.empty()
    except AttributeError:
        pass

    if hasattr(self, &#39;logs_df&#39;) and self.logs_df and isinstance(
            self.logs_df, DataFrame):
        self.logs_df.unpersist(blocking=True)
        self.logs_df = None

    if self.tools and self.tools.session:
        self.tools.session.commit()
        self.tools.disconnect_from_db()
    if self.spark:
        try:
            if self.spark_conf.spark_python_profile:
                self.spark.sparkContext.show_profiles()
            self.spark.catalog.clearCache()
        except AttributeError:
            self.logger.debug(&#39;clearCache attr error&#39;)
            pass</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by"><code class="name flex">
<span>def <span class="ident">get_columns_to_filter_by</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gathers all the columns that need to be present in the dataframe
for the processing to complete.
group_by_cols: the columns to group data on
active_columns: the columns that the active features have declared as
necessary
timestamp_column: the time column - all logs need to have a time column
:return: a set of the column names that need to be present in the
dataframe
:rtype: set[str]</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_columns_to_filter_by(self):
    &#34;&#34;&#34;
    Gathers all the columns that need to be present in the dataframe
    for the processing to complete.
    group_by_cols: the columns to group data on
    active_columns: the columns that the active features have declared as
    necessary
    timestamp_column: the time column - all logs need to have a time column
    :return: a set of the column names that need to be present in the
    dataframe
    :rtype: set[str]
    &#34;&#34;&#34;
    cols = self.group_by_cols + self.feature_manager.active_columns
    cols.append(self.engine_conf.data_config.timestamp_column)
    return set(cols)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.get_data"><code class="name flex">
<span>def <span class="ident">get_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Get dataframe of log data
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_data(self):
    &#34;&#34;&#34;
    Get dataframe of log data
    :return:
    &#34;&#34;&#34;
    raise NotImplementedError(
        &#39;SparkPipelineBase does not have an implementation &#39;
        &#39;for _get_data.&#39;
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs"><code class="name flex">
<span>def <span class="ident">get_group_by_aggs</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gathers all the group by arguments:
basic_aggs:
- first_request
- last_request
- num_requests
column_aggs: the columns the features need for computation are gathered
as lists
feature_aggs: the columns the features need for computation
Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
The basic aggs have a priority over the feature and column aggs.
The feature aggs have a priority over the column aggs (if a feature
has explicitly asked for a computation for a specific column it relies
upon, then the computation will be stored instead of the column
aggregation as list)</p>
<p>:return: a dictionary with the name of the group by aggregation columns
as keys and the respective Column aggregation as values
:rtype: dict[string, pyspark.Column]</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_group_by_aggs(self):
    &#34;&#34;&#34;
    Gathers all the group by arguments:
    basic_aggs:
        - first_request
        - last_request
        - num_requests
    column_aggs: the columns the features need for computation are gathered
     as lists
    feature_aggs: the columns the features need for computation
    Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
    The basic aggs have a priority over the feature and column aggs.
    The feature aggs have a priority over the column aggs (if a feature
    has explicitly asked for a computation for a specific column it relies
    upon, then the computation will be stored instead of the column
    aggregation as list)

    :return: a dictionary with the name of the group by aggregation columns
    as keys and the respective Column aggregation as values
    :rtype: dict[string, pyspark.Column]
    &#34;&#34;&#34;
    basic_aggs = {
        &#39;first_request&#39;: F.min(F.col(&#39;@timestamp&#39;)).alias(&#39;first_request&#39;),
        &#39;last_request&#39;: F.max(F.col(&#39;@timestamp&#39;)).alias(&#39;last_request&#39;),
        &#39;num_requests&#39;: F.count(F.col(&#39;@timestamp&#39;)).alias(&#39;num_requests&#39;)
    }

    column_aggs = {
        c: F.collect_list(F.col(c)).alias(c)
        for c in self.feature_manager.active_columns
    }

    feature_aggs = self.feature_manager.get_feature_group_by_aggs()

    basic_aggs.update(
        {k: v for k, v in feature_aggs.items() if k not in basic_aggs}
    )
    basic_aggs.update(
        {k: v for k, v in column_aggs.items() if k not in basic_aggs}
    )

    return basic_aggs</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations"><code class="name flex">
<span>def <span class="ident">get_post_group_by_calculations</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Gathers the columns and computations to be performed after the grouping
of the data (logs_df)
Basic post group by columns:
- <code>id_runtime</code>
- <code>time_bucket</code>
- <code>start</code>
- <code>stop</code>
- <code>subset_count</code></p>
<p>if there is an ML Model defined:
- <code>model_version</code>
- <code>classifier</code>
- <code>scaler</code>
- <code>model_features</code></p>
<p>Each feature can also define post group by calculations using the
post_group_by_calcs dict.</p>
<p>:return: A dictionary with the name of the result columns as keys and
their respective computations as values
:rtype: dict[string, pyspark.Column]</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_post_group_by_calculations(self):
    &#34;&#34;&#34;
    Gathers the columns and computations to be performed after the grouping
    of the data (logs_df)
    Basic post group by columns:
    - `id_runtime`
    - `time_bucket`
    - `start`
    - `stop`
    - `subset_count`

    if there is an ML Model defined:
    - `model_version`
    - `classifier`
    - `scaler`
    - `model_features`

    Each feature can also define post group by calculations using the
    post_group_by_calcs dict.

    :return: A dictionary with the name of the result columns as keys and
    their respective computations as values
    :rtype: dict[string, pyspark.Column]
    &#34;&#34;&#34;
    if self.post_group_by_aggs:
        return self.post_group_by_aggs

    post_group_by_columns = {
        &#39;id_runtime&#39;: F.lit(self.runtime.id),
        &#39;time_bucket&#39;: F.lit(self.time_bucket.sec),
        &#39;start&#39;: F.when(
            F.col(&#39;first_ever_request&#39;).isNotNull(),
            F.col(&#39;first_ever_request&#39;)
        ).otherwise(F.col(&#39;first_request&#39;)),
        &#39;stop&#39;: F.col(&#39;last_request&#39;),
        &#39;subset_count&#39;: F.when(
            F.col(&#39;old_subset_count&#39;).isNotNull(),
            F.col(&#39;old_subset_count&#39;)
        ).otherwise(F.lit(0))
    }

    if self.model_manager.ml_model:
        post_group_by_columns[&#39;model_version&#39;] = F.lit(
            self.model_manager.ml_model.id
        )

    # todo: what if a feature defines a column name that already exists?
    # e.g. like `subset_count`
    post_group_by_columns.update(
        self.feature_manager.post_group_by_calculations
    )

    return post_group_by_columns</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.get_window"><code class="name flex">
<span>def <span class="ident">get_window</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_window(self):

    from pyspark.sql import functions as F

    df = self.logs_df.withColumn(&#39;timestamp&#39;,
                                 F.col(&#39;@timestamp&#39;).cast(&#39;timestamp&#39;))
    df = df.sort(&#39;timestamp&#39;)
    current_window_start = df.agg({&#34;timestamp&#34;: &#34;min&#34;}).collect()[0][0]
    stop = df.agg({&#34;timestamp&#34;: &#34;max&#34;}).collect()[0][0]
    window_df = None
    current_end = current_window_start + self.time_bucket.td

    while True:
        if window_df:
            window_df.unpersist(blocking=True)
            del window_df
        filter_ = (
                (F.col(&#39;timestamp&#39;) &gt;= current_window_start) &amp;
                (F.col(&#39;timestamp&#39;) &lt; current_end)
        )
        window_df = df.where(filter_).persist(
            self.spark_conf.storage_level
        )
        if not window_df.rdd.isEmpty():
            print(f&#39;# Request sets = {window_df.count()}&#39;)
            yield window_df
        else:
            self.logger.info(f&#39;Empty window df for {str(filter_._jc)}&#39;)
        current_window_start = current_window_start + self.time_bucket.td
        current_end = current_window_start + self.time_bucket.td
        if current_window_start &gt;= stop:
            window_df.unpersist(blocking=True)
            del window_df
            break</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.group_by"><code class="name flex">
<span>def <span class="ident">group_by</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Group the logs df by the given group-by columns (normally IP, host).
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def group_by(self):
    &#34;&#34;&#34;
    Group the logs df by the given group-by columns (normally IP, host).
    :return: None
    &#34;&#34;&#34;
    self.logs_df = self.logs_df.groupBy(
        *self.group_by_cols
    ).agg(
        *self.group_by_aggs.values()
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns"><code class="name flex">
<span>def <span class="ident">handle_missing_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Check for missing columns and if any use the data parser to add them
and fill them with defaults, if specified in the schema.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_missing_columns(self):
    &#34;&#34;&#34;
    Check for missing columns and if any use the data parser to add them
    and fill them with defaults, if specified in the schema.
    :return:
    &#34;&#34;&#34;
    missing = self.data_parser.check_for_missing_columns(self.logs_df)
    if missing:
        self.logs_df = self.data_parser.add_missing_columns(
            self.logs_df, missing
        )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.handle_missing_values"><code class="name flex">
<span>def <span class="ident">handle_missing_values</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def handle_missing_values(self):
    self.logs_df = self.data_parser.fill_missing_values(self.logs_df)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.initialize"><code class="name flex">
<span>def <span class="ident">initialize</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Set the basics:
- Connect to the database
- Initialize spark session
- Get active model and scaler and set them to broadcast variables
- Get active features with their active columns, update columns etc and
set the relevant broadcast variables
- Set the _can_predict flag
- Instantiate the accumulators (for metrics)
- Instantiate request set cache.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize(self):
    &#34;&#34;&#34;
    Set the basics:
    - Connect to the database
    - Initialize spark session
    - Get active model and scaler and set them to broadcast variables
    - Get active features with their active columns, update columns etc and
    set the relevant broadcast variables
    - Set the _can_predict flag
    - Instantiate the accumulators (for metrics)
    - Instantiate request set cache.
    :return:
    &#34;&#34;&#34;

    # initialize db access tools
    self.tools = BaskervilleDBTools(self.db_conf)
    self.tools.connect_to_db()

    # initialize spark session
    self.spark = self.instantiate_spark_session()

    # todo: use a class for the feature related stuff
    self.model_manager.initialize(self.spark, self.tools)
    self.feature_manager.initialize(self.model_manager)

    self._can_predict = self.feature_manager.feature_config_is_valid()
    self.drop_if_missing_filter = self.data_parser.drop_if_missing_filter()

    # set broadcasts
    self.set_broadcasts()
    self.set_accumulators()

    # set up cache
    self.request_set_cache = self.set_up_request_set_cache()

    # gather calculations
    self.group_by_aggs = self.get_group_by_aggs()
    self.columns_to_filter_by = self.get_columns_to_filter_by()
    self.cols_to_drop = set(
        self.feature_manager.active_feature_names +
        self.feature_manager.active_columns +
        list(self.group_by_aggs.keys()) +
        self.feature_manager.update_feature_cols
    ).difference(RequestSet.columns)

    self._is_initialized = True</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session"><code class="name flex">
<span>def <span class="ident">instantiate_spark_session</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><h1 id="todo">todo</h1>
<p>:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instantiate_spark_session(self):
    &#34;&#34;&#34;
    #todo
    :return:
    &#34;&#34;&#34;
    return get_or_create_spark_session(self.spark_conf)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.label_or_predict"><code class="name flex">
<span>def <span class="ident">label_or_predict</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Apply label from MISP or predict label.</p>
<h1 id="todo-use-separate-steps-for-this">todo: use separate steps for this</h1>
<p>:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def label_or_predict(self):
    &#34;&#34;&#34;
    Apply label from MISP or predict label.
    #todo: use separate steps for this
    :return:
    &#34;&#34;&#34;

    from pyspark.sql import functions as F
    from pyspark.sql.types import IntegerType

    if self.engine_conf.cross_reference:
        self.cross_reference()
    else:
        self.logs_df = self.logs_df.withColumn(
            &#39;label&#39;, F.lit(None).cast(IntegerType()))

    if not self._can_predict:
        self.logger.warn(
            &#39;Active features do not match model features, &#39;
            &#39;skipping prediction&#39;
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;prediction&#39;, F.lit(None).cast(IntegerType()))
        self.logs_df = self.logs_df.withColumn(
            &#39;r&#39;, F.lit(None).cast(IntegerType()))
    elif not self.model_manager.ml_model:
        self.logger.warn(
            &#39;No ml model specified, &#39;
            &#39;skipping prediction&#39;
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;prediction&#39;, F.lit(None).cast(IntegerType()))
        self.logs_df = self.logs_df.withColumn(
            &#39;r&#39;, F.lit(None).cast(IntegerType()))
    else:
        self.predict()</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.load_test"><code class="name flex">
<span>def <span class="ident">load_test</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>If the user has set the load_test configuration, then multiply the
traffic by <code>self.engine_conf.load_test</code> times to do load testing.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_test(self):
    &#34;&#34;&#34;
    If the user has set the load_test configuration, then multiply the
    traffic by `self.engine_conf.load_test` times to do load testing.
    :return:
    &#34;&#34;&#34;
    if self.engine_conf.load_test:
        df = self.logs_df.persist(self.spark_conf.storage_level)

        for i in range(self.engine_conf.load_test - 1):
            df = df.withColumn(
                &#39;client_ip&#39;, F.round(F.rand(42)).cast(&#39;string&#39;)
            )
            self.logs_df = self.logs_df.union(df).persist(
                self.spark_conf.storage_level
            )

        df.unpersist()
        del df
        self.logger.info(
            f&#39;---- Count after multiplication: {self.logs_df.count()}&#39;
        )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names"><code class="name flex">
<span>def <span class="ident">normalize_host_names</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>From www.somedomain.tld keep somedomain</p>
<h1 id="todo-improve-this-and-remove-udf">todo: improve this and remove udf</h1>
<h1 id="todo-keep-original-target-in-a-separate-field-in-db">todo: keep original target in a separate field in db</h1>
<p>:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normalize_host_names(self):
    &#34;&#34;&#34;
    From www.somedomain.tld keep somedomain
    # todo: improve this and remove udf
    # todo: keep original target in a separate field in db
    :return:
    &#34;&#34;&#34;
    from baskerville.spark.udfs import udf_normalize_host_name

    self.logs_df = self.logs_df.withColumn(
        &#39;client_request_host&#39;,
        udf_normalize_host_name(
            F.col(&#39;client_request_host&#39;).cast(T.StringType())
        )
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.predict"><code class="name flex">
<span>def <span class="ident">predict</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Predict on the request_sets. Prediction on request_sets
requires feature averaging where there is an existing request_set.
`
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">    def predict(self):
        &#34;&#34;&#34;
        Predict on the request_sets. Prediction on request_sets
        requires feature averaging where there is an existing request_set.
`        :return: None
        &#34;&#34;&#34;
        global CLF, SCL, MF, CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

        self.logs_df = self.logs_df.withColumn(
            &#39;y&#39;,
            udf_predict_dict(
                &#39;target&#39;,
                &#39;features&#39;,
                F.lit(self.engine_conf.metrics is not None)
                # todo: will fail if metrics missing in the conf file
            )
        )
        self.logs_df = self.logs_df.withColumn(
            &#39;prediction&#39;, F.col(&#39;y.prediction&#39;)
        )
        self.logs_df = self.logs_df.withColumn(&#39;r&#39;, F.col(&#39;y.r&#39;))
        self.logs_df = self.logs_df.drop(&#39;y&#39;)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.preprocessing"><code class="name flex">
<span>def <span class="ident">preprocessing</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Fill missing values, add calculation cols, and filter.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocessing(self):
    &#34;&#34;&#34;
    Fill missing values, add calculation cols, and filter.
    :return:
    &#34;&#34;&#34;

    self.handle_missing_columns()
    self.rename_columns()
    self.filter_columns()
    self.handle_missing_values()
    self.normalize_host_names()
    self.add_calc_columns()</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.process_data"><code class="name flex">
<span>def <span class="ident">process_data</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Splits the data into time bucket lenght windows and executes all the steps
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_data(self):
    &#34;&#34;&#34;
    Splits the data into time bucket lenght windows and executes all the steps
    :return:
    &#34;&#34;&#34;
    if self.logs_df.count() == 0:
        self.logger.info(&#39;No data in to process.&#39;)
    else:
        for window_df in self.get_window():
            self.logs_df = window_df
            self.logs_df = self.logs_df.repartition(
                &#39;client_request_host&#39;,
                &#39;client_ip&#39;
            ).persist(self.spark_conf.storage_level)
            self.remaining_steps = list(self.step_to_action.keys())
            for step, action in self.step_to_action.items():
                self.logger.info(&#39;Starting step {}&#39;.format(step))
                action()
                self.logger.info(&#39;Completed step {}&#39;.format(step))
                self.remaining_steps.remove(step)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.refresh_cache"><code class="name flex">
<span>def <span class="ident">refresh_cache</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Update the cache with the current batch of logs in logs_df and clean up
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def refresh_cache(self):
    &#34;&#34;&#34;
    Update the cache with the current batch of logs in logs_df and clean up
    :return:
    &#34;&#34;&#34;
    self.request_set_cache.update_self(self.logs_df)
    self.logs_df.unpersist()
    self.logs_df = None</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.register_metrics"><code class="name flex">
<span>def <span class="ident">register_metrics</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Registers action hooks for:
- step_to_action: enum metric to show the progress
- get_data:
- 'total_number_of_requests': increments the metric counter by
the self.logs_df.count()
- 'current_number_of_requests': sets the metric counter to the
current self.logs_df.count()
- filter_columns:
- <code>total number of requests after filtering</code>
- group_by:
- <code>current_number_of_request_sets</code>:increments the metric counter by
the self.logs_df.count()</p>
<ul>
<li>save_df_to_table: average host prediction</li>
</ul>
<p>:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def register_metrics(self):
    &#34;&#34;&#34;
    Registers action hooks for:
    - step_to_action: enum metric to show the progress
    - get_data:
        - &#39;total_number_of_requests&#39;: increments the metric counter by
        the self.logs_df.count()
        - &#39;current_number_of_requests&#39;: sets the metric counter to the
        current self.logs_df.count()
    - filter_columns:
        - `total number of requests after filtering`
    - group_by:
        - `current_number_of_request_sets`:increments the metric counter by
        the self.logs_df.count()

    - save_df_to_table: average host prediction

    :return: None
    &#34;&#34;&#34;
    from baskerville.models.metrics.registry import metrics_registry
    from baskerville.util.enums import MetricClassEnum
    from baskerville.models.metrics.helpers import (
        incr_counter_for_logs_df,
        update_avg_hosts_counter,
        set_counter_for_logs_df
    )

    # wrap step_to_action
    self.step_to_action = metrics_registry.register_states(
        &#39;baskerville_steps&#39;,
        &#39;Tracks which step Baskerville is currently executing&#39;,
        self.step_to_action
    )

    # total number of requests in current df
    get_data = metrics_registry.register_action_hook(
        self.get_data,
        incr_counter_for_logs_df,
        metric_name=&#39;total_number_of_requests&#39;
    )
    # current number of requests in current df
    get_data = metrics_registry.register_action_hook(
        get_data,
        set_counter_for_logs_df,
        metric_cls=MetricClassEnum.gauge,
        metric_name=&#39;current_number_of_requests&#39;
    )
    # current number of requests in current df
    filter_columns = metrics_registry.register_action_hook(
        self.filter_columns,
        incr_counter_for_logs_df,
        metric_cls=MetricClassEnum.counter,
        metric_name=&#39;total_number_of_requests_after_filtering&#39;
    )
    # number of request_sets in current df after group by
    group_by = metrics_registry.register_action_hook(
        self.group_by,
        incr_counter_for_logs_df,
        metric_name=&#39;current_number_of_request_sets&#39;
    )

    # set the average prediction for each host, per batch
    save_df_to_table = metrics_registry.register_action_hook(
        self.save_df_to_table,
        update_avg_hosts_counter,
        metric_cls=MetricClassEnum.gauge,
        metric_name=&#39;avg_host_prediction&#39;,
        labelnames=[&#39;target&#39;]
    )

    # set wrapped methods
    setattr(self, &#39;get_data&#39;, get_data)
    setattr(self, &#39;filter_columns&#39;, filter_columns)
    setattr(self, &#39;group_by&#39;, group_by)
    setattr(self, &#39;save_df_to_table&#39;, save_df_to_table)

    self.logger.info(&#39;Registered metrics.&#39;)

    if self.engine_conf.metrics.exported_dashboard_file:
        from baskerville.models.metrics.dashboard_exporter import \
            DashboardExporter
        dashboard = DashboardExporter(&#39;Baskerville Metrics&#39;)
        dashboard.export(self.engine_conf.metrics.exported_dashboard_file)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.rename_columns"><code class="name flex">
<span>def <span class="ident">rename_columns</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Some column names may cause issues with spark, e.g. <code>geo.ip.lat</code>, so
the features that use those can declare in <code>columns_renamed</code> that those
columns should be renamed to something else, e.g. <code>geo_ip_lat</code>
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def rename_columns(self):
    &#34;&#34;&#34;
    Some column names may cause issues with spark, e.g. `geo.ip.lat`, so
    the features that use those can declare in `columns_renamed` that those
    columns should be renamed to something else, e.g. `geo_ip_lat`
    :return:
    &#34;&#34;&#34;
    for k, v in self.feature_manager.column_renamings:
        self.logs_df = self.logs_df.withColumnRenamed(k, v)</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Unpersist rdds and dataframes and call GC - see broadcast memory
release issue
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self):
    &#34;&#34;&#34;
    Unpersist rdds and dataframes and call GC - see broadcast memory
    release issue
    :return:
    &#34;&#34;&#34;
    global CLIENT_REQUEST_SET_COUNT, CLIENT_PREDICTION_ACCUMULATOR
    import gc

    for (id, rdd_) in list(
            self.spark.sparkContext._jsc.getPersistentRDDs().items()
    )[:]:
        rdd_.unpersist()
        del rdd_

    self.spark.catalog.clearCache()
    # self.spark.sparkContext._jvm.System.gc()
    gc.collect()</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Runs the configured steps.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34;
    Runs the configured steps.
    :return:
    &#34;&#34;&#34;
    self.logger.info(
        f&#39;Spark UI accessible at:{self.spark.sparkContext.uiWebUrl()}&#39;
    )
    self.create_runtime()
    self.get_data()
    self.process_data()</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.save"><code class="name flex">
<span>def <span class="ident">save</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Update dataframe, save to database, and update cache.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save(self):
    &#34;&#34;&#34;
    Update dataframe, save to database, and update cache.
    :return:
    &#34;&#34;&#34;
    request_set_columns = RequestSet.columns[:]
    not_common = {
        &#39;prediction&#39;, &#39;r&#39;, &#39;model_version&#39;, &#39;label&#39;, &#39;id_attribute&#39;,
        &#39;updated_at&#39;
    }.difference(self.logs_df.columns)

    for c in not_common:
        request_set_columns.remove(c)

    # filter the logs df with the request_set columns
    self.logs_df = self.logs_df.select(request_set_columns)

    # save request_sets
    self.logger.debug(&#39;Saving request_sets&#39;)
    self.save_df_to_table(
        self.logs_df.select(request_set_columns),
        RequestSet.__tablename__
    )
    self.refresh_cache()</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table"><code class="name flex">
<span>def <span class="ident">save_df_to_table</span></span>(<span>self, df, table_name, json_cols=('features',), mode='append')</span>
</code></dt>
<dd>
<section class="desc"><p>Save the dataframe to the database. Jsonify any columns that need to
be
:param pyspark.Dataframe df: the dataframe to save
:param str table_name: where to save the dataframe
:param iterable json_cols: the name of the columns that need to be
jsonified, e.g. <code>features</code>
:param str mode: save mode, 'append', 'overwrite' etc, see spark save
options
:return: None</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_df_to_table(
        self, df, table_name, json_cols=(&#39;features&#39;,), mode=&#39;append&#39;
):
    &#34;&#34;&#34;
    Save the dataframe to the database. Jsonify any columns that need to
    be
    :param pyspark.Dataframe df: the dataframe to save
    :param str table_name: where to save the dataframe
    :param iterable json_cols: the name of the columns that need to be
    jsonified, e.g. `features`
    :param str mode: save mode, &#39;append&#39;, &#39;overwrite&#39; etc, see spark save
    options
    :return: None
    &#34;&#34;&#34;
    df = df.persist(self.spark_conf.storage_level)
    for c in json_cols:
        df = col_to_json(df, c)
    df.write.format(&#39;jdbc&#39;).options(
        url=self.db_url,
        driver=self.spark_conf.db_driver,
        dbtable=table_name,
        user=self.db_conf.user,
        password=self.db_conf.password,
        stringtype=&#39;unspecified&#39;,
        batchsize=100000,
        max_connections=1250,
        rewriteBatchedStatements=True,
        reWriteBatchedInserts=True,
        useServerPrepStmts=False,
    ).mode(mode).save()</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.set_accumulators"><code class="name flex">
<span>def <span class="ident">set_accumulators</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_accumulators(self):
    global CLIENT_PREDICTION_ACCUMULATOR, CLIENT_REQUEST_SET_COUNT

    # todo: conditional set
    CLIENT_PREDICTION_ACCUMULATOR = self.spark.sparkContext.accumulator(
        defaultdict(float), DictAccumulatorParam(defaultdict(float))
    )
    CLIENT_REQUEST_SET_COUNT = self.spark.sparkContext.accumulator(
        defaultdict(int), DictAccumulatorParam(defaultdict(int))
    )</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.set_broadcasts"><code class="name flex">
<span>def <span class="ident">set_broadcasts</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_broadcasts(self):
    global CLF, SCL, MF, FEATURE_NAMES, ACTIVE_FEATURES

    if self.model_manager.ml_model:
        # We can use the classifier and scaler as objects since they are
        # picklable.
        CLF = self.model_manager.get_classifier_broadcast()
        SCL = self.model_manager.get_scaler_broadcast()
        MF = self.model_manager.get_model_features_broadcast()
    FEATURE_NAMES = self.spark.sparkContext.broadcast(
        self.feature_manager.active_feature_names)
    ACTIVE_FEATURES = self.spark.sparkContext.broadcast([  # #WST
        StrippedFeature(f.feature_name, f.update_row)
        for f in self.feature_manager.active_features
    ])</code></pre>
</details>
</dd>
<dt id="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache"><code class="name flex">
<span>def <span class="ident">set_up_request_set_cache</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Set up an instance of RequestSetSparkCache using the cache
configuration. Also, load past (start_time - expiry_time)
if cache_load_past is configured.
:return:</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_up_request_set_cache(self):
    &#34;&#34;&#34;
    Set up an instance of RequestSetSparkCache using the cache
    configuration. Also, load past (start_time - expiry_time)
    if cache_load_past is configured.
    :return:
    &#34;&#34;&#34;

    self.request_set_cache = RequestSetSparkCache(
        cache_config=self.cache_config,
        table_name=RequestSet.__tablename__,
        columns_to_keep=(
            &#39;target&#39;,
            &#39;ip&#39;,
            F.col(&#39;start&#39;).alias(&#39;first_ever_request&#39;),
            F.col(&#39;subset_count&#39;).alias(&#39;old_subset_count&#39;),
            F.col(&#39;features&#39;).alias(&#39;old_features&#39;),
            F.col(&#39;num_requests&#39;).alias(&#39;old_num_requests&#39;),
            F.col(&#39;updated_at&#39;)
        ),
        expire_if_longer_than=self.engine_conf.cache_expire_time

    )
    if self.engine_conf.cache_load_past:
        self.request_set_cache = self.request_set_cache.load(
            update_date=(
                self.start_time - datetime.timedelta(
                    seconds=self.engine_conf.cache_expire_time
                )
            ).replace(tzinfo=tzutc()),
            extra_filters=(
                    F.col(&#39;time_bucket&#39;) == self.time_bucket.sec
            )  # todo: &amp; (F.col(&#34;id_runtime&#34;) == self.runtime.id)?
        )
    else:
        schema = T.StructType([
            T.StructField(&#34;id&#34;, T.IntegerType(), False),
            T.StructField(&#34;target&#34;, T.StringType(), False),
            T.StructField(&#34;ip&#34;, T.StringType(), False),
            T.StructField(&#34;first_ever_request&#34;, T.TimestampType(), True),
            T.StructField(&#34;old_subset_count&#34;, T.IntegerType(), True),
            T.StructField(&#34;old_features&#34;,
                          T.MapType(T.StringType(), T.FloatType()), True),
            T.StructField(&#34;old_num_requests&#34;, T.IntegerType(), True),
            T.StructField(&#34;updated_at&#34;, T.TimestampType(), True)
        ])
        self.request_set_cache.load_empty(schema)

    self.logger.info(f&#39;In cache: {self.request_set_cache.count()}&#39;)

    return self.request_set_cache</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="baskerville.models" href="index.html">baskerville.models</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="baskerville.models.base_spark.predict_dict" href="#baskerville.models.base_spark.predict_dict">predict_dict</a></code></li>
<li><code><a title="baskerville.models.base_spark.udf_predict_dict" href="#baskerville.models.base_spark.udf_predict_dict">udf_predict_dict</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="baskerville.models.base_spark.SparkPipelineBase" href="#baskerville.models.base_spark.SparkPipelineBase">SparkPipelineBase</a></code></h4>
<ul class="">
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns" href="#baskerville.models.base_spark.SparkPipelineBase.add_cache_columns">add_cache_columns</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns" href="#baskerville.models.base_spark.SparkPipelineBase.add_calc_columns">add_calc_columns</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns" href="#baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns">add_post_groupby_columns</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.create_runtime" href="#baskerville.models.base_spark.SparkPipelineBase.create_runtime">create_runtime</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.cross_reference" href="#baskerville.models.base_spark.SparkPipelineBase.cross_reference">cross_reference</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.feature_calculation" href="#baskerville.models.base_spark.SparkPipelineBase.feature_calculation">feature_calculation</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.feature_extraction" href="#baskerville.models.base_spark.SparkPipelineBase.feature_extraction">feature_extraction</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.feature_update" href="#baskerville.models.base_spark.SparkPipelineBase.feature_update">feature_update</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.features_to_dict" href="#baskerville.models.base_spark.SparkPipelineBase.features_to_dict">features_to_dict</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.filter_cache" href="#baskerville.models.base_spark.SparkPipelineBase.filter_cache">filter_cache</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.filter_columns" href="#baskerville.models.base_spark.SparkPipelineBase.filter_columns">filter_columns</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.finish_up" href="#baskerville.models.base_spark.SparkPipelineBase.finish_up">finish_up</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by" href="#baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by">get_columns_to_filter_by</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.get_data" href="#baskerville.models.base_spark.SparkPipelineBase.get_data">get_data</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs" href="#baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs">get_group_by_aggs</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations" href="#baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations">get_post_group_by_calculations</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.get_window" href="#baskerville.models.base_spark.SparkPipelineBase.get_window">get_window</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.group_by" href="#baskerville.models.base_spark.SparkPipelineBase.group_by">group_by</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns" href="#baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns">handle_missing_columns</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_values" href="#baskerville.models.base_spark.SparkPipelineBase.handle_missing_values">handle_missing_values</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.initialize" href="#baskerville.models.base_spark.SparkPipelineBase.initialize">initialize</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session" href="#baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session">instantiate_spark_session</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.label_or_predict" href="#baskerville.models.base_spark.SparkPipelineBase.label_or_predict">label_or_predict</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.load_test" href="#baskerville.models.base_spark.SparkPipelineBase.load_test">load_test</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names" href="#baskerville.models.base_spark.SparkPipelineBase.normalize_host_names">normalize_host_names</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.predict" href="#baskerville.models.base_spark.SparkPipelineBase.predict">predict</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.preprocessing" href="#baskerville.models.base_spark.SparkPipelineBase.preprocessing">preprocessing</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.process_data" href="#baskerville.models.base_spark.SparkPipelineBase.process_data">process_data</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.refresh_cache" href="#baskerville.models.base_spark.SparkPipelineBase.refresh_cache">refresh_cache</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.register_metrics" href="#baskerville.models.base_spark.SparkPipelineBase.register_metrics">register_metrics</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.rename_columns" href="#baskerville.models.base_spark.SparkPipelineBase.rename_columns">rename_columns</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.reset" href="#baskerville.models.base_spark.SparkPipelineBase.reset">reset</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.run" href="#baskerville.models.base_spark.SparkPipelineBase.run">run</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.save" href="#baskerville.models.base_spark.SparkPipelineBase.save">save</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table" href="#baskerville.models.base_spark.SparkPipelineBase.save_df_to_table">save_df_to_table</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.set_accumulators" href="#baskerville.models.base_spark.SparkPipelineBase.set_accumulators">set_accumulators</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.set_broadcasts" href="#baskerville.models.base_spark.SparkPipelineBase.set_broadcasts">set_broadcasts</a></code></li>
<li><code><a title="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache" href="#baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache">set_up_request_set_cache</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.2</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>