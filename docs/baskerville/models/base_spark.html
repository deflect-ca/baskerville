<!DOCTYPE html>
<html lang="en">
 <head>
  <meta charset="utf-8"/>
  <meta content="width=device-width, initial-scale=1, minimum-scale=1" name="viewport"/>
  <meta content="pdoc 0.7.2" name="generator"/>
  <title>
   baskerville.models.base_spark API documentation
  </title>
  <meta content="" name="description"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css" rel="stylesheet"/>
  <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet"/>
  <style>
   .flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}
  </style>
  <style media="screen and (min-width: 700px)">
   @media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}
  </style>
  <style media="print">
   @media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}
  </style>
 </head>
 <body>
  <main>
   <article id="content">
    <header>
     <h1 class="title">
      Module
      <code>
       baskerville.models.base_spark
      </code>
     </h1>
    </header>
    <section id="section-intro">
     <details class="source">
      <summary>
       <span>
        Expand source code
       </span>
      </summary>
      <pre><code class="python"># Copyright (c) 2020, eQualit.ie inc.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.


import json
import datetime
import itertools
import os

from baskerville.models.base import PipelineBase
from baskerville.models.feature_manager import FeatureManager
from baskerville.spark.helpers import save_df_to_table, reset_spark_storage, set_unknown_prediction
from baskerville.spark.schemas import get_cache_schema
from baskerville.util.helpers import TimeBucket, FOLDER_CACHE, instantiate_from_str
from pyspark.sql import types as T, DataFrame

from baskerville.spark import get_or_create_spark_session
from dateutil.tz import tzutc
from collections import OrderedDict

from baskerville.util.baskerville_tools import BaskervilleDBTools
from baskerville.util.enums import Step

from pyspark.sql import functions as F
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

from baskerville.db.models import RequestSet
from baskerville.models.request_set_cache import RequestSetSparkCache


class SparkPipelineBase(PipelineBase):
    """
    The base class for all pipelines that use spark. It initializes spark
    session and provides basic implementation for some of the main methods
    """

    def __init__(self,
                 db_conf,
                 engine_conf,
                 spark_conf,
                 clean_up=True,
                 group_by_cols=('client_request_host', 'client_ip'),
                 *args,
                 **kwargs
                 ):

        super().__init__(db_conf, engine_conf, clean_up)
        self.start_time = datetime.datetime.utcnow()
        self.request_set_cache = None
        self.spark = None
        self.tools = None
        self.metrics_registry = None
        self.spark_conf = spark_conf
        self.data_parser = self.engine_conf.data_config.parser
        self.group_by_cols = list(set(group_by_cols))
        self.group_by_aggs = None
        self.post_group_by_aggs = None
        self.columns_to_filter_by = None
        self._can_predict = False
        self._is_initialized = False
        self.drop_if_missing_filter = None
        self.cols_to_drop = None
        self.cache_columns = [
            'target',
            'ip',
            'first_ever_request',
            'old_subset_count',
            'old_features',
            'old_num_requests',
        ]
        self.cache_config = {
            'db_url': self.db_url,
            'db_driver': self.spark_conf.db_driver,
            'user': self.db_conf.user,
            'password': self.db_conf.password
        }
        self.step_to_action = OrderedDict(
            zip([
                Step.preprocessing,
                Step.group_by,
                Step.feature_calculation,
                Step.label_or_predict,
                Step.trigger_challenge,
                Step.save
            ], [
                self.preprocessing,
                self.group_by,
                self.feature_calculation,
                self.label_or_predict,
                self.trigger_challenge,
                self.save
            ]))

        self.remaining_steps = list(self.step_to_action.keys())

        self.time_bucket = TimeBucket(self.engine_conf.time_bucket)
        self.feature_manager = FeatureManager(self.engine_conf)
        self.model_index = None
        self.model = None

    def load_test(self):
        """
        If the user has set the load_test configuration, then multiply the
        traffic by `self.engine_conf.load_test` times to do load testing.
        :return:
        """
        if self.engine_conf.load_test:
            df = self.logs_df.persist(self.spark_conf.storage_level)

            for i in range(self.engine_conf.load_test - 1):
                df = df.withColumn(
                    'client_ip', F.round(F.rand(42)).cast('string')
                )
                self.logs_df = self.logs_df.union(df).persist(
                    self.spark_conf.storage_level
                )

            df.unpersist()
            del df
            self.logger.info(
                f'---- Count after multiplication: {self.logs_df.count()}'
            )

    def reset(self):
        """
        Unpersist rdds and dataframes and call GC - see broadcast memory
        release issue
        :return:
        """
        import gc

        reset_spark_storage()
        gc.collect()

    def initialize(self):
        """
        Set the basics:
        - Connect to the database
        - Initialize spark session
        - Get active model and scaler and set them to broadcast variables
        - Get active features with their active columns, update columns etc and
        set the relevant broadcast variables
        - Set the _can_predict flag
        - Instantiate the accumulators (for metrics)
        - Instantiate request set cache.
        :return:
        """

        # initialize db access tools
        self.tools = BaskervilleDBTools(self.db_conf)
        self.tools.connect_to_db()

        # initialize spark session
        self.spark = self.instantiate_spark_session()
        self.feature_manager.initialize()
        self.drop_if_missing_filter = self.data_parser.drop_if_missing_filter()

        # set up cache
        self.request_set_cache = self.set_up_request_set_cache()

        # gather calculations
        self.group_by_aggs = self.get_group_by_aggs()
        self.columns_to_filter_by = self.get_columns_to_filter_by()
        self.cols_to_drop = set(
            self.feature_manager.active_feature_names +
            self.feature_manager.active_columns +
            list(self.group_by_aggs.keys()) +
            self.feature_manager.update_feature_cols
        ).difference(RequestSet.columns)

        if self.engine_conf.model_id:
            self.model_index = self.tools.get_ml_model_from_db(
                self.engine_conf.model_id)
            self.model = instantiate_from_str(self.model_index.algorithm)
            self.model.load(bytes.decode(
                self.model_index.classifier, 'utf8'), self.spark)
        else:
            self.model = None

        self._is_initialized = True

    def get_columns_to_filter_by(self):
        """
        Gathers all the columns that need to be present in the dataframe
        for the processing to complete.
        group_by_cols: the columns to group data on
        active_columns: the columns that the active features have declared as
        necessary
        timestamp_column: the time column - all logs need to have a time column
        :return: a set of the column names that need to be present in the
        dataframe
        :rtype: set[str]
        """
        cols = self.group_by_cols + self.feature_manager.active_columns
        cols.append(self.engine_conf.data_config.timestamp_column)
        return set(cols)

    def get_group_by_aggs(self):
        """
        Gathers all the group by arguments:
        basic_aggs:
            - first_request
            - last_request
            - num_requests
        column_aggs: the columns the features need for computation are gathered
         as lists
        feature_aggs: the columns the features need for computation
        Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
        The basic aggs have a priority over the feature and column aggs.
        The feature aggs have a priority over the column aggs (if a feature
        has explicitly asked for a computation for a specific column it relies
        upon, then the computation will be stored instead of the column
        aggregation as list)

        :return: a dictionary with the name of the group by aggregation columns
        as keys and the respective Column aggregation as values
        :rtype: dict[string, pyspark.Column]
        """
        basic_aggs = {
            'first_request': F.min(F.col('@timestamp')).alias('first_request'),
            'last_request': F.max(F.col('@timestamp')).alias('last_request'),
            'num_requests': F.count(F.col('@timestamp')).alias('num_requests')
        }

        column_aggs = {
            c: F.collect_list(F.col(c)).alias(c)
            for c in self.feature_manager.active_columns
        }

        feature_aggs = self.feature_manager.get_feature_group_by_aggs()

        basic_aggs.update(
            {k: v for k, v in feature_aggs.items() if k not in basic_aggs}
        )
        basic_aggs.update(
            {k: v for k, v in column_aggs.items() if k not in basic_aggs}
        )

        return basic_aggs

    def get_post_group_by_calculations(self):
        """
        Gathers the columns and computations to be performed after the grouping
        of the data (logs_df)
        Basic post group by columns:
        - `id_runtime`
        - `time_bucket`
        - `start`
        - `stop`
        - `subset_count`

        if there is an ML Model defined:
        - `model_version`
        - `classifier`
        - `scaler`
        - `model_features`

        Each feature can also define post group by calculations using the
        post_group_by_calcs dict.

        :return: A dictionary with the name of the result columns as keys and
        their respective computations as values
        :rtype: dict[string, pyspark.Column]
        """
        if self.post_group_by_aggs:
            return self.post_group_by_aggs

        post_group_by_columns = {
            'id_runtime': F.lit(self.runtime.id),
            'time_bucket': F.lit(self.time_bucket.sec),
            'start': F.when(
                F.col('first_ever_request').isNotNull(),
                F.col('first_ever_request')
            ).otherwise(F.col('first_request')),
            'stop': F.col('last_request'),
            'subset_count': F.when(
                F.col('old_subset_count').isNotNull(),
                F.col('old_subset_count')
            ).otherwise(F.lit(0))
        }

        if self.model_index:
            post_group_by_columns['model_version'] = F.lit(
                self.model_index.id
            )

        # todo: what if a feature defines a column name that already exists?
        # e.g. like `subset_count`
        post_group_by_columns.update(
            self.feature_manager.post_group_by_calculations
        )

        return post_group_by_columns

    def __getitem__(self, name):
        if name == 'run':
            if not self._is_initialized:
                raise RuntimeError(
                    f'__getitem__: {self.__class__.__name__} '
                    f'has not been initialized yet.'
                )
        return getattr(self, name)

    def __getattribute__(self, name):
        if name == 'run':
            if not self._is_initialized:
                raise RuntimeError(
                    f'__getattribute__:{self.__class__.__name__} '
                    f'has not been initialized yet.'
                )

        return super().__getattribute__(name)

    def filter_cache(self):
        """
        Use the current logs to find the past request sets - if any - in the
        request set cache
        :return:
        """
        df = self.logs_df.select(
            F.col('client_request_host').alias('target'),
            F.col('client_ip').alias('ip'),
        ).distinct().alias('a').persist(self.spark_conf.storage_level)

        self.request_set_cache.filter_by(df)

        df.unpersist()
        del df

    def run(self):
        """
        Runs the configured steps.
        :return:
        """
        self.logger.info(
            f'Spark UI accessible at:{self.spark.sparkContext.uiWebUrl}'
        )
        self.create_runtime()
        self.get_data()
        self.process_data()

    def process_data(self):
        """
        Splits the data into time bucket length windows and executes all the steps
        :return:
        """
        if self.logs_df.count() == 0:
            self.logger.info('No data in to process.')
        else:
            for window_df in self.get_window():
                self.logs_df = window_df
                self.logs_df = self.logs_df.repartition(
                    *self.group_by_cols
                ).persist(self.spark_conf.storage_level)
                self.remaining_steps = list(self.step_to_action.keys())

                for step, action in self.step_to_action.items():
                    self.logger.info('Starting step {}'.format(step))
                    action()
                    self.logger.info('Completed step {}'.format(step))
                    self.remaining_steps.remove(step)
                self.reset()

    def get_window(self):

        from pyspark.sql import functions as F

        df = self.logs_df.withColumn('timestamp',
                                     F.col('@timestamp').cast('timestamp'))
        df = df.sort('timestamp')
        current_window_start = df.agg({"timestamp": "min"}).collect()[0][0]
        stop = df.agg({"timestamp": "max"}).collect()[0][0]
        window_df = None
        current_end = current_window_start + self.time_bucket.td

        while True:
            if window_df:
                window_df.unpersist(blocking=True)
                del window_df
            filter_ = (
                (F.col('timestamp') &gt;= current_window_start) &amp;
                (F.col('timestamp') &lt; current_end)
            )
            window_df = df.where(filter_).persist(
                self.spark_conf.storage_level
            )
            if not window_df.rdd.isEmpty():
                print(f'# Request sets = {window_df.count()}')
                yield window_df
            else:
                self.logger.info(f'Empty window df for {str(filter_._jc)}')
            current_window_start = current_window_start + self.time_bucket.td
            current_end = current_window_start + self.time_bucket.td
            if current_window_start &gt;= stop:
                window_df.unpersist(blocking=True)
                del window_df
                break

    def create_runtime(self):
        """
        Create a Runtime in the Baskerville database.
        :return:
        """
        raise NotImplementedError(
            'SparkPipelineBase does not have an implementation '
            'for _create_runtime.'
        )

    def get_data(self):
        """
        Get dataframe of log data
        :return:
        """
        raise NotImplementedError(
            'SparkPipelineBase does not have an implementation '
            'for _get_data.'
        )

    def preprocessing(self):
        """
        Fill missing values, add calculation cols, and filter.
        :return:
        """

        self.handle_missing_columns()
        self.rename_columns()
        self.filter_columns()
        self.handle_missing_values()
        self.normalize_host_names()
        self.add_calc_columns()

    def group_by(self):
        """
        Group the logs df by the given group-by columns (normally IP, host).
        :return: None
        """
        self.logs_df = self.logs_df.groupBy(
            *self.group_by_cols
        ).agg(
            *self.group_by_aggs.values()
        )

    def feature_calculation(self):
        """
        Add calculation cols, extract features, and update.
        :return:
        """
        self.add_post_groupby_columns()
        self.feature_extraction()
        self.feature_update()

    def label_or_predict(self):
        """
        Apply label from MISP or predict label.
        #todo: use separate steps for this
        :return:
        """

        from pyspark.sql import functions as F
        from pyspark.sql.types import IntegerType

        if self.engine_conf.cross_reference:
            self.cross_reference()
        else:
            self.logs_df = self.logs_df.withColumn(
                'label', F.lit(None).cast(IntegerType()))
        self.predict()

    def get_challenges(self, df, challenge_threshold):
        def challenge_decision(num_normals, num_anomalies, threshold):
            if num_anomalies &gt;= threshold * (num_anomalies + num_normals):
                return 1
            return 0

        challenge_decision_udf = udf(challenge_decision, IntegerType())

        df = df.select(['target', 'prediction'])\
            .groupBy(['target', 'prediction'])\
            .count()\
            .groupBy('target')\
            .pivot('prediction').agg(F.first('count'))\
            .withColumn('challenge', challenge_decision_udf(F.col('0'), F.col('1'), F.lit(challenge_threshold)))\
            .select(['target', 'challenge'])
        return []

    def send_challenges(self, challenges):
        pass

    def save_challenges_to_db(self, challenges):
        pass

    def trigger_challenge(self):
        """
        Trigger the challenge per host
        :return:
        """
        # if not self.engine_conf.trigger_challenge:
        #     return
        #
        # challenges = self.get_challenges(self.logs_df, self.engine_conf.challenge_threshold)
        # if len(challenges):
        #     self.send_challenges(challenges)
        #     self.save_challenges_to_db(challenges)

    def save(self):
        """
        Update dataframe, save to database, and update cache.
        :return:
        """
        request_set_columns = RequestSet.columns[:]
        not_common = {
            'prediction', 'model_version', 'label', 'id_attribute',
            'updated_at'
        }.difference(self.logs_df.columns)

        for c in not_common:
            request_set_columns.remove(c)

        # filter the logs df with the request_set columns
        self.logs_df = self.logs_df.select(request_set_columns)

        # save request_sets
        self.logger.debug('Saving request_sets')
        self.save_df_to_table(
            self.logs_df.select(request_set_columns),
            RequestSet.__tablename__
        )
        self.refresh_cache()

    def refresh_cache(self):
        """
        Update the cache with the current batch of logs in logs_df and clean up
        :return:
        """
        self.request_set_cache.update_self(self.logs_df)
        self.logs_df.unpersist()
        self.logs_df = None
        # self.spark.catalog.clearCache()

    def finish_up(self):
        """
        Try to gracefully stop by committing to database, emptying the cache,
        unpersist everything, disconnecting from db and clearing spark's cache
        :return: None
        """
        if len(self.remaining_steps) == 0:
            request_set_count = self.tools.session.query(RequestSet).filter(
                RequestSet.id_runtime == self.runtime.id).count()
            self.runtime.processed = True
            self.runtime.n_request_sets = request_set_count
            self.tools.session.commit()
            self.logger.debug('finished updating runtime')
        try:
            self.request_set_cache.empty()
        except AttributeError:
            pass

        if hasattr(self, 'logs_df') and self.logs_df and isinstance(
                self.logs_df, DataFrame):
            self.logs_df.unpersist(blocking=True)
            self.logs_df = None

        if self.tools and self.tools.session:
            self.tools.session.commit()
            self.tools.disconnect_from_db()
        if self.spark:
            try:
                if self.spark_conf.spark_python_profile:
                    self.spark.sparkContext.show_profiles()
                reset_spark_storage()
            except AttributeError:
                self.logger.debug('clearCache attr error')
                pass

    def instantiate_spark_session(self):
        """
        #todo
        :return:
        """
        return get_or_create_spark_session(self.spark_conf)

    def set_up_request_set_cache(self):
        """
        Set up an instance of RequestSetSparkCache using the cache
        configuration. Also, load past (start_time - expiry_time)
        if cache_load_past is configured.
        :return:
        """
        self.request_set_cache = RequestSetSparkCache(
            cache_config=self.cache_config,
            table_name=RequestSet.__tablename__,
            columns_to_keep=(
                'target',
                'ip',
                F.col('start').alias('first_ever_request'),
                F.col('subset_count').alias('old_subset_count'),
                F.col('features').alias('old_features'),
                F.col('num_requests').alias('old_num_requests'),
                F.col('updated_at')
            ),
            expire_if_longer_than=self.engine_conf.cache_expire_time,
            path=os.path.join(self.engine_conf.storage_path, FOLDER_CACHE)
        )
        if self.engine_conf.cache_load_past:
            self.request_set_cache = self.request_set_cache.load(
                update_date=(
                    self.start_time - datetime.timedelta(
                        seconds=self.engine_conf.cache_expire_time
                    )
                ).replace(tzinfo=tzutc()),
                extra_filters=(
                    F.col('time_bucket') == self.time_bucket.sec
                )  # todo: &amp; (F.col("id_runtime") == self.runtime.id)?
            )
        else:
            self.request_set_cache.load_empty(get_cache_schema())

        self.logger.info(f'In cache: {self.request_set_cache.count()}')

        return self.request_set_cache

    def handle_missing_columns(self):
        """
        Check for missing columns and if any use the data parser to add them
        and fill them with defaults, if specified in the schema.
        :return:
        """
        missing = self.data_parser.check_for_missing_columns(self.logs_df)
        if missing:
            self.logs_df = self.data_parser.add_missing_columns(
                self.logs_df, missing
            )

    def rename_columns(self):
        """
        Some column names may cause issues with spark, e.g. `geo.ip.lat`, so
        the features that use those can declare in `columns_renamed` that those
        columns should be renamed to something else, e.g. `geo_ip_lat`
        :return:
        """
        for k, v in self.feature_manager.column_renamings:
            self.logs_df = self.logs_df.withColumnRenamed(k, v)

    def filter_columns(self):
        """
        Logs df may have columns that are not necessary for the analysis,
        filter them out to reduce the memory footprint.
        The absolutely essential columns are the group by columns and the
        timestamp column, or else the rest of the process will fail.
        And of course the columns the features need, the active columns.
        :return:None
        """

        where = self.drop_if_missing_filter
        self.logs_df = self.logs_df.select(*self.columns_to_filter_by)
        if where is not None:
            self.logs_df = self.logs_df.where(where)

        # todo: metric for dropped logs
        print(f'{self.logs_df.count()}')

    def handle_missing_values(self):
        self.logs_df = self.data_parser.fill_missing_values(self.logs_df)

    def normalize_host_names(self):
        """
        From www.somedomain.tld keep somedomain
        # todo: improve this and remove udf
        # todo: keep original target in a separate field in db
        :return:
        """
        from baskerville.spark.udfs import udf_normalize_host_name

        self.logs_df = self.logs_df.withColumn(
            'client_request_host',
            udf_normalize_host_name(
                F.col('client_request_host').cast(T.StringType())
            )
        )

    def add_calc_columns(self):
        """
        Each feature needs different calculations in order to be able to
        compute the feature value. Go through the features and apply the
        calculations. Each calculation can occur only once, calculations
        with the same name will be ignored.
        :return:
        """

        self.logs_df = self.logs_df.withColumn(
            '@timestamp', F.col('@timestamp').cast('timestamp')
        )

        for k, v in self.feature_manager.pre_group_by_calculations.items():
            self.logs_df = self.logs_df.withColumn(
                k, v
            )

        for f in self.feature_manager.active_features:
            self.logs_df = f.misc_compute(self.logs_df)

    def add_cache_columns(self):
        """
        Add columns from the cache to facilitate the
        feature extraction, prediction, and save processes
        :return:
        """
        self.logs_df = self.logs_df.alias('logs_df')
        self.filter_cache()
        self.logs_df = self.request_set_cache.update_df(
            self.logs_df, select_cols=self.cache_columns
        )
        self.logger.debug(
            f'****** &gt; # of rows in cache: {self.request_set_cache.count()}')

    def add_post_groupby_columns(self):
        """
        Add extra columns after the grouping of the logs to facilitate the
        feature extraction, prediction, and save processes
        Extra columns:
        * general:
        ----------
        - ip
        - target
        - id_runtime
        - time_bucket
        - start
        - subset_count

        * cache columns:
        ----------------
        - 'id',
        - 'first_ever_request',
        - 'old_subset_count',
        - 'old_features',
        - 'old_num_requests'

        * model related:
        ----------------
        - model_version
        - classifier
        - scaler
        - model_features

        :return: None
        """
        # todo: shouldn't this be a renaming?
        self.logs_df = self.logs_df.withColumn('ip', F.col('client_ip'))
        self.logs_df = self.logs_df.withColumn(
            'target', F.col('client_request_host')
        )
        self.add_cache_columns()

        for k, v in self.get_post_group_by_calculations().items():
            self.logs_df = self.logs_df.withColumn(k, v)

        self.logs_df = self.logs_df.drop('old_subset_count')

    def feature_extraction(self):
        """
        For each feature compute the feature value and add it as a column in
        the dataframe
        :return: None
        """

        for feature in self.feature_manager.active_features:
            self.logs_df = feature.compute(self.logs_df)

        self.logger.info(
            f'Number of logs after feature extraction {self.logs_df.count()}'
        )
        # self.logs_df = self.logs_df.cache()

    def remove_feature_columns(self):
        self.logs_df = self.logs_df.drop(
            *self.feature_manager.active_feature_names
        )

    def feature_update(self):
        """
        Update current batch's features with past features - if any - using
        the request set cache.
        :return:
        """
        # convert current features to dict since the already saved request_sets
        # have the features as json
        self.features_to_dict('features')
        self.features_to_dict('old_features')
        self.logs_df.persist(self.spark_conf.storage_level)

        for f in self.feature_manager.updateable_active_features:
            self.logs_df = f.update(self.logs_df).cache()

        self.logs_df = self.logs_df.withColumn('features', F.create_map(
            *list(
                itertools.chain(
                    *[
                        (F.lit(f.feature_name),
                         F.col(f.updated_feature_col_name))
                        for f in
                        self.feature_manager.updateable_active_features
                    ]
                )
            )
        ))

        # older way with a udf:
        # self.logs_df = self.logs_df.withColumn(
        #     'features',
        #     udf_update_features(
        #         F.lit(cPickle.dumps([  # #WST
        #             StrippedFeature(f.feature_name, f.update_row)
        #             for f in self.feature_manager.active_features
        #         ]
        #         )),
        #         'features',
        #         'old_features',
        #         'subset_count',
        #         'start',
        #         'last_request'
        #     )
        # ).persist(self.spark_conf.storage_level)
        self.logs_df = self.logs_df.drop('old_features')

        self.logs_df = self.logs_df.withColumn(
            'subset_count',
            F.col('subset_count') + F.lit(1)
        )

        self.logs_df = self.logs_df.withColumn(
            'num_requests',
            F.when(
                F.col('old_num_requests') &gt; 0,
                F.col('old_num_requests') + F.col('num_requests')
            ).otherwise(F.col('num_requests'))
        )
        self.logs_df = self.logs_df.drop('old_num_requests')
        diff = (F.unix_timestamp('last_request', format="YYYY-MM-DD %H:%M:%S")
                - F.unix_timestamp(
                    'start', format="YYYY-MM-DD %H:%M:%S")
                ).cast('float')
        self.logs_df = self.logs_df.withColumn('total_seconds', diff)
        self.logs_df = self.logs_df.drop(*self.cols_to_drop)

    def cross_reference(self):
        """
        Look up IPs in attributes table, and label as malicious (-1) if listed
        there.
        :return:
        """
        from pyspark.sql import functions as F
        from baskerville.spark.udfs import udf_cross_reference_misp

        self.logs_df = self.logs_df.withColumn(
            'cross_reference',
            udf_cross_reference_misp('ip',
                                     F.lit(json.dumps(self.db_conf.__dict__)))
        )
        self.logs_df = self.logs_df.withColumn(
            'label',
            F.when(F.col('cross_reference.label') != 0,
                   F.col('cross_reference.label')).otherwise(None)
        )
        self.logs_df = self.logs_df.withColumn(
            'id_attribute',
            F.when(F.col('cross_reference.id_attribute') != 0,
                   F.col('cross_reference.id_attribute')).otherwise(None)
        )

    def predict(self):
        """
        Predict on the request_sets. Prediction on request_sets
        requires feature averaging where there is an existing request_set.
`        :return: None
        """
        if self.model:
            self.logs_df = self.model.predict(self.logs_df)
        else:
            self.logs_df = set_unknown_prediction(self.logs_df).withColumn(
                'prediction', F.col('prediction').cast(T.IntegerType())
            ).withColumn(
                'score', F.col('score').cast(T.FloatType())
            ).withColumn(
                'threshold', F.col('threshold').cast(T.FloatType()))

    def save_df_to_table(
            self, df, table_name, json_cols=('features',), mode='append'
    ):
        """
        Save the dataframe to the database. Jsonify any columns that need to
        be
        :param pyspark.Dataframe df: the dataframe to save
        :param str table_name: where to save the dataframe
        :param iterable json_cols: the name of the columns that need to be
        jsonified, e.g. `features`
        :param str mode: save mode, 'append', 'overwrite' etc, see spark save
        options
        :return: None
        """
        # todo: mostly duplicated code, refactor
        self.db_conf.conn_str = self.db_url
        save_df_to_table(
            df,
            table_name,
            self.db_conf.__dict__,
            json_cols=json_cols,
            storage_level=self.spark_conf.storage_level,
            mode=mode,
            db_driver=self.spark_conf.db_driver
        )

    def features_to_dict(self, column):
        """
        Convert the features to dictionary
        :param string column: the name of the column to use as output
        :return: None
        """
        from pyspark.sql import functions as F

        self.logs_df = self.logs_df.withColumn(
            column,
            F.create_map(
                *list(
                    itertools.chain(
                        *[
                            (F.lit(f.feature_name), F.col(f.feature_name))
                            for f in self.feature_manager.active_features
                        ]
                    )
                ))
        )</code></pre>
     </details>
    </section>
    <section>
    </section>
    <section>
    </section>
    <section>
    </section>
    <section>
     <h2 class="section-title" id="header-classes">
      Classes
     </h2>
     <dl>
      <dt id="baskerville.models.base_spark.SparkPipelineBase">
       <code class="flex name class">
        <span>
         class
         <span class="ident">
          SparkPipelineBase
         </span>
        </span>
        <span>
         (
        </span>
        <span>
         db_conf, engine_conf, spark_conf, clean_up=True, group_by_cols=('client_request_host', 'client_ip'), *args, **kwargs)
        </span>
       </code>
      </dt>
      <dd>
       <section class="desc">
        <p>
         The base class for all pipelines that use spark. It initializes spark
session and provides basic implementation for some of the main methods
        </p>
       </section>
       <details class="source">
        <summary>
         <span>
          Expand source code
         </span>
        </summary>
        <pre><code class="python">class SparkPipelineBase(PipelineBase):
    """
    The base class for all pipelines that use spark. It initializes spark
    session and provides basic implementation for some of the main methods
    """

    def __init__(self,
                 db_conf,
                 engine_conf,
                 spark_conf,
                 clean_up=True,
                 group_by_cols=('client_request_host', 'client_ip'),
                 *args,
                 **kwargs
                 ):

        super().__init__(db_conf, engine_conf, clean_up)
        self.start_time = datetime.datetime.utcnow()
        self.request_set_cache = None
        self.spark = None
        self.tools = None
        self.metrics_registry = None
        self.spark_conf = spark_conf
        self.data_parser = self.engine_conf.data_config.parser
        self.group_by_cols = list(set(group_by_cols))
        self.group_by_aggs = None
        self.post_group_by_aggs = None
        self.columns_to_filter_by = None
        self._can_predict = False
        self._is_initialized = False
        self.drop_if_missing_filter = None
        self.cols_to_drop = None
        self.cache_columns = [
            'target',
            'ip',
            'first_ever_request',
            'old_subset_count',
            'old_features',
            'old_num_requests',
        ]
        self.cache_config = {
            'db_url': self.db_url,
            'db_driver': self.spark_conf.db_driver,
            'user': self.db_conf.user,
            'password': self.db_conf.password
        }
        self.step_to_action = OrderedDict(
            zip([
                Step.preprocessing,
                Step.group_by,
                Step.feature_calculation,
                Step.label_or_predict,
                Step.trigger_challenge,
                Step.save
            ], [
                self.preprocessing,
                self.group_by,
                self.feature_calculation,
                self.label_or_predict,
                self.trigger_challenge,
                self.save
            ]))

        self.remaining_steps = list(self.step_to_action.keys())

        self.time_bucket = TimeBucket(self.engine_conf.time_bucket)
        self.feature_manager = FeatureManager(self.engine_conf)
        self.model_index = None
        self.model = None

    def load_test(self):
        """
        If the user has set the load_test configuration, then multiply the
        traffic by `self.engine_conf.load_test` times to do load testing.
        :return:
        """
        if self.engine_conf.load_test:
            df = self.logs_df.persist(self.spark_conf.storage_level)

            for i in range(self.engine_conf.load_test - 1):
                df = df.withColumn(
                    'client_ip', F.round(F.rand(42)).cast('string')
                )
                self.logs_df = self.logs_df.union(df).persist(
                    self.spark_conf.storage_level
                )

            df.unpersist()
            del df
            self.logger.info(
                f'---- Count after multiplication: {self.logs_df.count()}'
            )

    def reset(self):
        """
        Unpersist rdds and dataframes and call GC - see broadcast memory
        release issue
        :return:
        """
        import gc

        reset_spark_storage()
        gc.collect()

    def initialize(self):
        """
        Set the basics:
        - Connect to the database
        - Initialize spark session
        - Get active model and scaler and set them to broadcast variables
        - Get active features with their active columns, update columns etc and
        set the relevant broadcast variables
        - Set the _can_predict flag
        - Instantiate the accumulators (for metrics)
        - Instantiate request set cache.
        :return:
        """

        # initialize db access tools
        self.tools = BaskervilleDBTools(self.db_conf)
        self.tools.connect_to_db()

        # initialize spark session
        self.spark = self.instantiate_spark_session()
        self.feature_manager.initialize()
        self.drop_if_missing_filter = self.data_parser.drop_if_missing_filter()

        # set up cache
        self.request_set_cache = self.set_up_request_set_cache()

        # gather calculations
        self.group_by_aggs = self.get_group_by_aggs()
        self.columns_to_filter_by = self.get_columns_to_filter_by()
        self.cols_to_drop = set(
            self.feature_manager.active_feature_names +
            self.feature_manager.active_columns +
            list(self.group_by_aggs.keys()) +
            self.feature_manager.update_feature_cols
        ).difference(RequestSet.columns)

        if self.engine_conf.model_id:
            self.model_index = self.tools.get_ml_model_from_db(
                self.engine_conf.model_id)
            self.model = instantiate_from_str(self.model_index.algorithm)
            self.model.load(bytes.decode(
                self.model_index.classifier, 'utf8'), self.spark)
        else:
            self.model = None

        self._is_initialized = True

    def get_columns_to_filter_by(self):
        """
        Gathers all the columns that need to be present in the dataframe
        for the processing to complete.
        group_by_cols: the columns to group data on
        active_columns: the columns that the active features have declared as
        necessary
        timestamp_column: the time column - all logs need to have a time column
        :return: a set of the column names that need to be present in the
        dataframe
        :rtype: set[str]
        """
        cols = self.group_by_cols + self.feature_manager.active_columns
        cols.append(self.engine_conf.data_config.timestamp_column)
        return set(cols)

    def get_group_by_aggs(self):
        """
        Gathers all the group by arguments:
        basic_aggs:
            - first_request
            - last_request
            - num_requests
        column_aggs: the columns the features need for computation are gathered
         as lists
        feature_aggs: the columns the features need for computation
        Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
        The basic aggs have a priority over the feature and column aggs.
        The feature aggs have a priority over the column aggs (if a feature
        has explicitly asked for a computation for a specific column it relies
        upon, then the computation will be stored instead of the column
        aggregation as list)

        :return: a dictionary with the name of the group by aggregation columns
        as keys and the respective Column aggregation as values
        :rtype: dict[string, pyspark.Column]
        """
        basic_aggs = {
            'first_request': F.min(F.col('@timestamp')).alias('first_request'),
            'last_request': F.max(F.col('@timestamp')).alias('last_request'),
            'num_requests': F.count(F.col('@timestamp')).alias('num_requests')
        }

        column_aggs = {
            c: F.collect_list(F.col(c)).alias(c)
            for c in self.feature_manager.active_columns
        }

        feature_aggs = self.feature_manager.get_feature_group_by_aggs()

        basic_aggs.update(
            {k: v for k, v in feature_aggs.items() if k not in basic_aggs}
        )
        basic_aggs.update(
            {k: v for k, v in column_aggs.items() if k not in basic_aggs}
        )

        return basic_aggs

    def get_post_group_by_calculations(self):
        """
        Gathers the columns and computations to be performed after the grouping
        of the data (logs_df)
        Basic post group by columns:
        - `id_runtime`
        - `time_bucket`
        - `start`
        - `stop`
        - `subset_count`

        if there is an ML Model defined:
        - `model_version`
        - `classifier`
        - `scaler`
        - `model_features`

        Each feature can also define post group by calculations using the
        post_group_by_calcs dict.

        :return: A dictionary with the name of the result columns as keys and
        their respective computations as values
        :rtype: dict[string, pyspark.Column]
        """
        if self.post_group_by_aggs:
            return self.post_group_by_aggs

        post_group_by_columns = {
            'id_runtime': F.lit(self.runtime.id),
            'time_bucket': F.lit(self.time_bucket.sec),
            'start': F.when(
                F.col('first_ever_request').isNotNull(),
                F.col('first_ever_request')
            ).otherwise(F.col('first_request')),
            'stop': F.col('last_request'),
            'subset_count': F.when(
                F.col('old_subset_count').isNotNull(),
                F.col('old_subset_count')
            ).otherwise(F.lit(0))
        }

        if self.model_index:
            post_group_by_columns['model_version'] = F.lit(
                self.model_index.id
            )

        # todo: what if a feature defines a column name that already exists?
        # e.g. like `subset_count`
        post_group_by_columns.update(
            self.feature_manager.post_group_by_calculations
        )

        return post_group_by_columns

    def __getitem__(self, name):
        if name == 'run':
            if not self._is_initialized:
                raise RuntimeError(
                    f'__getitem__: {self.__class__.__name__} '
                    f'has not been initialized yet.'
                )
        return getattr(self, name)

    def __getattribute__(self, name):
        if name == 'run':
            if not self._is_initialized:
                raise RuntimeError(
                    f'__getattribute__:{self.__class__.__name__} '
                    f'has not been initialized yet.'
                )

        return super().__getattribute__(name)

    def filter_cache(self):
        """
        Use the current logs to find the past request sets - if any - in the
        request set cache
        :return:
        """
        df = self.logs_df.select(
            F.col('client_request_host').alias('target'),
            F.col('client_ip').alias('ip'),
        ).distinct().alias('a').persist(self.spark_conf.storage_level)

        self.request_set_cache.filter_by(df)

        df.unpersist()
        del df

    def run(self):
        """
        Runs the configured steps.
        :return:
        """
        self.logger.info(
            f'Spark UI accessible at:{self.spark.sparkContext.uiWebUrl}'
        )
        self.create_runtime()
        self.get_data()
        self.process_data()

    def process_data(self):
        """
        Splits the data into time bucket length windows and executes all the steps
        :return:
        """
        if self.logs_df.count() == 0:
            self.logger.info('No data in to process.')
        else:
            for window_df in self.get_window():
                self.logs_df = window_df
                self.logs_df = self.logs_df.repartition(
                    *self.group_by_cols
                ).persist(self.spark_conf.storage_level)
                self.remaining_steps = list(self.step_to_action.keys())

                for step, action in self.step_to_action.items():
                    self.logger.info('Starting step {}'.format(step))
                    action()
                    self.logger.info('Completed step {}'.format(step))
                    self.remaining_steps.remove(step)
                self.reset()

    def get_window(self):

        from pyspark.sql import functions as F

        df = self.logs_df.withColumn('timestamp',
                                     F.col('@timestamp').cast('timestamp'))
        df = df.sort('timestamp')
        current_window_start = df.agg({"timestamp": "min"}).collect()[0][0]
        stop = df.agg({"timestamp": "max"}).collect()[0][0]
        window_df = None
        current_end = current_window_start + self.time_bucket.td

        while True:
            if window_df:
                window_df.unpersist(blocking=True)
                del window_df
            filter_ = (
                (F.col('timestamp') &gt;= current_window_start) &amp;
                (F.col('timestamp') &lt; current_end)
            )
            window_df = df.where(filter_).persist(
                self.spark_conf.storage_level
            )
            if not window_df.rdd.isEmpty():
                print(f'# Request sets = {window_df.count()}')
                yield window_df
            else:
                self.logger.info(f'Empty window df for {str(filter_._jc)}')
            current_window_start = current_window_start + self.time_bucket.td
            current_end = current_window_start + self.time_bucket.td
            if current_window_start &gt;= stop:
                window_df.unpersist(blocking=True)
                del window_df
                break

    def create_runtime(self):
        """
        Create a Runtime in the Baskerville database.
        :return:
        """
        raise NotImplementedError(
            'SparkPipelineBase does not have an implementation '
            'for _create_runtime.'
        )

    def get_data(self):
        """
        Get dataframe of log data
        :return:
        """
        raise NotImplementedError(
            'SparkPipelineBase does not have an implementation '
            'for _get_data.'
        )

    def preprocessing(self):
        """
        Fill missing values, add calculation cols, and filter.
        :return:
        """

        self.handle_missing_columns()
        self.rename_columns()
        self.filter_columns()
        self.handle_missing_values()
        self.normalize_host_names()
        self.add_calc_columns()

    def group_by(self):
        """
        Group the logs df by the given group-by columns (normally IP, host).
        :return: None
        """
        self.logs_df = self.logs_df.groupBy(
            *self.group_by_cols
        ).agg(
            *self.group_by_aggs.values()
        )

    def feature_calculation(self):
        """
        Add calculation cols, extract features, and update.
        :return:
        """
        self.add_post_groupby_columns()
        self.feature_extraction()
        self.feature_update()

    def label_or_predict(self):
        """
        Apply label from MISP or predict label.
        #todo: use separate steps for this
        :return:
        """

        from pyspark.sql import functions as F
        from pyspark.sql.types import IntegerType

        if self.engine_conf.cross_reference:
            self.cross_reference()
        else:
            self.logs_df = self.logs_df.withColumn(
                'label', F.lit(None).cast(IntegerType()))
        self.predict()

    def get_challenges(self, df, challenge_threshold):
        def challenge_decision(num_normals, num_anomalies, threshold):
            if num_anomalies &gt;= threshold * (num_anomalies + num_normals):
                return 1
            return 0

        challenge_decision_udf = udf(challenge_decision, IntegerType())

        df = df.select(['target', 'prediction'])\
            .groupBy(['target', 'prediction'])\
            .count()\
            .groupBy('target')\
            .pivot('prediction').agg(F.first('count'))\
            .withColumn('challenge', challenge_decision_udf(F.col('0'), F.col('1'), F.lit(challenge_threshold)))\
            .select(['target', 'challenge'])
        return []

    def send_challenges(self, challenges):
        pass

    def save_challenges_to_db(self, challenges):
        pass

    def trigger_challenge(self):
        """
        Trigger the challenge per host
        :return:
        """
        # if not self.engine_conf.trigger_challenge:
        #     return
        #
        # challenges = self.get_challenges(self.logs_df, self.engine_conf.challenge_threshold)
        # if len(challenges):
        #     self.send_challenges(challenges)
        #     self.save_challenges_to_db(challenges)

    def save(self):
        """
        Update dataframe, save to database, and update cache.
        :return:
        """
        request_set_columns = RequestSet.columns[:]
        not_common = {
            'prediction', 'model_version', 'label', 'id_attribute',
            'updated_at'
        }.difference(self.logs_df.columns)

        for c in not_common:
            request_set_columns.remove(c)

        # filter the logs df with the request_set columns
        self.logs_df = self.logs_df.select(request_set_columns)

        # save request_sets
        self.logger.debug('Saving request_sets')
        self.save_df_to_table(
            self.logs_df.select(request_set_columns),
            RequestSet.__tablename__
        )
        self.refresh_cache()

    def refresh_cache(self):
        """
        Update the cache with the current batch of logs in logs_df and clean up
        :return:
        """
        self.request_set_cache.update_self(self.logs_df)
        self.logs_df.unpersist()
        self.logs_df = None
        # self.spark.catalog.clearCache()

    def finish_up(self):
        """
        Try to gracefully stop by committing to database, emptying the cache,
        unpersist everything, disconnecting from db and clearing spark's cache
        :return: None
        """
        if len(self.remaining_steps) == 0:
            request_set_count = self.tools.session.query(RequestSet).filter(
                RequestSet.id_runtime == self.runtime.id).count()
            self.runtime.processed = True
            self.runtime.n_request_sets = request_set_count
            self.tools.session.commit()
            self.logger.debug('finished updating runtime')
        try:
            self.request_set_cache.empty()
        except AttributeError:
            pass

        if hasattr(self, 'logs_df') and self.logs_df and isinstance(
                self.logs_df, DataFrame):
            self.logs_df.unpersist(blocking=True)
            self.logs_df = None

        if self.tools and self.tools.session:
            self.tools.session.commit()
            self.tools.disconnect_from_db()
        if self.spark:
            try:
                if self.spark_conf.spark_python_profile:
                    self.spark.sparkContext.show_profiles()
                reset_spark_storage()
            except AttributeError:
                self.logger.debug('clearCache attr error')
                pass

    def instantiate_spark_session(self):
        """
        #todo
        :return:
        """
        return get_or_create_spark_session(self.spark_conf)

    def set_up_request_set_cache(self):
        """
        Set up an instance of RequestSetSparkCache using the cache
        configuration. Also, load past (start_time - expiry_time)
        if cache_load_past is configured.
        :return:
        """
        self.request_set_cache = RequestSetSparkCache(
            cache_config=self.cache_config,
            table_name=RequestSet.__tablename__,
            columns_to_keep=(
                'target',
                'ip',
                F.col('start').alias('first_ever_request'),
                F.col('subset_count').alias('old_subset_count'),
                F.col('features').alias('old_features'),
                F.col('num_requests').alias('old_num_requests'),
                F.col('updated_at')
            ),
            expire_if_longer_than=self.engine_conf.cache_expire_time,
            path=os.path.join(self.engine_conf.storage_path, FOLDER_CACHE)
        )
        if self.engine_conf.cache_load_past:
            self.request_set_cache = self.request_set_cache.load(
                update_date=(
                    self.start_time - datetime.timedelta(
                        seconds=self.engine_conf.cache_expire_time
                    )
                ).replace(tzinfo=tzutc()),
                extra_filters=(
                    F.col('time_bucket') == self.time_bucket.sec
                )  # todo: &amp; (F.col("id_runtime") == self.runtime.id)?
            )
        else:
            self.request_set_cache.load_empty(get_cache_schema())

        self.logger.info(f'In cache: {self.request_set_cache.count()}')

        return self.request_set_cache

    def handle_missing_columns(self):
        """
        Check for missing columns and if any use the data parser to add them
        and fill them with defaults, if specified in the schema.
        :return:
        """
        missing = self.data_parser.check_for_missing_columns(self.logs_df)
        if missing:
            self.logs_df = self.data_parser.add_missing_columns(
                self.logs_df, missing
            )

    def rename_columns(self):
        """
        Some column names may cause issues with spark, e.g. `geo.ip.lat`, so
        the features that use those can declare in `columns_renamed` that those
        columns should be renamed to something else, e.g. `geo_ip_lat`
        :return:
        """
        for k, v in self.feature_manager.column_renamings:
            self.logs_df = self.logs_df.withColumnRenamed(k, v)

    def filter_columns(self):
        """
        Logs df may have columns that are not necessary for the analysis,
        filter them out to reduce the memory footprint.
        The absolutely essential columns are the group by columns and the
        timestamp column, or else the rest of the process will fail.
        And of course the columns the features need, the active columns.
        :return:None
        """

        where = self.drop_if_missing_filter
        self.logs_df = self.logs_df.select(*self.columns_to_filter_by)
        if where is not None:
            self.logs_df = self.logs_df.where(where)

        # todo: metric for dropped logs
        print(f'{self.logs_df.count()}')

    def handle_missing_values(self):
        self.logs_df = self.data_parser.fill_missing_values(self.logs_df)

    def normalize_host_names(self):
        """
        From www.somedomain.tld keep somedomain
        # todo: improve this and remove udf
        # todo: keep original target in a separate field in db
        :return:
        """
        from baskerville.spark.udfs import udf_normalize_host_name

        self.logs_df = self.logs_df.withColumn(
            'client_request_host',
            udf_normalize_host_name(
                F.col('client_request_host').cast(T.StringType())
            )
        )

    def add_calc_columns(self):
        """
        Each feature needs different calculations in order to be able to
        compute the feature value. Go through the features and apply the
        calculations. Each calculation can occur only once, calculations
        with the same name will be ignored.
        :return:
        """

        self.logs_df = self.logs_df.withColumn(
            '@timestamp', F.col('@timestamp').cast('timestamp')
        )

        for k, v in self.feature_manager.pre_group_by_calculations.items():
            self.logs_df = self.logs_df.withColumn(
                k, v
            )

        for f in self.feature_manager.active_features:
            self.logs_df = f.misc_compute(self.logs_df)

    def add_cache_columns(self):
        """
        Add columns from the cache to facilitate the
        feature extraction, prediction, and save processes
        :return:
        """
        self.logs_df = self.logs_df.alias('logs_df')
        self.filter_cache()
        self.logs_df = self.request_set_cache.update_df(
            self.logs_df, select_cols=self.cache_columns
        )
        self.logger.debug(
            f'****** &gt; # of rows in cache: {self.request_set_cache.count()}')

    def add_post_groupby_columns(self):
        """
        Add extra columns after the grouping of the logs to facilitate the
        feature extraction, prediction, and save processes
        Extra columns:
        * general:
        ----------
        - ip
        - target
        - id_runtime
        - time_bucket
        - start
        - subset_count

        * cache columns:
        ----------------
        - 'id',
        - 'first_ever_request',
        - 'old_subset_count',
        - 'old_features',
        - 'old_num_requests'

        * model related:
        ----------------
        - model_version
        - classifier
        - scaler
        - model_features

        :return: None
        """
        # todo: shouldn't this be a renaming?
        self.logs_df = self.logs_df.withColumn('ip', F.col('client_ip'))
        self.logs_df = self.logs_df.withColumn(
            'target', F.col('client_request_host')
        )
        self.add_cache_columns()

        for k, v in self.get_post_group_by_calculations().items():
            self.logs_df = self.logs_df.withColumn(k, v)

        self.logs_df = self.logs_df.drop('old_subset_count')

    def feature_extraction(self):
        """
        For each feature compute the feature value and add it as a column in
        the dataframe
        :return: None
        """

        for feature in self.feature_manager.active_features:
            self.logs_df = feature.compute(self.logs_df)

        self.logger.info(
            f'Number of logs after feature extraction {self.logs_df.count()}'
        )
        # self.logs_df = self.logs_df.cache()

    def remove_feature_columns(self):
        self.logs_df = self.logs_df.drop(
            *self.feature_manager.active_feature_names
        )

    def feature_update(self):
        """
        Update current batch's features with past features - if any - using
        the request set cache.
        :return:
        """
        # convert current features to dict since the already saved request_sets
        # have the features as json
        self.features_to_dict('features')
        self.features_to_dict('old_features')
        self.logs_df.persist(self.spark_conf.storage_level)

        for f in self.feature_manager.updateable_active_features:
            self.logs_df = f.update(self.logs_df).cache()

        self.logs_df = self.logs_df.withColumn('features', F.create_map(
            *list(
                itertools.chain(
                    *[
                        (F.lit(f.feature_name),
                         F.col(f.updated_feature_col_name))
                        for f in
                        self.feature_manager.updateable_active_features
                    ]
                )
            )
        ))

        # older way with a udf:
        # self.logs_df = self.logs_df.withColumn(
        #     'features',
        #     udf_update_features(
        #         F.lit(cPickle.dumps([  # #WST
        #             StrippedFeature(f.feature_name, f.update_row)
        #             for f in self.feature_manager.active_features
        #         ]
        #         )),
        #         'features',
        #         'old_features',
        #         'subset_count',
        #         'start',
        #         'last_request'
        #     )
        # ).persist(self.spark_conf.storage_level)
        self.logs_df = self.logs_df.drop('old_features')

        self.logs_df = self.logs_df.withColumn(
            'subset_count',
            F.col('subset_count') + F.lit(1)
        )

        self.logs_df = self.logs_df.withColumn(
            'num_requests',
            F.when(
                F.col('old_num_requests') &gt; 0,
                F.col('old_num_requests') + F.col('num_requests')
            ).otherwise(F.col('num_requests'))
        )
        self.logs_df = self.logs_df.drop('old_num_requests')
        diff = (F.unix_timestamp('last_request', format="YYYY-MM-DD %H:%M:%S")
                - F.unix_timestamp(
                    'start', format="YYYY-MM-DD %H:%M:%S")
                ).cast('float')
        self.logs_df = self.logs_df.withColumn('total_seconds', diff)
        self.logs_df = self.logs_df.drop(*self.cols_to_drop)

    def cross_reference(self):
        """
        Look up IPs in attributes table, and label as malicious (-1) if listed
        there.
        :return:
        """
        from pyspark.sql import functions as F
        from baskerville.spark.udfs import udf_cross_reference_misp

        self.logs_df = self.logs_df.withColumn(
            'cross_reference',
            udf_cross_reference_misp('ip',
                                     F.lit(json.dumps(self.db_conf.__dict__)))
        )
        self.logs_df = self.logs_df.withColumn(
            'label',
            F.when(F.col('cross_reference.label') != 0,
                   F.col('cross_reference.label')).otherwise(None)
        )
        self.logs_df = self.logs_df.withColumn(
            'id_attribute',
            F.when(F.col('cross_reference.id_attribute') != 0,
                   F.col('cross_reference.id_attribute')).otherwise(None)
        )

    def predict(self):
        """
        Predict on the request_sets. Prediction on request_sets
        requires feature averaging where there is an existing request_set.
`        :return: None
        """
        if self.model:
            self.logs_df = self.model.predict(self.logs_df)
        else:
            self.logs_df = set_unknown_prediction(self.logs_df).withColumn(
                'prediction', F.col('prediction').cast(T.IntegerType())
            ).withColumn(
                'score', F.col('score').cast(T.FloatType())
            ).withColumn(
                'threshold', F.col('threshold').cast(T.FloatType()))

    def save_df_to_table(
            self, df, table_name, json_cols=('features',), mode='append'
    ):
        """
        Save the dataframe to the database. Jsonify any columns that need to
        be
        :param pyspark.Dataframe df: the dataframe to save
        :param str table_name: where to save the dataframe
        :param iterable json_cols: the name of the columns that need to be
        jsonified, e.g. `features`
        :param str mode: save mode, 'append', 'overwrite' etc, see spark save
        options
        :return: None
        """
        # todo: mostly duplicated code, refactor
        self.db_conf.conn_str = self.db_url
        save_df_to_table(
            df,
            table_name,
            self.db_conf.__dict__,
            json_cols=json_cols,
            storage_level=self.spark_conf.storage_level,
            mode=mode,
            db_driver=self.spark_conf.db_driver
        )

    def features_to_dict(self, column):
        """
        Convert the features to dictionary
        :param string column: the name of the column to use as output
        :return: None
        """
        from pyspark.sql import functions as F

        self.logs_df = self.logs_df.withColumn(
            column,
            F.create_map(
                *list(
                    itertools.chain(
                        *[
                            (F.lit(f.feature_name), F.col(f.feature_name))
                            for f in self.feature_manager.active_features
                        ]
                    )
                ))
        )</code></pre>
       </details>
       <h3>
        Ancestors
       </h3>
       <ul class="hlist">
        <li>
         <a href="base.html#baskerville.models.base.PipelineBase" title="baskerville.models.base.PipelineBase">
          PipelineBase
         </a>
        </li>
       </ul>
       <h3>
        Subclasses
       </h3>
       <ul class="hlist">
        <li>
         <a href="pipelines.html#baskerville.models.pipelines.ElasticsearchPipeline" title="baskerville.models.pipelines.ElasticsearchPipeline">
          ElasticsearchPipeline
         </a>
        </li>
        <li>
         <a href="pipelines.html#baskerville.models.pipelines.RawLogPipeline" title="baskerville.models.pipelines.RawLogPipeline">
          RawLogPipeline
         </a>
        </li>
        <li>
         <a href="pipelines.html#baskerville.models.pipelines.KafkaPipeline" title="baskerville.models.pipelines.KafkaPipeline">
          KafkaPipeline
         </a>
        </li>
        <li>
         <a href="pipelines.html#baskerville.models.pipelines.SparkStructuredStreamingRealTimePipeline" title="baskerville.models.pipelines.SparkStructuredStreamingRealTimePipeline">
          SparkStructuredStreamingRealTimePipeline
         </a>
        </li>
       </ul>
       <h3>
        Methods
       </h3>
       <dl>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            add_cache_columns
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Add columns from the cache to facilitate the
feature extraction, prediction, and save processes
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def add_cache_columns(self):
    """
    Add columns from the cache to facilitate the
    feature extraction, prediction, and save processes
    :return:
    """
    self.logs_df = self.logs_df.alias('logs_df')
    self.filter_cache()
    self.logs_df = self.request_set_cache.update_df(
        self.logs_df, select_cols=self.cache_columns
    )
    self.logger.debug(
        f'****** &gt; # of rows in cache: {self.request_set_cache.count()}')</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            add_calc_columns
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Each feature needs different calculations in order to be able to
compute the feature value. Go through the features and apply the
calculations. Each calculation can occur only once, calculations
with the same name will be ignored.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def add_calc_columns(self):
    """
    Each feature needs different calculations in order to be able to
    compute the feature value. Go through the features and apply the
    calculations. Each calculation can occur only once, calculations
    with the same name will be ignored.
    :return:
    """

    self.logs_df = self.logs_df.withColumn(
        '@timestamp', F.col('@timestamp').cast('timestamp')
    )

    for k, v in self.feature_manager.pre_group_by_calculations.items():
        self.logs_df = self.logs_df.withColumn(
            k, v
        )

    for f in self.feature_manager.active_features:
        self.logs_df = f.misc_compute(self.logs_df)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            add_post_groupby_columns
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Add extra columns after the grouping of the logs to facilitate the
feature extraction, prediction, and save processes
Extra columns:
* general:
          </p>
          <hr/>
          <ul>
           <li>
            ip
           </li>
           <li>
            target
           </li>
           <li>
            id_runtime
           </li>
           <li>
            time_bucket
           </li>
           <li>
            start
           </li>
           <li>
            subset_count
           </li>
          </ul>
          <h2 id="cache-columns">
           * cache columns:
          </h2>
          <ul>
           <li>
            'id',
           </li>
           <li>
            'first_ever_request',
           </li>
           <li>
            'old_subset_count',
           </li>
           <li>
            'old_features',
           </li>
           <li>
            'old_num_requests'
           </li>
          </ul>
          <h2 id="model-related">
           * model related:
          </h2>
          <ul>
           <li>
            model_version
           </li>
           <li>
            classifier
           </li>
           <li>
            scaler
           </li>
           <li>
            model_features
           </li>
          </ul>
          <p>
           :return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def add_post_groupby_columns(self):
    """
    Add extra columns after the grouping of the logs to facilitate the
    feature extraction, prediction, and save processes
    Extra columns:
    * general:
    ----------
    - ip
    - target
    - id_runtime
    - time_bucket
    - start
    - subset_count

    * cache columns:
    ----------------
    - 'id',
    - 'first_ever_request',
    - 'old_subset_count',
    - 'old_features',
    - 'old_num_requests'

    * model related:
    ----------------
    - model_version
    - classifier
    - scaler
    - model_features

    :return: None
    """
    # todo: shouldn't this be a renaming?
    self.logs_df = self.logs_df.withColumn('ip', F.col('client_ip'))
    self.logs_df = self.logs_df.withColumn(
        'target', F.col('client_request_host')
    )
    self.add_cache_columns()

    for k, v in self.get_post_group_by_calculations().items():
        self.logs_df = self.logs_df.withColumn(k, v)

    self.logs_df = self.logs_df.drop('old_subset_count')</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.create_runtime">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            create_runtime
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Create a Runtime in the Baskerville database.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def create_runtime(self):
    """
    Create a Runtime in the Baskerville database.
    :return:
    """
    raise NotImplementedError(
        'SparkPipelineBase does not have an implementation '
        'for _create_runtime.'
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.cross_reference">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            cross_reference
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Look up IPs in attributes table, and label as malicious (-1) if listed
there.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def cross_reference(self):
    """
    Look up IPs in attributes table, and label as malicious (-1) if listed
    there.
    :return:
    """
    from pyspark.sql import functions as F
    from baskerville.spark.udfs import udf_cross_reference_misp

    self.logs_df = self.logs_df.withColumn(
        'cross_reference',
        udf_cross_reference_misp('ip',
                                 F.lit(json.dumps(self.db_conf.__dict__)))
    )
    self.logs_df = self.logs_df.withColumn(
        'label',
        F.when(F.col('cross_reference.label') != 0,
               F.col('cross_reference.label')).otherwise(None)
    )
    self.logs_df = self.logs_df.withColumn(
        'id_attribute',
        F.when(F.col('cross_reference.id_attribute') != 0,
               F.col('cross_reference.id_attribute')).otherwise(None)
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.feature_calculation">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            feature_calculation
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Add calculation cols, extract features, and update.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def feature_calculation(self):
    """
    Add calculation cols, extract features, and update.
    :return:
    """
    self.add_post_groupby_columns()
    self.feature_extraction()
    self.feature_update()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.feature_extraction">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            feature_extraction
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           For each feature compute the feature value and add it as a column in
the dataframe
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def feature_extraction(self):
    """
    For each feature compute the feature value and add it as a column in
    the dataframe
    :return: None
    """

    for feature in self.feature_manager.active_features:
        self.logs_df = feature.compute(self.logs_df)

    self.logger.info(
        f'Number of logs after feature extraction {self.logs_df.count()}'
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.feature_update">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            feature_update
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Update current batch's features with past features - if any - using
the request set cache.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def feature_update(self):
    """
    Update current batch's features with past features - if any - using
    the request set cache.
    :return:
    """
    # convert current features to dict since the already saved request_sets
    # have the features as json
    self.features_to_dict('features')
    self.features_to_dict('old_features')
    self.logs_df.persist(self.spark_conf.storage_level)

    for f in self.feature_manager.updateable_active_features:
        self.logs_df = f.update(self.logs_df).cache()

    self.logs_df = self.logs_df.withColumn('features', F.create_map(
        *list(
            itertools.chain(
                *[
                    (F.lit(f.feature_name),
                     F.col(f.updated_feature_col_name))
                    for f in
                    self.feature_manager.updateable_active_features
                ]
            )
        )
    ))

    # older way with a udf:
    # self.logs_df = self.logs_df.withColumn(
    #     'features',
    #     udf_update_features(
    #         F.lit(cPickle.dumps([  # #WST
    #             StrippedFeature(f.feature_name, f.update_row)
    #             for f in self.feature_manager.active_features
    #         ]
    #         )),
    #         'features',
    #         'old_features',
    #         'subset_count',
    #         'start',
    #         'last_request'
    #     )
    # ).persist(self.spark_conf.storage_level)
    self.logs_df = self.logs_df.drop('old_features')

    self.logs_df = self.logs_df.withColumn(
        'subset_count',
        F.col('subset_count') + F.lit(1)
    )

    self.logs_df = self.logs_df.withColumn(
        'num_requests',
        F.when(
            F.col('old_num_requests') &gt; 0,
            F.col('old_num_requests') + F.col('num_requests')
        ).otherwise(F.col('num_requests'))
    )
    self.logs_df = self.logs_df.drop('old_num_requests')
    diff = (F.unix_timestamp('last_request', format="YYYY-MM-DD %H:%M:%S")
            - F.unix_timestamp(
                'start', format="YYYY-MM-DD %H:%M:%S")
            ).cast('float')
    self.logs_df = self.logs_df.withColumn('total_seconds', diff)
    self.logs_df = self.logs_df.drop(*self.cols_to_drop)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.features_to_dict">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            features_to_dict
           </span>
          </span>
          (
          <span>
           self, column)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Convert the features to dictionary
:param string column: the name of the column to use as output
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def features_to_dict(self, column):
    """
    Convert the features to dictionary
    :param string column: the name of the column to use as output
    :return: None
    """
    from pyspark.sql import functions as F

    self.logs_df = self.logs_df.withColumn(
        column,
        F.create_map(
            *list(
                itertools.chain(
                    *[
                        (F.lit(f.feature_name), F.col(f.feature_name))
                        for f in self.feature_manager.active_features
                    ]
                )
            ))
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.filter_cache">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            filter_cache
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Use the current logs to find the past request sets - if any - in the
request set cache
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def filter_cache(self):
    """
    Use the current logs to find the past request sets - if any - in the
    request set cache
    :return:
    """
    df = self.logs_df.select(
        F.col('client_request_host').alias('target'),
        F.col('client_ip').alias('ip'),
    ).distinct().alias('a').persist(self.spark_conf.storage_level)

    self.request_set_cache.filter_by(df)

    df.unpersist()
    del df</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.filter_columns">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            filter_columns
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Logs df may have columns that are not necessary for the analysis,
filter them out to reduce the memory footprint.
The absolutely essential columns are the group by columns and the
timestamp column, or else the rest of the process will fail.
And of course the columns the features need, the active columns.
:return:None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def filter_columns(self):
    """
    Logs df may have columns that are not necessary for the analysis,
    filter them out to reduce the memory footprint.
    The absolutely essential columns are the group by columns and the
    timestamp column, or else the rest of the process will fail.
    And of course the columns the features need, the active columns.
    :return:None
    """

    where = self.drop_if_missing_filter
    self.logs_df = self.logs_df.select(*self.columns_to_filter_by)
    if where is not None:
        self.logs_df = self.logs_df.where(where)

    # todo: metric for dropped logs
    print(f'{self.logs_df.count()}')</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.finish_up">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            finish_up
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Try to gracefully stop by committing to database, emptying the cache,
unpersist everything, disconnecting from db and clearing spark's cache
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def finish_up(self):
    """
    Try to gracefully stop by committing to database, emptying the cache,
    unpersist everything, disconnecting from db and clearing spark's cache
    :return: None
    """
    if len(self.remaining_steps) == 0:
        request_set_count = self.tools.session.query(RequestSet).filter(
            RequestSet.id_runtime == self.runtime.id).count()
        self.runtime.processed = True
        self.runtime.n_request_sets = request_set_count
        self.tools.session.commit()
        self.logger.debug('finished updating runtime')
    try:
        self.request_set_cache.empty()
    except AttributeError:
        pass

    if hasattr(self, 'logs_df') and self.logs_df and isinstance(
            self.logs_df, DataFrame):
        self.logs_df.unpersist(blocking=True)
        self.logs_df = None

    if self.tools and self.tools.session:
        self.tools.session.commit()
        self.tools.disconnect_from_db()
    if self.spark:
        try:
            if self.spark_conf.spark_python_profile:
                self.spark.sparkContext.show_profiles()
            reset_spark_storage()
        except AttributeError:
            self.logger.debug('clearCache attr error')
            pass</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.get_challenges">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            get_challenges
           </span>
          </span>
          (
          <span>
           self, df, challenge_threshold)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def get_challenges(self, df, challenge_threshold):
    def challenge_decision(num_normals, num_anomalies, threshold):
        if num_anomalies &gt;= threshold * (num_anomalies + num_normals):
            return 1
        return 0

    challenge_decision_udf = udf(challenge_decision, IntegerType())

    df = df.select(['target', 'prediction'])\
        .groupBy(['target', 'prediction'])\
        .count()\
        .groupBy('target')\
        .pivot('prediction').agg(F.first('count'))\
        .withColumn('challenge', challenge_decision_udf(F.col('0'), F.col('1'), F.lit(challenge_threshold)))\
        .select(['target', 'challenge'])
    return []</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            get_columns_to_filter_by
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Gathers all the columns that need to be present in the dataframe
for the processing to complete.
group_by_cols: the columns to group data on
active_columns: the columns that the active features have declared as
necessary
timestamp_column: the time column - all logs need to have a time column
:return: a set of the column names that need to be present in the
dataframe
:rtype: set[str]
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def get_columns_to_filter_by(self):
    """
    Gathers all the columns that need to be present in the dataframe
    for the processing to complete.
    group_by_cols: the columns to group data on
    active_columns: the columns that the active features have declared as
    necessary
    timestamp_column: the time column - all logs need to have a time column
    :return: a set of the column names that need to be present in the
    dataframe
    :rtype: set[str]
    """
    cols = self.group_by_cols + self.feature_manager.active_columns
    cols.append(self.engine_conf.data_config.timestamp_column)
    return set(cols)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.get_data">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            get_data
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Get dataframe of log data
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def get_data(self):
    """
    Get dataframe of log data
    :return:
    """
    raise NotImplementedError(
        'SparkPipelineBase does not have an implementation '
        'for _get_data.'
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            get_group_by_aggs
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Gathers all the group by arguments:
basic_aggs:
- first_request
- last_request
- num_requests
column_aggs: the columns the features need for computation are gathered
as lists
feature_aggs: the columns the features need for computation
Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
The basic aggs have a priority over the feature and column aggs.
The feature aggs have a priority over the column aggs (if a feature
has explicitly asked for a computation for a specific column it relies
upon, then the computation will be stored instead of the column
aggregation as list)
          </p>
          <p>
           :return: a dictionary with the name of the group by aggregation columns
as keys and the respective Column aggregation as values
:rtype: dict[string, pyspark.Column]
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def get_group_by_aggs(self):
    """
    Gathers all the group by arguments:
    basic_aggs:
        - first_request
        - last_request
        - num_requests
    column_aggs: the columns the features need for computation are gathered
     as lists
    feature_aggs: the columns the features need for computation
    Priority: basic_aggs &gt; feature_aggs &gt; column_aggs
    The basic aggs have a priority over the feature and column aggs.
    The feature aggs have a priority over the column aggs (if a feature
    has explicitly asked for a computation for a specific column it relies
    upon, then the computation will be stored instead of the column
    aggregation as list)

    :return: a dictionary with the name of the group by aggregation columns
    as keys and the respective Column aggregation as values
    :rtype: dict[string, pyspark.Column]
    """
    basic_aggs = {
        'first_request': F.min(F.col('@timestamp')).alias('first_request'),
        'last_request': F.max(F.col('@timestamp')).alias('last_request'),
        'num_requests': F.count(F.col('@timestamp')).alias('num_requests')
    }

    column_aggs = {
        c: F.collect_list(F.col(c)).alias(c)
        for c in self.feature_manager.active_columns
    }

    feature_aggs = self.feature_manager.get_feature_group_by_aggs()

    basic_aggs.update(
        {k: v for k, v in feature_aggs.items() if k not in basic_aggs}
    )
    basic_aggs.update(
        {k: v for k, v in column_aggs.items() if k not in basic_aggs}
    )

    return basic_aggs</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            get_post_group_by_calculations
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Gathers the columns and computations to be performed after the grouping
of the data (logs_df)
Basic post group by columns:
-
           <code>
            id_runtime
           </code>
           -
           <code>
            time_bucket
           </code>
           -
           <code>
            start
           </code>
           -
           <code>
            stop
           </code>
           -
           <code>
            subset_count
           </code>
          </p>
          <p>
           if there is an ML Model defined:
-
           <code>
            model_version
           </code>
           -
           <code>
            classifier
           </code>
           -
           <code>
            scaler
           </code>
           -
           <code>
            model_features
           </code>
          </p>
          <p>
           Each feature can also define post group by calculations using the
post_group_by_calcs dict.
          </p>
          <p>
           :return: A dictionary with the name of the result columns as keys and
their respective computations as values
:rtype: dict[string, pyspark.Column]
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def get_post_group_by_calculations(self):
    """
    Gathers the columns and computations to be performed after the grouping
    of the data (logs_df)
    Basic post group by columns:
    - `id_runtime`
    - `time_bucket`
    - `start`
    - `stop`
    - `subset_count`

    if there is an ML Model defined:
    - `model_version`
    - `classifier`
    - `scaler`
    - `model_features`

    Each feature can also define post group by calculations using the
    post_group_by_calcs dict.

    :return: A dictionary with the name of the result columns as keys and
    their respective computations as values
    :rtype: dict[string, pyspark.Column]
    """
    if self.post_group_by_aggs:
        return self.post_group_by_aggs

    post_group_by_columns = {
        'id_runtime': F.lit(self.runtime.id),
        'time_bucket': F.lit(self.time_bucket.sec),
        'start': F.when(
            F.col('first_ever_request').isNotNull(),
            F.col('first_ever_request')
        ).otherwise(F.col('first_request')),
        'stop': F.col('last_request'),
        'subset_count': F.when(
            F.col('old_subset_count').isNotNull(),
            F.col('old_subset_count')
        ).otherwise(F.lit(0))
    }

    if self.model_index:
        post_group_by_columns['model_version'] = F.lit(
            self.model_index.id
        )

    # todo: what if a feature defines a column name that already exists?
    # e.g. like `subset_count`
    post_group_by_columns.update(
        self.feature_manager.post_group_by_calculations
    )

    return post_group_by_columns</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.get_window">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            get_window
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def get_window(self):

    from pyspark.sql import functions as F

    df = self.logs_df.withColumn('timestamp',
                                 F.col('@timestamp').cast('timestamp'))
    df = df.sort('timestamp')
    current_window_start = df.agg({"timestamp": "min"}).collect()[0][0]
    stop = df.agg({"timestamp": "max"}).collect()[0][0]
    window_df = None
    current_end = current_window_start + self.time_bucket.td

    while True:
        if window_df:
            window_df.unpersist(blocking=True)
            del window_df
        filter_ = (
            (F.col('timestamp') &gt;= current_window_start) &amp;
            (F.col('timestamp') &lt; current_end)
        )
        window_df = df.where(filter_).persist(
            self.spark_conf.storage_level
        )
        if not window_df.rdd.isEmpty():
            print(f'# Request sets = {window_df.count()}')
            yield window_df
        else:
            self.logger.info(f'Empty window df for {str(filter_._jc)}')
        current_window_start = current_window_start + self.time_bucket.td
        current_end = current_window_start + self.time_bucket.td
        if current_window_start &gt;= stop:
            window_df.unpersist(blocking=True)
            del window_df
            break</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.group_by">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            group_by
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Group the logs df by the given group-by columns (normally IP, host).
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def group_by(self):
    """
    Group the logs df by the given group-by columns (normally IP, host).
    :return: None
    """
    self.logs_df = self.logs_df.groupBy(
        *self.group_by_cols
    ).agg(
        *self.group_by_aggs.values()
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            handle_missing_columns
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Check for missing columns and if any use the data parser to add them
and fill them with defaults, if specified in the schema.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def handle_missing_columns(self):
    """
    Check for missing columns and if any use the data parser to add them
    and fill them with defaults, if specified in the schema.
    :return:
    """
    missing = self.data_parser.check_for_missing_columns(self.logs_df)
    if missing:
        self.logs_df = self.data_parser.add_missing_columns(
            self.logs_df, missing
        )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.handle_missing_values">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            handle_missing_values
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def handle_missing_values(self):
    self.logs_df = self.data_parser.fill_missing_values(self.logs_df)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.initialize">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            initialize
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Set the basics:
- Connect to the database
- Initialize spark session
- Get active model and scaler and set them to broadcast variables
- Get active features with their active columns, update columns etc and
set the relevant broadcast variables
- Set the _can_predict flag
- Instantiate the accumulators (for metrics)
- Instantiate request set cache.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def initialize(self):
    """
    Set the basics:
    - Connect to the database
    - Initialize spark session
    - Get active model and scaler and set them to broadcast variables
    - Get active features with their active columns, update columns etc and
    set the relevant broadcast variables
    - Set the _can_predict flag
    - Instantiate the accumulators (for metrics)
    - Instantiate request set cache.
    :return:
    """

    # initialize db access tools
    self.tools = BaskervilleDBTools(self.db_conf)
    self.tools.connect_to_db()

    # initialize spark session
    self.spark = self.instantiate_spark_session()
    self.feature_manager.initialize()
    self.drop_if_missing_filter = self.data_parser.drop_if_missing_filter()

    # set up cache
    self.request_set_cache = self.set_up_request_set_cache()

    # gather calculations
    self.group_by_aggs = self.get_group_by_aggs()
    self.columns_to_filter_by = self.get_columns_to_filter_by()
    self.cols_to_drop = set(
        self.feature_manager.active_feature_names +
        self.feature_manager.active_columns +
        list(self.group_by_aggs.keys()) +
        self.feature_manager.update_feature_cols
    ).difference(RequestSet.columns)

    if self.engine_conf.model_id:
        self.model_index = self.tools.get_ml_model_from_db(
            self.engine_conf.model_id)
        self.model = instantiate_from_str(self.model_index.algorithm)
        self.model.load(bytes.decode(
            self.model_index.classifier, 'utf8'), self.spark)
    else:
        self.model = None

    self._is_initialized = True</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            instantiate_spark_session
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <h1 id="todo">
           todo
          </h1>
          <p>
           :return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def instantiate_spark_session(self):
    """
    #todo
    :return:
    """
    return get_or_create_spark_session(self.spark_conf)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.label_or_predict">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            label_or_predict
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Apply label from MISP or predict label.
          </p>
          <h1 id="todo-use-separate-steps-for-this">
           todo: use separate steps for this
          </h1>
          <p>
           :return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def label_or_predict(self):
    """
    Apply label from MISP or predict label.
    #todo: use separate steps for this
    :return:
    """

    from pyspark.sql import functions as F
    from pyspark.sql.types import IntegerType

    if self.engine_conf.cross_reference:
        self.cross_reference()
    else:
        self.logs_df = self.logs_df.withColumn(
            'label', F.lit(None).cast(IntegerType()))
    self.predict()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.load_test">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            load_test
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           If the user has set the load_test configuration, then multiply the
traffic by
           <code>
            self.engine_conf.load_test
           </code>
           times to do load testing.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def load_test(self):
    """
    If the user has set the load_test configuration, then multiply the
    traffic by `self.engine_conf.load_test` times to do load testing.
    :return:
    """
    if self.engine_conf.load_test:
        df = self.logs_df.persist(self.spark_conf.storage_level)

        for i in range(self.engine_conf.load_test - 1):
            df = df.withColumn(
                'client_ip', F.round(F.rand(42)).cast('string')
            )
            self.logs_df = self.logs_df.union(df).persist(
                self.spark_conf.storage_level
            )

        df.unpersist()
        del df
        self.logger.info(
            f'---- Count after multiplication: {self.logs_df.count()}'
        )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            normalize_host_names
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           From www.somedomain.tld keep somedomain
          </p>
          <h1 id="todo-improve-this-and-remove-udf">
           todo: improve this and remove udf
          </h1>
          <h1 id="todo-keep-original-target-in-a-separate-field-in-db">
           todo: keep original target in a separate field in db
          </h1>
          <p>
           :return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def normalize_host_names(self):
    """
    From www.somedomain.tld keep somedomain
    # todo: improve this and remove udf
    # todo: keep original target in a separate field in db
    :return:
    """
    from baskerville.spark.udfs import udf_normalize_host_name

    self.logs_df = self.logs_df.withColumn(
        'client_request_host',
        udf_normalize_host_name(
            F.col('client_request_host').cast(T.StringType())
        )
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.predict">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            predict
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Predict on the request_sets. Prediction on request_sets
requires feature averaging where there is an existing request_set.
`
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">    def predict(self):
        """
        Predict on the request_sets. Prediction on request_sets
        requires feature averaging where there is an existing request_set.
`        :return: None
        """
        if self.model:
            self.logs_df = self.model.predict(self.logs_df)
        else:
            self.logs_df = set_unknown_prediction(self.logs_df).withColumn(
                'prediction', F.col('prediction').cast(T.IntegerType())
            ).withColumn(
                'score', F.col('score').cast(T.FloatType())
            ).withColumn(
                'threshold', F.col('threshold').cast(T.FloatType()))</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.preprocessing">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            preprocessing
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Fill missing values, add calculation cols, and filter.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def preprocessing(self):
    """
    Fill missing values, add calculation cols, and filter.
    :return:
    """

    self.handle_missing_columns()
    self.rename_columns()
    self.filter_columns()
    self.handle_missing_values()
    self.normalize_host_names()
    self.add_calc_columns()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.process_data">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            process_data
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Splits the data into time bucket length windows and executes all the steps
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def process_data(self):
    """
    Splits the data into time bucket length windows and executes all the steps
    :return:
    """
    if self.logs_df.count() == 0:
        self.logger.info('No data in to process.')
    else:
        for window_df in self.get_window():
            self.logs_df = window_df
            self.logs_df = self.logs_df.repartition(
                *self.group_by_cols
            ).persist(self.spark_conf.storage_level)
            self.remaining_steps = list(self.step_to_action.keys())

            for step, action in self.step_to_action.items():
                self.logger.info('Starting step {}'.format(step))
                action()
                self.logger.info('Completed step {}'.format(step))
                self.remaining_steps.remove(step)
            self.reset()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.refresh_cache">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            refresh_cache
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Update the cache with the current batch of logs in logs_df and clean up
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def refresh_cache(self):
    """
    Update the cache with the current batch of logs in logs_df and clean up
    :return:
    """
    self.request_set_cache.update_self(self.logs_df)
    self.logs_df.unpersist()
    self.logs_df = None</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.remove_feature_columns">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            remove_feature_columns
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def remove_feature_columns(self):
    self.logs_df = self.logs_df.drop(
        *self.feature_manager.active_feature_names
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.rename_columns">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            rename_columns
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Some column names may cause issues with spark, e.g.
           <code>
            geo.ip.lat
           </code>
           , so
the features that use those can declare in
           <code>
            columns_renamed
           </code>
           that those
columns should be renamed to something else, e.g.
           <code>
            geo_ip_lat
           </code>
           :return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def rename_columns(self):
    """
    Some column names may cause issues with spark, e.g. `geo.ip.lat`, so
    the features that use those can declare in `columns_renamed` that those
    columns should be renamed to something else, e.g. `geo_ip_lat`
    :return:
    """
    for k, v in self.feature_manager.column_renamings:
        self.logs_df = self.logs_df.withColumnRenamed(k, v)</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.reset">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            reset
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Unpersist rdds and dataframes and call GC - see broadcast memory
release issue
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def reset(self):
    """
    Unpersist rdds and dataframes and call GC - see broadcast memory
    release issue
    :return:
    """
    import gc

    reset_spark_storage()
    gc.collect()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.run">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            run
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Runs the configured steps.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def run(self):
    """
    Runs the configured steps.
    :return:
    """
    self.logger.info(
        f'Spark UI accessible at:{self.spark.sparkContext.uiWebUrl}'
    )
    self.create_runtime()
    self.get_data()
    self.process_data()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.save">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            save
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Update dataframe, save to database, and update cache.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def save(self):
    """
    Update dataframe, save to database, and update cache.
    :return:
    """
    request_set_columns = RequestSet.columns[:]
    not_common = {
        'prediction', 'model_version', 'label', 'id_attribute',
        'updated_at'
    }.difference(self.logs_df.columns)

    for c in not_common:
        request_set_columns.remove(c)

    # filter the logs df with the request_set columns
    self.logs_df = self.logs_df.select(request_set_columns)

    # save request_sets
    self.logger.debug('Saving request_sets')
    self.save_df_to_table(
        self.logs_df.select(request_set_columns),
        RequestSet.__tablename__
    )
    self.refresh_cache()</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.save_challenges_to_db">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            save_challenges_to_db
           </span>
          </span>
          (
          <span>
           self, challenges)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def save_challenges_to_db(self, challenges):
    pass</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            save_df_to_table
           </span>
          </span>
          (
          <span>
           self, df, table_name, json_cols=('features',), mode='append')
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Save the dataframe to the database. Jsonify any columns that need to
be
:param pyspark.Dataframe df: the dataframe to save
:param str table_name: where to save the dataframe
:param iterable json_cols: the name of the columns that need to be
jsonified, e.g.
           <code>
            features
           </code>
           :param str mode: save mode, 'append', 'overwrite' etc, see spark save
options
:return: None
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def save_df_to_table(
        self, df, table_name, json_cols=('features',), mode='append'
):
    """
    Save the dataframe to the database. Jsonify any columns that need to
    be
    :param pyspark.Dataframe df: the dataframe to save
    :param str table_name: where to save the dataframe
    :param iterable json_cols: the name of the columns that need to be
    jsonified, e.g. `features`
    :param str mode: save mode, 'append', 'overwrite' etc, see spark save
    options
    :return: None
    """
    # todo: mostly duplicated code, refactor
    self.db_conf.conn_str = self.db_url
    save_df_to_table(
        df,
        table_name,
        self.db_conf.__dict__,
        json_cols=json_cols,
        storage_level=self.spark_conf.storage_level,
        mode=mode,
        db_driver=self.spark_conf.db_driver
    )</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.send_challenges">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            send_challenges
           </span>
          </span>
          (
          <span>
           self, challenges)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def send_challenges(self, challenges):
    pass</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            set_up_request_set_cache
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Set up an instance of RequestSetSparkCache using the cache
configuration. Also, load past (start_time - expiry_time)
if cache_load_past is configured.
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def set_up_request_set_cache(self):
    """
    Set up an instance of RequestSetSparkCache using the cache
    configuration. Also, load past (start_time - expiry_time)
    if cache_load_past is configured.
    :return:
    """
    self.request_set_cache = RequestSetSparkCache(
        cache_config=self.cache_config,
        table_name=RequestSet.__tablename__,
        columns_to_keep=(
            'target',
            'ip',
            F.col('start').alias('first_ever_request'),
            F.col('subset_count').alias('old_subset_count'),
            F.col('features').alias('old_features'),
            F.col('num_requests').alias('old_num_requests'),
            F.col('updated_at')
        ),
        expire_if_longer_than=self.engine_conf.cache_expire_time,
        path=os.path.join(self.engine_conf.storage_path, FOLDER_CACHE)
    )
    if self.engine_conf.cache_load_past:
        self.request_set_cache = self.request_set_cache.load(
            update_date=(
                self.start_time - datetime.timedelta(
                    seconds=self.engine_conf.cache_expire_time
                )
            ).replace(tzinfo=tzutc()),
            extra_filters=(
                F.col('time_bucket') == self.time_bucket.sec
            )  # todo: &amp; (F.col("id_runtime") == self.runtime.id)?
        )
    else:
        self.request_set_cache.load_empty(get_cache_schema())

    self.logger.info(f'In cache: {self.request_set_cache.count()}')

    return self.request_set_cache</code></pre>
         </details>
        </dd>
        <dt id="baskerville.models.base_spark.SparkPipelineBase.trigger_challenge">
         <code class="name flex">
          <span>
           def
           <span class="ident">
            trigger_challenge
           </span>
          </span>
          (
          <span>
           self)
          </span>
         </code>
        </dt>
        <dd>
         <section class="desc">
          <p>
           Trigger the challenge per host
:return:
          </p>
         </section>
         <details class="source">
          <summary>
           <span>
            Expand source code
           </span>
          </summary>
          <pre><code class="python">def trigger_challenge(self):
    """
    Trigger the challenge per host
    :return:
    """</code></pre>
         </details>
        </dd>
       </dl>
      </dd>
     </dl>
    </section>
   </article>
   <nav id="sidebar">
    <h1>
     Index
    </h1>
    <div class="toc">
     <ul>
     </ul>
    </div>
    <ul id="index">
     <li>
      <h3>
       Super-module
      </h3>
      <ul>
       <li>
        <code>
         <a href="index.html" title="baskerville.models">
          baskerville.models
         </a>
        </code>
       </li>
      </ul>
     </li>
     <li>
      <h3>
       <a href="#header-classes">
        Classes
       </a>
      </h3>
      <ul>
       <li>
        <h4>
         <code>
          <a href="#baskerville.models.base_spark.SparkPipelineBase" title="baskerville.models.base_spark.SparkPipelineBase">
           SparkPipelineBase
          </a>
         </code>
        </h4>
        <ul class="">
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.add_cache_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_cache_columns">
            add_cache_columns
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.add_calc_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_calc_columns">
            add_calc_columns
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns" title="baskerville.models.base_spark.SparkPipelineBase.add_post_groupby_columns">
            add_post_groupby_columns
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.create_runtime" title="baskerville.models.base_spark.SparkPipelineBase.create_runtime">
            create_runtime
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.cross_reference" title="baskerville.models.base_spark.SparkPipelineBase.cross_reference">
            cross_reference
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.feature_calculation" title="baskerville.models.base_spark.SparkPipelineBase.feature_calculation">
            feature_calculation
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.feature_extraction" title="baskerville.models.base_spark.SparkPipelineBase.feature_extraction">
            feature_extraction
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.feature_update" title="baskerville.models.base_spark.SparkPipelineBase.feature_update">
            feature_update
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.features_to_dict" title="baskerville.models.base_spark.SparkPipelineBase.features_to_dict">
            features_to_dict
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.filter_cache" title="baskerville.models.base_spark.SparkPipelineBase.filter_cache">
            filter_cache
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.filter_columns" title="baskerville.models.base_spark.SparkPipelineBase.filter_columns">
            filter_columns
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.finish_up" title="baskerville.models.base_spark.SparkPipelineBase.finish_up">
            finish_up
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.get_challenges" title="baskerville.models.base_spark.SparkPipelineBase.get_challenges">
            get_challenges
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by" title="baskerville.models.base_spark.SparkPipelineBase.get_columns_to_filter_by">
            get_columns_to_filter_by
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.get_data" title="baskerville.models.base_spark.SparkPipelineBase.get_data">
            get_data
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs" title="baskerville.models.base_spark.SparkPipelineBase.get_group_by_aggs">
            get_group_by_aggs
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations" title="baskerville.models.base_spark.SparkPipelineBase.get_post_group_by_calculations">
            get_post_group_by_calculations
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.get_window" title="baskerville.models.base_spark.SparkPipelineBase.get_window">
            get_window
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.group_by" title="baskerville.models.base_spark.SparkPipelineBase.group_by">
            group_by
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns" title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_columns">
            handle_missing_columns
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.handle_missing_values" title="baskerville.models.base_spark.SparkPipelineBase.handle_missing_values">
            handle_missing_values
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.initialize" title="baskerville.models.base_spark.SparkPipelineBase.initialize">
            initialize
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session" title="baskerville.models.base_spark.SparkPipelineBase.instantiate_spark_session">
            instantiate_spark_session
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.label_or_predict" title="baskerville.models.base_spark.SparkPipelineBase.label_or_predict">
            label_or_predict
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.load_test" title="baskerville.models.base_spark.SparkPipelineBase.load_test">
            load_test
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.normalize_host_names" title="baskerville.models.base_spark.SparkPipelineBase.normalize_host_names">
            normalize_host_names
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.predict" title="baskerville.models.base_spark.SparkPipelineBase.predict">
            predict
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.preprocessing" title="baskerville.models.base_spark.SparkPipelineBase.preprocessing">
            preprocessing
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.process_data" title="baskerville.models.base_spark.SparkPipelineBase.process_data">
            process_data
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.refresh_cache" title="baskerville.models.base_spark.SparkPipelineBase.refresh_cache">
            refresh_cache
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.remove_feature_columns" title="baskerville.models.base_spark.SparkPipelineBase.remove_feature_columns">
            remove_feature_columns
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.rename_columns" title="baskerville.models.base_spark.SparkPipelineBase.rename_columns">
            rename_columns
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.reset" title="baskerville.models.base_spark.SparkPipelineBase.reset">
            reset
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.run" title="baskerville.models.base_spark.SparkPipelineBase.run">
            run
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.save" title="baskerville.models.base_spark.SparkPipelineBase.save">
            save
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.save_challenges_to_db" title="baskerville.models.base_spark.SparkPipelineBase.save_challenges_to_db">
            save_challenges_to_db
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.save_df_to_table" title="baskerville.models.base_spark.SparkPipelineBase.save_df_to_table">
            save_df_to_table
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.send_challenges" title="baskerville.models.base_spark.SparkPipelineBase.send_challenges">
            send_challenges
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache" title="baskerville.models.base_spark.SparkPipelineBase.set_up_request_set_cache">
            set_up_request_set_cache
           </a>
          </code>
         </li>
         <li>
          <code>
           <a href="#baskerville.models.base_spark.SparkPipelineBase.trigger_challenge" title="baskerville.models.base_spark.SparkPipelineBase.trigger_challenge">
            trigger_challenge
           </a>
          </code>
         </li>
        </ul>
       </li>
      </ul>
     </li>
    </ul>
   </nav>
  </main>
  <footer id="footer">
   <p>
    Generated by
    <a href="https://pdoc3.github.io/pdoc">
     <cite>
      pdoc
     </cite>
     0.7.2
    </a>
    .
   </p>
   <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">
    <img alt="Creative Commons Licence" src="https://i.creativecommons.org/l/by/4.0/80x15.png" style="border-width:0"/>
   </a>
   <br/>
   This work is copyright (c) 2020, eQualit.ie inc., and is licensed under a
   <a href="http://creativecommons.org/licenses/by/4.0/" rel="license">
    Creative Commons Attribution 4.0 International License
   </a>
   .
  </footer>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js">
  </script>
  <script>
   hljs.initHighlightingOnLoad()
  </script>
 </body>
</html>